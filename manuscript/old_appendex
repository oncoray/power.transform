



```{r optimal_weighting_function_all_criteria_box_cox, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "Things."
)
# This finds lambda values at the optimised weighting parameter settings.
data <- .get_transformation_parameters(manuscript_dir = manuscript_dir)

data <- data[method == "Box-Cox"]
data <- data[, "lambda_error" := abs(lambda - target_lambda)]
non_robust_data <- data[weight_method == "non-robust", mget(c("method", "estimation_method", "k", "ii", "lambda_error"))]
data[non_robust_data, on=c("method", "estimation_method", "k", "ii"), "non_robust_lambda_error" := i.lambda_error]

data[, "better_than_non_robust" := lambda_error <= non_robust_lambda_error]
data[k == 0.0, "better_than_non_robust" := NA_integer_]
data[weight_method == "non-robust", "better_than_non_robust" := NA_integer_]

pass_rate_data <- data[, list(
  "pass_rate" = sum(better_than_non_robust, na.rm=TRUE),
  "n" = sum(!is.na(better_than_non_robust)),
  "median_error" = stats::median(lambda_error)),
  by=c("method", "estimation_method", "weight_method")]
pass_rate_data <- pass_rate_data[, "pass_rate" := round(pass_rate / n * 100.0, 1)]

p <- ggplot2::ggplot(
  data = data,
  mapping = ggplot2::aes(
    x = weight_method,
    y = lambda_error,
    fill = estimation_method))
p <- p + plot_theme
p <- p + ggplot2::geom_violin(draw_quantiles = 0.5)
p <- p + ggplot2::facet_wrap("estimation_method", nrow=nlevels(data$estimation_method))
p <- p + paletteer::scale_fill_paletteer_d(
  palette = "ggthemes::Tableau_10",
  drop = FALSE)
p <- p + ggplot2::scale_y_sqrt(name = latex2exp::TeX("$| \\hat{\\lambda}^r - \\hat{\\lambda}_{0}|$"))
p <- p + ggplot2::xlab("weighting method")
p <- p + ggplot2::ggtitle("shift-sensitive Box-Cox transformation")
p <- p + ggplot2::theme(
  axis.text.x = ggplot2::element_text(
    angle=30.0,
    hjust=1.0))
p
```


```{r optimal_weighting_function_all_criteria_yeo_johnson, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "Things."
)
# This finds lambda values at the optimised weighting parameter settings.
data <- .get_transformation_parameters(manuscript_dir = manuscript_dir)

data <- data[method == "Yeo-Johnson"]
data <- data[, "lambda_error" := abs(lambda - target_lambda)]
non_robust_data <- data[weight_method == "non-robust", mget(c("method", "estimation_method", "k", "ii", "lambda_error"))]
data[non_robust_data, on=c("method", "estimation_method", "k", "ii"), "non_robust_lambda_error" := i.lambda_error]

data[, "better_than_non_robust" := lambda_error <= non_robust_lambda_error]
data[k == 0.0, "better_than_non_robust" := NA_integer_]
data[weight_method == "non-robust", "better_than_non_robust" := NA_integer_]

pass_rate_data <- data[, list(
  "pass_rate" = sum(better_than_non_robust, na.rm=TRUE),
  "n" = sum(!is.na(better_than_non_robust)),
  "median_error" = stats::median(lambda_error)),
  by=c("method", "estimation_method", "weight_method")]
pass_rate_data[, "pass_rate" := round(pass_rate / n * 100.0, 1)][order(pass_rate)]

p <- ggplot2::ggplot(
  data = data,
  mapping = ggplot2::aes(
    x = weight_method,
    y = lambda_error,
    fill = estimation_method))
p <- p + plot_theme
p <- p + ggplot2::geom_violin(draw_quantiles = 0.5)
p <- p + ggplot2::facet_wrap("estimation_method", nrow=nlevels(data$estimation_method))
p <- p + paletteer::scale_fill_paletteer_d(
  palette = "ggthemes::Tableau_10",
  drop = FALSE)
p <- p + guides(fill = ggplot2::guide_legend(title = "optimisation criterion"))
p <- p + ggplot2::scale_y_sqrt(name = latex2exp::TeX("$| \\hat{\\lambda}^r - \\hat{\\lambda}_{0}|$"))
p <- p + ggplot2::xlab("weighting method")
p <- p + ggplot2::ggtitle("shift-sensitive Yeo-Johnson transformation")
p <- p + ggplot2::theme(
  axis.text.x = ggplot2::element_text(
    angle=30.0,
    hjust=1.0))
p

```
# Appendix C: Right-sided outliers

```{r eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE}

.get_optimised_weighting_function_parameters(manuscript_dir = manuscript_dir, side = "right")

if(!file.exists(file.path(manuscript_dir, "robustness_comparison_rightside_outlier.RDS"))){
  set.seed(95)

  .compute_lambda <- function(x, method){
    # Create transformer object.
    transformer <- suppressWarnings(
      power.transform::find_transformation_parameters(
        x = x,
        method = method,
        robust = FALSE,
        shift = TRUE))

    return(transformer@lambda)
  }

  # Generate table of asymmetric generalised normal distribution parameters.
  n_distributions <- 100

  # Generate alpha, beta and n.
  n <- stats::runif(n=n_distributions, min=2, max=4)
  n <- ceiling(10^n)

  alpha <- stats::runif(n=n_distributions, min=0.01, max=0.99)
  beta <- stats::runif(n=n_distributions, min=1.00, max=5.00)

  # Generate corresponding distributions.
  x <- mapply(
    power.transform::ragn,
    n = n,
    alpha = alpha,
    beta = beta)

  # Compute lambda values without weighting for Box-Cox.
  target_lambda_bc <- sapply(
    x,
    .compute_lambda,
    method = "box_cox")

  # Compute lambda values without weighting for Yeo-Johnson
  target_lambda_yj <- sapply(
    x,
    .compute_lambda,
    method = "yeo_johnson")

  # Add outliers, between 0.00 and 0.10.
  n_outliers <- 10
  outlier_fraction <- (seq_len(n_outliers) - 1)^2 /((n_outliers-1)^2 * 20)

  # Find optimised k1 and k2 values for all combinations of weight sources and
  # weighting functions.
  x <- mapply(
    function(x, n, alpha, beta, target_lambda_bc, target_lambda_yj, outlier_fraction){

      parameters <- list("n" = n, "alpha"=alpha, "beta" = beta, "target_lambda_bc"=target_lambda_bc, "target_lambda_yj"=target_lambda_yj)

      x <- lapply(
        outlier_fraction,
        function(k, x, parameters){
          # Set parameters.
          parameters$k <- k

          # Compute interquartile range.
          interquartile_range <- stats::IQR(x)

          # Compute upper and lower quartiles.
          q_lower <- stats::quantile(x, probs=0.25, names=FALSE)
          q_upper <- stats::quantile(x, probs=0.75, names=FALSE)

          # Set data where the outliers will be copied into.
          x_outlier <- x

          if(k != 0.0){
            n_draw <- ceiling(k * length(x))

            # Generate outlier values that are smaller than Q1 - 1.5 IQR or larger
            # than Q3 + 1.5 IQR.
            x_random <- stats::runif(n_draw, min=0.0, max=2.0)
            outlier <- q_upper + 1.5 * interquartile_range + x_random * interquartile_range

            # Randomly insert outlier values.
            x_outlier[sample(seq_along(x), size=n_draw, replace=FALSE)] <- outlier
          }

          return(list(
            "x" = x_outlier,
            "parameters" = parameters))
        },
        x = x,
        parameters)

      return(x)
    },
    x = x,
    n = n,
    alpha = alpha,
    beta = beta,
    target_lambda_bc = target_lambda_bc,
    target_lambda_yj = target_lambda_yj,
    MoreArgs=list("outlier_fraction"=outlier_fraction),
    SIMPLIFY=FALSE,
    USE.NAMES=FALSE)

  x <- unlist(x, recursive = FALSE)

  # Optimise k1, k2 so that sum of absolute differences with target-lambda is
  # minimised for each combination of source and weighting method.
  experiment_args <- coro::generator(
    function(){

      for(method in c("box_cox", "yeo_johnson")){
        # Non-robust transformation.
        fun_args <- list(
          "method" = method,
          "robust" = FALSE,
          "shift" = TRUE)

        opt_args <- list()

        yield(list(
          "name" = "non-robust",
          "method" = method,
          "fun_args" = fun_args,
          "opt_args" = opt_args
        ))
      }

      for(robustness_source in c("original", "transformed", "residual")){

        def_opt_limits <- list("k1"=c(0.0, 10.0), "k2"=c(0.0, 10.0))

        if(robustness_source == "original"){
          def_opt_limits$k1[2] <- 1.0
          def_opt_limits$k2[2] <- 1.0

          def_opt_init <- list("k1" = 0.80, "k2" = 0.95)

        } else if(robustness_source == "transformed"){
          def_opt_init <- list("k1" = 1.28, "k2" = 1.96)

        } else if(robustness_source == "residual"){
          def_opt_init <- list("k1" = 0.50, "k2" = 1.0)
        }

        for(method in c("box_cox", "yeo_johnson")){
          for(robustness_weighting_function in c("step", "triangle", "cosine")){

            fun_args <- list(
              "method" = method,
              "robust" = TRUE,
              "shift" = TRUE,
              "weight_method" = paste0(robustness_source, "_", robustness_weighting_function),
              "backup_use_default" = FALSE)

            opt_limits <- def_opt_limits
            opt_init <- def_opt_init

            # Step-weighting only has k1 as a parameter.
            if(robustness_weighting_function == "step"){
              opt_limits$k2 <- NULL
              opt_init$k2 <- NULL
            }

            yield(list(
              "name" = paste0(robustness_source, "-", robustness_weighting_function),
              "method" = method,
              "fun_args" = fun_args,
              "opt_args" = list("initial" = opt_init, "limits" = opt_limits)
            ))
          }
        }
      }
    })

  # Function for computing transformer parameters under optimisation constraints.
  compute_robust_lambda <- function(
    x,
    fun_args,
    opt_args,
    cl_internal=NULL){

    # Custom parser.
    ..outlier_parser <- function(
    x,
    fun_args,
    opt_args
    ){
      # Create transformer.
      transformer <- suppressWarnings(do.call(
        power.transform::find_transformation_parameters,
        args=c(
          list("x" = x$x),
          fun_args,
          opt_args)))

      parameter_data <- c(
        x$parameters,
        opt_args,
        list(
          "method" = fun_args$method,
          "weight_method" = fun_args$weight_method,
          "lambda" = transformer@lambda,
          "shift" = transformer@shift)
      )

      return(parameter_data)
    }

    if(is.null(cl_internal)){
      parameter_data <- lapply(
        x,
        ..outlier_parser,
        fun_args = fun_args,
        opt_args = opt_args)

    } else {
      parameter_data <- parallel::parLapply(
        cl = cl_internal,
        X = x,
        fun = ..outlier_parser,
        fun_args = fun_args,
        opt_args = opt_args)
    }

    return(parameter_data)
  }


  # Inner optimisation function that sets the loss.
  ..optimisation_function <- function(
    opt_param,
    x,
    fun_args,
    cl_internal = NULL){

    # Parse options.
    opt_args <- list()
    if(length(opt_param) >= 1) opt_args <- c(opt_args, list("k1"=opt_param[1]))
    if(length(opt_param) >= 2) opt_args <- c(opt_args, list("k2"=opt_param[2]))

    parameter_data <- compute_robust_lambda(
      x = x,
      fun_args = fun_args,
      opt_args = opt_args,
      cl_internal = cl_internal)

    if(fun_args$method == "box_cox"){
      lambda_error <- sapply(
        parameter_data,
        function(x) (abs(x$lambda - x$target_lambda_bc)))

    } else {
      # Yeo-Johnson.
      lambda_error <- sapply(
        parameter_data,
        function(x) (abs(x$lambda - x$target_lambda_yj)))
    }

    return(sum(lambda_error))
  }


  # Outer optimisation function
  .optimisation_function <- function(
    experiment,
    data,
    cl_internal = NULL){

    set.seed(9)

    # Run optimiser.
    if(length(experiment$opt_args) > 0){

      x0 <- unlist(experiment$opt_args$initial)

      lower <- experiment$opt_args$limits$k1[1]
      upper <- experiment$opt_args$limits$k1[2]

      if(length(experiment$opt_args$limits) > 1){
        lower <- c(lower, experiment$opt_args$limits$k2[1])
        upper <- c(upper, experiment$opt_args$limits$k2[2])
      }

      h <- nloptr::sbplx(
        x0 = x0,
        fn = .optimisation_inner,
        lower = lower,
        upper = upper,
        control=list("xtol_rel"=1e-3, "ftol_rel"=1e-4),
        x = data,
        fun_args = experiment$fun_args,
        cl_internal = cl_internal)

      if(length(h$par) == 1){
        return(list("name"=experiment$name, "method"=experiment$method, "k1" = h$par, "value" = h$value))

      } else {
        return(list("name"=experiment$name, "method"=experiment$method, "k1" = h$par[1], "k2" = h$par[2], "value" = h$value))
      }

    } else {
      value <- ..optimisation_function(
        opt_param=list(),
        x = data,
        fun_args=experiment$fun_args,
        cl = cl_internal)

      return(list("name"=experiment$name, "method"=experiment$method, "value" = value))
    }
  }


  # Start cluster
  cl <- parallel::makeCluster(18L)

  parallel::clusterExport(
    cl=cl,
    varlist = c("compute_robust_lambda", "..optimisation_function")
  )

  experiment_results <- parallel::parLapplyLB(
    cl = cl,
    X = coro::collect(experiment_args()),
    fun = .optimisation_function,
    data = x,
    chunk.size = 1L)

  # Stop cluster.
  parallel::stopCluster(cl)

  saveRDS(experiment_results, file = file.path(manuscript_dir, "robustness_comparison_rightside_outlier.RDS"))
}
```

# Appendix D: Generalisability of empirical goodness of fit test

The empirical goodness of fit test was characterised using the empirical distribution of type I errors related to the mean residual error in the central portion ($\kappa = 0.80$) of the transformed feature distribution, in the presence of outliers. The corresponding power transformations were both robust and shift sensitive, and used maximum likelihood estimation (MLE) for optimisation of transformation parameters. An important concern is how well the test performs for conventional power transformations (in absence of outliers), and for optimisation criteria that are objectively better than MLE, i.e. the Cramér-von Mises criterion.

For Figure *insert* determined type 1 error rate as before, but for several variants. In addition to robust, shift sensitive transformations using MLE, conventional transformations and robust, shift-sensitive using the Crámer-von Mises criterion were assessed. Conventional transformations only used randomly drawn asymmetric generalised normal distributions without added outliers. This figure suggests two things: If conventional transformations were used instead of robust, shift sensitive variants, the test characteristics would be highly similar. However, the robust, shift sensitive variant shows that the test characteristics are even stable in the presence of outliers. Secondly, the optimisation criterion does directly matter, and compared to MLE, the Crámer-von Mises criterion results in transformed distributions that better approximate central normality. In this case, one could argue whether characteristics of the empirical goodness of fit test should be based on the Crámer-von Mises criterion instead of MLE. We would argue that MLE-based test characteristics form a good foundation, for two reasons. One, MLE is the most prevalent optimisation criterion for power transformations, and two, the resulting transformations still approach central normality.

```{r gof_test_type_1_error_rate_appendix, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "Type I error rates for power transformation variants. ",
  "The robust, shift sensitive power transformation using maximum likelihood estimation (MLE) is shown as comparison. ",
  "Optimisation of transformation parameters using the Crámer-von Mises (C-vM) criterion leads to lower residual errors compared to MLE, ",
  "particularly for Box-Cox transformations, i.e. they lead to transformations that are closer to central normality. ",
  "Conventional transformations (in the absence of outliers) yield curves that are comparable to their robust, shift sensitive variants."
)

.plot_type_1_error_rate_appendix(
  plot_theme = plot_theme,
  manuscript_dir = manuscript_dir)

```

```{r include = FALSE, eval = FALSE}
# CANDIDATE FOR REMOVAL !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

data <- .get_ml_experiment_data(manuscript_dir = manuscript_dir)

# Convert to data matrix. First aggregate within each experiment because the
# Wilcoxon signed rank test is not for repeated measurements.
data <- data.table::dcast(
  data,
  dataset ~ learner + transformation_method + normalisation_method, value.var="value_rank",
  fun.aggregate = mean
)
data[, "dataset" := NULL]

# Compute p-values through pair-wise comparison.
config_pairs <- utils::combn(colnames(data), m = 2L)
test_data <- apply(
  config_pairs,
  MARGIN = 2,
  function(config_pair, data) {
    h <- stats::wilcox.test(
      x = data[[config_pair[1L]]],
      y = data[[config_pair[2L]]],
      paired = TRUE,
      alternative = "two.sided"
    )

    return(data.table::data.table(
      "config_1" = config_pair[1L],
      "config_2" = config_pair[2L],
      "raw_p_value" = h$p.value
    ))
  },
  data = data
)
test_data <- data.table::rbindlist(test_data)

# Perform multiple testing correction.
test_data[, "p_value_corrected" := stats::p.adjust(raw_p_value)]
```
