---
title: "Shift-sensitive Power Transformations for Transforming Data to Normality"
author: "Alex Zwanenburg"
date: "2023-01-06"
output: 
  pdf_document:
    latex_engine: lualatex
bibliography: "refs.bib"
header-includes:
   - \usepackage{amsmath}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.align= "center")

# Allow for defining figure captions within a chunk.
knitr::opts_knit$set(
  eval.after = "fig.cap")

require(power.transform)
require(ggplot2)
require(data.table)
require(latex2exp)

# Base font size.
base_size <- 10

# Set seed.
set.seed(19L)

get_annotation_settings <- function(ggtheme=NULL){
  # Import formatting settings from the provided ggtheme.
  
  # Find the text size for the table. This is based on text sizes in the
  # ggtheme.
  fontsize <- ggtheme$text$size
  fontsize_rel <- 1.0
  
  # Attempt to base the text size on the general axis.text attribute.
  if(!is.null(ggtheme$axis.text$size)){
    if(inherits(ggtheme$axis.text$size, "rel")){
      # Find the relative text size of axis text.
      fontsize_rel <- as.numeric(ggtheme$axis.text$size)
      
    } else {
      # Set absolute text size.
      fontsize <- ggtheme$axis.text$size
      fontsize_rel <- 1.0
    }
  }
  
  # Attempt to refine the text size using the axis.text.y attribute in
  # particular.
  if(!is.null(ggtheme$axis.text.y$size)){
    if(inherits(ggtheme$axis.text.y$size, "rel")){
      # Set relative text size of axis text
      fontsize_rel <- as.numeric(ggtheme$axis.text.y$size)
      
    } else {
      # Set absolute text size.
      fontsize <- as.numeric(ggtheme$axis.text.y$size)
      fontsize_rel <- 1.0
    }
  }
  
  # Update the text size using the magical ggplot2 point size (ggplot2:::.pt).
  geom_text_size <- fontsize * fontsize_rel / 2.845276
  
  # Obtain lineheight
  lineheight <- ggtheme$text$lineheight
  if(!is.null(ggtheme$axis.text$lineheight)) lineheight <- ggtheme$axis.text$lineheight
  if(!is.null(ggtheme$axis.text.y$lineheight)) lineheight <- ggtheme$axis.text.y$lineheight
  
  # Obtain family
  fontfamily <- ggtheme$text$family
  if(!is.null(ggtheme$axis.text$family)) fontfamily <- ggtheme$axis.text$family
  if(!is.null(ggtheme$axis.text.y$family)) fontfamily <- ggtheme$axis.text.y$family
  if(!is.null(ggtheme$axis.text.x$family)) fontfamily <- ggtheme$axis.text.x$family
  
  # Obtain face
  fontface <- ggtheme$text$face
  if(!is.null(ggtheme$axis.text$face)) fontface <- ggtheme$axis.text$face
  if(!is.null(ggtheme$axis.text.y$face)) fontface <- ggtheme$axis.text.y$face
  if(!is.null(ggtheme$axis.text.x$face)) fontface <- ggtheme$axis.text.x$face
  
  # Obtain colour
  colour <- ggtheme$text$colour
  if(!is.null(ggtheme$axis.text$colour)) colour <- ggtheme$axis.text$colour
  if(!is.null(ggtheme$axis.text.y$colour)) colour <- ggtheme$axis.text.y$colour
  if(!is.null(ggtheme$axis.text.x$colour)) colour <- ggtheme$axis.text.x$colour
  
  return(list(
    "geom_text_size"=geom_text_size,
    "fontsize"=fontsize,
    "fontsize_rel"=fontsize_rel,
    "colour"=colour,
    "family"=fontfamily,
    "face"=fontface,
    "lineheight"=lineheight))
}

```

# Abstract

# Introduction

Numerical variables in datasets may strongly deviate from normal distributions, e.g. by being skewed. This may complicate further analysis. Power transformations [@Tukey1957-rt] can help improve normality of such features. The two most commonly used transformations are that of @Box1964-mz and @Yeo2000-vw. The Box-Cox transformation of a feature $x$ under the transformation parameter $\lambda$ is defined as:

```{=latex}
\begin{equation}
\phi_{\text{BC}}^\lambda (x) = 
\begin{cases}
\left(x^\lambda - 1 \right) / \lambda & \text{if } \lambda \neq 0\\
\log(x) & \text{if } \lambda = 0
\end{cases}
\end{equation}
```
One limitation of th Box-Cox transformation is that it is only defined for $x > 0$. In contrast, the Yeo-Johnson transformation under the transformation parameter $\lambda$ is defined for any $x \in \mathbb{R}$:

```{=latex}
\begin{equation}
\phi_{\text{YJ}}^\lambda (x) = 
\begin{cases}
\left( \left( 1 + x \right)^\lambda - 1\right) / \lambda & \text{if } \lambda \neq 0 \text{ and } x \geq 0\\
\log(1 + x) & \text{if } \lambda = 0 \text{ and } x \geq 0\\
-\left( \left( 1 - x\right)^{2 - \lambda} - 1 \right) / \left(2 - \lambda \right) & \text{if } \lambda \neq 2 \text{ and } x < 0\\
-\log(1 - x) & \text{if } \lambda = 2 \text{ and } x < 0
\end{cases}
\end{equation}
```
The $\lambda$-parameter is typically optimised using maximum likelihood estimation under the assumption that the transformed feature is normally distributed. As noted by Raymaekers and Rousseeuw, this approach is very sensitive to the presence of outliers, and robust versions of Box-Cox and Yeo-Johnson transformations were devised [@Raymaekers2021-kq].

Power transformation does not guarantee that transformed variables are normally distributed. In fact, depending on location and scale of the variable, power transformation may reduce normality, as shown in Figure.

``` {r reduced-normality, echo=FALSE, fig.cap=cap}
cap <- paste0(
  "Power transformations may reduce normality depending on location. ",
  "Variable $x$ consists of $10000$ values are randomly drawn from a normal distribution $\\mathcal{N}(\\mu, 1)$. ",
  "For both Box-Cox and Yeo-Johnson transformations, we would expect $\\lambda = 1$, since $x$ is already normally distributed. ",
  "The figure shows that instead, $\\lambda$ is strongly dependent on the location $\\mu$, ",
  "and even breaks down due to numerical issues for $\\mu > 10^3$.")

compute_lambda <- function(mu, x, method){
  # Create transformer object.
  transformer <- suppressWarnings(
    power.transform::find_transformation_parameters(
      x = x + mu,
      method = method,
      robust = FALSE,
      shift = FALSE,
      lambda = NULL))
  
  # Return lambda value.
  return(transformer@lambda)
}

x <- stats::rnorm(10000L)
shift_range <- 10^seq(from=0, to=6, by=0.1)

box_cox_values <- sapply(
  shift_range,
  compute_lambda,
  x = x,
  method = "box_cox")

yeo_johnson_values <- sapply(
  shift_range,
  compute_lambda,
  x = x,
  method = "yeo_johnson")

data <- data.table::data.table(
  "mu" = log10(c(shift_range, shift_range)),
  "lambda" = c(box_cox_values, yeo_johnson_values),
  "method" = factor(x=rep(c("Box-Cox", "Yeo-Johnson"), each = length(shift_range), levels=c("Box-Cox", "Yeo-Johnson"))))

annotation_settings <- get_annotation_settings(ggplot2::theme_light(base_size = base_size))

p <- ggplot2::ggplot(
  data = data,
  mapping = ggplot2::aes(
    x = mu,
    y = lambda,
    colour = method))
p <- p + ggplot2::theme_light(base_size=base_size)
p <- p + ggplot2::geom_point()
p <- p + ggplot2::geom_hline(yintercept = 1.0, linetype="longdash", colour="grey40")
p <- p + ggplot2::annotate(
  geom = "text",
  x = 6,
  y = 1,
  label = "expected",
  colour=annotation_settings$colour,
  family=annotation_settings$family,
  fontface=annotation_settings$face,
  size=annotation_settings$geom_text_size,
  vjust=1.5,
  hjust=1.0)
p <- p + ggplot2::scale_x_continuous(
  name = latex2exp::TeX("$\\mu$"),
  labels = scales::math_format())

p <- p + ggplot2::scale_y_continuous(
  name = latex2exp::TeX("$\\lambda$"))

p
```



Figure showing Ames dataset feature "Year_Built". Panels: Histogram of feature; Q-Q plot of original feature, normal Box-Cox, robust Box-Cox, normal Yeo-Johnson, robust Yeo-Johnson.

We make the following contributions:

-   We devise shift-sensitive versions of the Box-Cox and Yeo-Johnson transformation, including versions robust to outliers.

-   We define an empirical measure of the goodness of fit for detecting cases when power transformations fail to yield a normally distributed transformed feature.

-   We assess the effect of power transformations on the performance of machine learning models.

# Methods

# Simulation

# Experimental Results

# Discussion

# Conclusion

# Code availability

Shift-sensitive power transformations are implemented in the `power.transform` package.

# Appendix A.

# References

