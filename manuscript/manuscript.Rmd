---
title: "Shift-sensitive Power Transformations for Transforming Data to Normality"
author: "Alex Zwanenburg"
date: "2023-01-06"
output: 
  pdf_document:
    latex_engine: lualatex
bibliography: "refs.bib"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{booktabs}
   - \DeclareMathOperator*{\argmax}{argmax}
   - \DeclareMathOperator{\sgn}{sgn}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.align= "center")

# Allow for defining figure captions within a chunk.
knitr::opts_knit$set(
  eval.after = "fig.cap")

require(power.transform)
require(ggplot2)
require(data.table)
require(latex2exp)
require(coro)

manuscript_dir <- file.path(
  getwd())

# Base font size.
base_size <- 10

# Set general theme.
plot_theme <- ggplot2::theme_light(base_size=base_size)

get_annotation_settings <- function(ggtheme=NULL){
  # Import formatting settings from the provided ggtheme.
  
  # Find the text size for the table. This is based on text sizes in the
  # ggtheme.
  fontsize <- ggtheme$text$size
  fontsize_rel <- 1.0
  
  # Attempt to base the text size on the general axis.text attribute.
  if(!is.null(ggtheme$axis.text$size)){
    if(inherits(ggtheme$axis.text$size, "rel")){
      # Find the relative text size of axis text.
      fontsize_rel <- as.numeric(ggtheme$axis.text$size)
      
    } else {
      # Set absolute text size.
      fontsize <- ggtheme$axis.text$size
      fontsize_rel <- 1.0
    }
  }
  
  # Attempt to refine the text size using the axis.text.y attribute in
  # particular.
  if(!is.null(ggtheme$axis.text.y$size)){
    if(inherits(ggtheme$axis.text.y$size, "rel")){
      # Set relative text size of axis text
      fontsize_rel <- as.numeric(ggtheme$axis.text.y$size)
      
    } else {
      # Set absolute text size.
      fontsize <- as.numeric(ggtheme$axis.text.y$size)
      fontsize_rel <- 1.0
    }
  }
  
  # Update the text size using the magical ggplot2 point size (ggplot2:::.pt).
  geom_text_size <- fontsize * fontsize_rel / 2.845276
  
  # Obtain lineheight
  lineheight <- ggtheme$text$lineheight
  if(!is.null(ggtheme$axis.text$lineheight)) lineheight <- ggtheme$axis.text$lineheight
  if(!is.null(ggtheme$axis.text.y$lineheight)) lineheight <- ggtheme$axis.text.y$lineheight
  
  # Obtain family
  fontfamily <- ggtheme$text$family
  if(!is.null(ggtheme$axis.text$family)) fontfamily <- ggtheme$axis.text$family
  if(!is.null(ggtheme$axis.text.y$family)) fontfamily <- ggtheme$axis.text.y$family
  if(!is.null(ggtheme$axis.text.x$family)) fontfamily <- ggtheme$axis.text.x$family
  
  # Obtain face
  fontface <- ggtheme$text$face
  if(!is.null(ggtheme$axis.text$face)) fontface <- ggtheme$axis.text$face
  if(!is.null(ggtheme$axis.text.y$face)) fontface <- ggtheme$axis.text.y$face
  if(!is.null(ggtheme$axis.text.x$face)) fontface <- ggtheme$axis.text.x$face
  
  # Obtain colour
  colour <- ggtheme$text$colour
  if(!is.null(ggtheme$axis.text$colour)) colour <- ggtheme$axis.text$colour
  if(!is.null(ggtheme$axis.text.y$colour)) colour <- ggtheme$axis.text.y$colour
  if(!is.null(ggtheme$axis.text.x$colour)) colour <- ggtheme$axis.text.x$colour
  
  return(list(
    "geom_text_size"=geom_text_size,
    "fontsize"=fontsize,
    "fontsize_rel"=fontsize_rel,
    "colour"=colour,
    "family"=fontfamily,
    "face"=fontface,
    "lineheight"=lineheight))
}

parse_mean_sd_latex <- function(mu, sigma, digits=2L){
  return(paste0(
    "$", round(mu, digits=digits), " \\pm ", round(sigma, digits=digits), "$"
  ))
}

```

# Abstract

# Introduction

Numerical variables in datasets may strongly deviate from normal distributions, e.g. by being skewed. This may complicate further analysis. Power transformations [@Tukey1957-rt] can help improve normality of such features. The two most commonly used transformations are that of @Box1964-mz and @Yeo2000-vw. The Box-Cox transformation of a feature $x$ under the transformation parameter $\lambda$ is defined as:

```{=latex}
\begin{equation}
\phi_{\text{BC}}^\lambda (x) = 
\begin{cases}
\left(x^\lambda - 1 \right) / \lambda & \text{if } \lambda \neq 0\\
\log(x) & \text{if } \lambda = 0
\end{cases}
\end{equation}
```
One limitation of th Box-Cox transformation is that it is only defined for $x > 0$. In contrast, the Yeo-Johnson transformation under the transformation parameter $\lambda$ is defined for any $x \in \mathbb{R}$:

```{=latex}
\begin{equation}
\phi_{\text{YJ}}^\lambda (x) = 
\begin{cases}
\left( \left( 1 + x \right)^\lambda - 1\right) / \lambda & \text{if } \lambda \neq 0 \text{ and } x \geq 0\\
\log(1 + x) & \text{if } \lambda = 0 \text{ and } x \geq 0\\
-\left( \left( 1 - x\right)^{2 - \lambda} - 1 \right) / \left(2 - \lambda \right) & \text{if } \lambda \neq 2 \text{ and } x < 0\\
-\log(1 - x) & \text{if } \lambda = 2 \text{ and } x < 0
\end{cases}
\end{equation}
```
The $\lambda$-parameter is typically optimised using maximum likelihood estimation under the assumption that the transformed feature is normally distributed. As noted by Raymaekers and Rousseeuw, this approach is very sensitive to the presence of outliers, and robust versions of Box-Cox and Yeo-Johnson transformations were devised [@Raymaekers2021-kq].

Power transformation does not guarantee that transformed variables are normally distributed. In fact, depending on location and scale of the variable, power transformation may reduce normality, as shown in Figure **INSERT REFERENCE**. This is problematic for automated power transformations, for three reasons. First, normality is often desired in machine learning applications. Second, a large negative or positive $\lambda$-parameter may lead to numeric issues. Third, we currently do not have a test to automatically reject poor transformations. Statistical tests for normality exist, such as the Shapiro-Wilk test [@Shapiro1965-zd]. Given sufficiently large sample sizes, these tests can detect trivial deviations from normality and are therefore not well-suited for practical applications of automated power transformations.

```{r reduced-normality, echo=FALSE, fig.cap=cap, eval=FALSE}
cap <- paste0(
  "Power transformations may reduce normality depending on location. ",
  "Variable $x$ consists of $10000$ values are randomly drawn from a normal distribution $\\mathcal{N}(\\mu, 1)$. ",
  "For both Box-Cox and Yeo-Johnson transformations, we would expect $\\lambda = 1$, since $x$ is already normally distributed. ",
  "The figure shows that instead, $\\lambda$ is strongly dependent on the location $\\mu$, ",
  "and even breaks down due to numerical issues for $\\mu > 5 10^4$.")

compute_lambda <- function(mu, x, method){
  # Create transformer object.
  transformer <- suppressWarnings(
    power.transform::find_transformation_parameters(
      x = x + mu,
      method = method,
      robust = FALSE,
      shift = FALSE,
      lambda = NULL))
  
  # Return lambda value.
  return(transformer@lambda)
}

# Non-standard evaluation
mu <- lambda <- method <- NULL

# Set seed.
set.seed(19L)

x <- power.transform::ragn(10000L, location=0, scale=1/sqrt(2), alpha=0.5, beta=2)
shift_range <- 10^seq(from=0, to=6, by=0.1)

box_cox_values <- sapply(
  shift_range,
  compute_lambda,
  x = x,
  method = "box_cox")

yeo_johnson_values <- sapply(
  shift_range,
  compute_lambda,
  x = x,
  method = "yeo_johnson")

data <- data.table::data.table(
  "mu" = log10(c(shift_range, shift_range)),
  "lambda" = c(box_cox_values, yeo_johnson_values),
  "method" = factor(
    x=rep(
      c("Box-Cox", "Yeo-Johnson"),
      each = length(shift_range)),
    levels=c("Box-Cox", "Yeo-Johnson")))

annotation_settings <- get_annotation_settings(ggplot2::theme_light(base_size = base_size))

p_bc <- ggplot2::ggplot(
  data = data[method == "Box-Cox"],
  mapping = ggplot2::aes(
    x = mu,
    y = lambda,
    colour = method))
p_bc <- p_bc + plot_theme
p_bc <- p_bc + ggplot2::geom_point()
p_bc <- p_bc + ggplot2::geom_hline(
  yintercept = 1.0, 
  linetype="longdash", 
  colour="grey40")
p_bc <- p_bc + ggplot2::annotate(
  geom = "text",
  x = 6,
  y = 1,
  label = "expected",
  colour=annotation_settings$colour,
  family=annotation_settings$family,
  fontface=annotation_settings$face,
  size=annotation_settings$geom_text_size,
  vjust=-1.0,
  hjust=1.0)
p_bc <- p_bc + ggplot2::scale_x_continuous(
  name = latex2exp::TeX("$\\mu$"),
  labels = scales::math_format())
p_bc <- p_bc + ggplot2::scale_y_continuous(
  name = latex2exp::TeX("$\\lambda$"),
  limits = c(0.0, 35.0))
p_bc <- p_bc + paletteer::scale_color_paletteer_d(
  palette="ggthemes::Tableau_10",
  drop=FALSE,
  guide="none")

p_yj <- ggplot2::ggplot(
  data = data[method == "Yeo-Johnson"],
  mapping = ggplot2::aes(
    x = mu,
    y = lambda,
    colour = method))
p_yj <- p_yj + plot_theme
p_yj <- p_yj + ggplot2::geom_point()
p_yj <- p_yj + ggplot2::geom_hline(
  yintercept = 1.0,
  linetype="longdash",
  colour="grey40")
p_yj <- p_yj + ggplot2::annotate(
  geom = "text",
  x = 6,
  y = 1,
  label = "expected",
  colour=annotation_settings$colour,
  family=annotation_settings$family,
  fontface=annotation_settings$face,
  size=annotation_settings$geom_text_size,
  vjust=-1.0,
  hjust=1.0)
p_yj <- p_yj + ggplot2::scale_x_continuous(
  name = latex2exp::TeX("$\\mu$"),
  labels = scales::math_format())
p_yj <- p_yj + ggplot2::scale_y_continuous(
  name = latex2exp::TeX("$\\lambda$"),
  limits = c(0.0, 35.0))
p_yj <- p_yj + paletteer::scale_color_paletteer_d(
  palette="ggthemes::Tableau_10",
  drop=FALSE)
p_yj <- p_yj + ggplot2::theme(
  axis.title.y=ggplot2::element_blank(),
  axis.text.y=ggplot2::element_blank())

p <- egg::ggarrange(plots=list(p_bc, p_yj), ncol=2, draw=FALSE)
p
```

Here we make the following contributions:

-   We devise shift-sensitive versions of the Box-Cox and Yeo-Johnson transformation, including versions robust to outliers.

-   We define an empirical measure of the goodness of fit for detecting cases when power transformations fail to yield a normally distributed transformed feature.

-   We assess the effect of power transformations on the performance of machine learning models.

# Methods

## Shift-sensitive power transformation

Box-Cox and Yeo-Johnson transformations are modified by adding a shift parameter $x_0$. The modified Box-Cox transformation of a feature $x$ under transformation parameter $\lambda$ and shift parameter $x_0$ is then:

```{=latex}
\begin{equation}
\phi_{\text{BC}}^{\lambda, x_0} (x) = 
\begin{cases}
\left( \left(x - x_0 \right)^\lambda - 1 \right) / \lambda & \text{if } \lambda \neq 0\\
\log(x - x_0) & \text{if } \lambda = 0
\end{cases}
\end{equation}
```
Where $x - x_0 > 0$. Likewise, the modified Yeo-Johnson transformation of a feature $x$ under transformation parameter $\lambda$ and shift parameter $x_0$ is:

```{=latex}
\begin{equation}
\phi_{\text{YJ}}^{\lambda, x_0} (x) = 
\begin{cases}
\left( \left( 1 + x - x_0\right)^\lambda - 1\right) / \lambda & \text{if } \lambda \neq 0 \text{ and } x - x_0 \geq 0\\
\log(1 + x - x_0) & \text{if } \lambda = 0 \text{ and } x - x_0 \geq 0\\
-\left( \left( 1 - x - x_0\right)^{2 - \lambda} - 1 \right) / \left(2 - \lambda \right) & \text{if } \lambda \neq 2 \text{ and } x - x_0< 0\\
-\log(1 - x - x_0) & \text{if } \lambda = 2 \text{ and } x - x_0 < 0
\end{cases}
\end{equation}
```
For both modified transformations, $\lambda$ and $x_0$ are obtained by maximising the normal log-likelihood function. For the modified Box-Cox transformation, and ignoring constant terms, this can be written as:

```{=latex}
\begin{equation}
\left\{ \hat{\lambda}^{\text{BC}}, \hat{x}_0^{\text{BC}} \right\} = \argmax_{\lambda, x_0} \sum_{i=1}^n -\frac{1}{2}\log(\sigma^2) + (\lambda - 1) \log(x_i - x_0)
\end{equation}
```
where $\sigma^2$ is the variance of the Box-Cox transformed feature $\phi_{\text{BC}}^{\lambda, x_0} (x)$. Similarly for the modified Yeo-Johnson transformation:

```{=latex}
\begin{equation}
\left\{ \hat{\lambda}^{\text{YJ}}, \hat{x}_0^{\text{YJ}} \right\} = \argmax_{\lambda, x_0} \sum_{i=1}^n -\frac{1}{2}\log(\sigma^2) + (\lambda - 1) \sgn(x_i - x_0) \log(1 + |x_i - x_0|)
\end{equation}
```
where $\sigma^2$ is the variance of Yeo-Johnson transformed feature $\phi_{\text{YJ}}^{\lambda, x_0} (x)$.

## Robust shift-sensitive power transformations

Real-world data may contain outliers, to which maximum likelihood estimation can be sensitive. Their presence may lead to poor transformations to normality, as shown in Figure **INSERT REFERENCE**. As indicated by @Raymaekers2021-kq, the general aim of power transformations should be to transform non-outlier data to normality, i.e. achieve *central normality*. To achieve this, they devised an iterative procedure to find a robust estimate of the transformation parameter $\lambda$. Because $\lambda$ is updated during this procedure, simultaneous estimation of $\lambda$ and $x_0$ for modified power transformations is not possible.

In essence, obtaining transformation parameters that provide central normality requires identifying outliers in the data, and weighting such instances during the optimisation process. @Raymaekers2021-kq do this (during the final steps of the iterative procedure) by introducing a weighted maximum likelihood estimation. This weighted maximum log-likelihood estimator is based on equations **INSERT REFERENCE**. The weighted maximum log-likelihood estimator for the modified Box-Cox power transformation is:

```{=latex}
\begin{equation}
\left\{ \hat{\lambda}^{\text{rBC}}, \hat{x}_0^{\text{rBC}} \right\} = \argmax_{\lambda, x_0} \sum_{i=1}^n w_i \left[ -\frac{1}{2}\log(\sigma^2_w) + (\lambda - 1) \log(x_i - x_0) \right]
\end{equation}
```
where $\sigma^2_w$ is the weighted variance of the Box-Cox transformed feature $\phi_{\text{BC}}^{\lambda, x_0} (x)$:

```{=latex}
\begin{equation}
\sigma_w^2 = \frac{\sum_{i=1}^n w_i \left(\phi_{\text{BC}}^{\lambda, x_0} (x_i) - \mu_w \right)^2}{\sum_{i=1}^n w_i} \quad \text{with } \mu_w = \frac{\sum_{i=1}^n \phi_{\text{BC}}^{\lambda, x_0} (x_i)} {\sum_{i=1}^n w_i}
\end{equation}
```
Analogously, the weighted maximum log-likelihood estimator for the modified Yeo-Johnson power transformation is:

```{=latex}
\begin{equation}
\left\{ \hat{\lambda}^{\text{rYJ}}, \hat{x}_0^{\text{rYJ}} \right\} = \argmax_{\lambda, x_0} \sum_{i=1}^n w_i \left[ -\frac{1}{2}\log(\sigma^2) + (\lambda - 1) \sgn(x_i - x_0) \log(1 + |x_i - x_0|) \right]
\end{equation}
```
where $\sigma^2_w$ is the weighted variance of the Yeo-Johnson transformed feature $\phi_{\text{YJ}}^{\lambda, x_0} (x)$:

```{=latex}
\begin{equation}
\sigma_w^2 = \frac{\sum_{i=1}^n w_i \left(\phi_{\text{YJ}}^{\lambda, x_0} (x_i) - \mu_w \right)^2}{\sum_{i=1}^n w_i} \quad \text{with } \mu_w = \frac{\sum_{i=1}^n \phi_{\text{YJ}}^{\lambda, x_0} (x_i)} {\sum_{i=1}^n w_i}
\end{equation}
```

There are a few options based on which weights may be set. Here, we investigate three:

- Set weights based on probabilities of the empirical distribution of the original feature $x$. Probabilities are determined as $p_i = \frac{i - 1/3}{n + 1/3}$, with $i = 1, 2, \ldots n$, with $n$ the number of instances of feature $x$.

- Set weights based on z-score of the transformed feature $\phi^{\lambda, x_0} (x)$. After @Raymaekers2021-kq, $z_i = \frac{\phi^{\lambda, x_0}(x_i) - \mu_M}{\sigma_M}$. Here, $\mu_M$ and $\sigma_M$ are robust Huber M-estimates of location and scale of the transformed feature $\phi^{\lambda, x_0} (x)$ [@Huber1981-su].

- Set weights based on the residual error between the z-score of the transformed feature $\phi^{\lambda, x_0} (x)$ and the theoretical z-score from a standard normal distribution: $r_i =\left| \left( \phi^{\lambda, x_0}(x_i) - \mu_M)\right) / \sigma_M - F^{-1}_{\mathcal{N}}(p_i) \right|$, with $\mu_M$, $\sigma_M$ and $p_i$ as defined above.

Based on either of the above, weights are set using a weighting function. We will investigate the following functions, where $x_i$ can be either $p^{*}_i=2 \left( p_i - 0.5\right)$ (so that the quantiles are zero-centred), $z_i$ or $r_i$:

- A step function, with $k_1 \geq 0$ as threshold parameter:
```{=latex}
\begin{equation}
w_i =
\begin{cases}
1 & \text{if } \left| x_i \right| \leq k_1\\
0 & \text{if } \left| x_i \right| > k_1
\end{cases}
\end{equation}
```

- A triangle function (or generalised Huber weight), with $k_1 \geq 0$ and $k_2 \geq k_1$ as threshold parameters:
```{=latex}
\begin{equation}
w_i =
\begin{cases}
1 & \text{if } \left| x_i \right| < k_1\\
1 - \frac{\left| x_i \right| - k_1}{k_2 - k_1} & \text{if } k_1 \leq \left| x_i \right| \leq k_2 \\
0 & \text{if } \left| x_i \right| > k_2
\end{cases}
\end{equation}
```

- A tapered cosine function [@Tukey1967-eb], with $k_1 \geq 0$ and $k_2 \geq k_1$ as threshold parameters:
```{=latex}
\begin{equation}
w_i =
\begin{cases}
1 & \text{if } \left| x_i \right| < k_1\\
0.5 + 0.5 \cos\left(\pi \frac{\left| x_i \right| - k_1}{k_2 - k_1} \right) & \text{if } k_1 \leq \left| x_i \right| \leq k_2 \\
0 & \text{if } \left| x_i \right| > k_2
\end{cases}
\end{equation}
```

All weighting functions share the characteristic that for $x_i < k_1$, instances are fully weighted, i.e. when $k_1 > 0$ the weighting functions are symmetric window functions with a flat top. The triangle and tapered cosine functions then gradually down-weight instances with $k_1 \leq x_i \leq k_2$, and assign no weight to instances $x_i > k_2$.

```{r weight-functions , echo=FALSE, warning=FALSE, message=FALSE, fig.cap=cap, eval = FALSE}
cap <- paste0(
  "Weighting functions investigated in this study to make power transformations more robust against outliers. ",
  "The step function was parameterised with $k_1 = 0.60$. ",
  "The triangle and tapered cosine functions were both parameterised with $k_1 = 0.30$ and $k_2 = 0.90$.")

# Non-standard evaluation
d <- distribution <- method <- mean_lambda <- sd_lambda <- NULL

data_step <- data.table::data.table(x = seq(from=-1.0, to=1.0, by=0.001))
data_step[, "weight":=power.transform:::.step_window(x, k1=0.60)]

p_step <- ggplot2::ggplot(
  data = data_step,
  mapping = ggplot2::aes(
    x = x,
    y = weight))
p_step <- p_step + plot_theme
p_step <- p_step + ggplot2::geom_line()
p_step <- p_step + ggplot2::ggtitle(label="step")
p_step <- p_step + ggplot2::theme(
  plot.title=ggplot2::element_text(
    hjust=0.5,
    size=ggplot2::rel(1.0)))

data_triangle <- data.table::data.table(x = seq(from=-1.0, to=1.0, by=0.001))
data_triangle[, "weight":=power.transform:::.triangular_window(x, k1=0.30, k2=0.90)]

p_triangle <- ggplot2::ggplot(
  data = data_triangle,
  mapping = ggplot2::aes(
    x = x,
    y = weight))
p_triangle <- p_triangle + plot_theme
p_triangle <- p_triangle + ggplot2::geom_line()
p_triangle <- p_triangle + ggplot2::ggtitle(label="triangle")
p_triangle <- p_triangle + ggplot2::theme(
  plot.title=ggplot2::element_text(
    hjust=0.5,
    size=ggplot2::rel(1.0)),
  axis.text.y = ggplot2::element_blank(),
  axis.title.y = ggplot2::element_blank())

data_cosine <- data.table::data.table(x = seq(from=-1.0, to=1.0, by=0.001))
data_cosine[, "weight":=power.transform:::.tapered_cosine_window(x, k1=0.30, k2=0.90)]

p_cosine <- ggplot2::ggplot(
  data = data_cosine,
  mapping = ggplot2::aes(
    x = x,
    y = weight))
p_cosine <- p_cosine + plot_theme
p_cosine <- p_cosine + ggplot2::geom_line()
p_cosine <- p_cosine + ggplot2::ggtitle(label="tapered cosine")
p_cosine <- p_cosine + ggplot2::theme(
  plot.title = ggplot2::element_text(
    hjust = 0.5,
    size = ggplot2::rel(1.0)),
  axis.text.y = ggplot2::element_blank(),
  axis.title.y = ggplot2::element_blank())

p <- egg::ggarrange(plots=list(p_step, p_triangle, p_cosine), ncol=3, draw=FALSE)
p
```

<!-- Weights $w_i$ can be set based on the original feature $x$, as well as the transformed feature $\phi^{\lambda, x_0} (x)$. @Raymaekers2021-kq use the latter: -->

<!-- ```{=latex} -->
<!-- \begin{equation} -->
<!-- w_i = -->
<!-- \begin{cases} -->
<!-- 1 & \text{if } \frac{\left| \phi^{\lambda}(x) - \mu_M \right|}{\sigma_M} \leq Q_{\mathcal{N}} (0.995)\\ -->
<!-- 0 & \text{if } \frac{\left| \phi^{\lambda}(x) - \mu_M \right|}{\sigma_M} > Q_{\mathcal{N}} (0.995) -->
<!-- \end{cases} -->
<!-- \end{equation} -->
<!-- ``` -->
<!-- Here, $\mu_M$ and $\sigma_M$ are robust Huber M-estimates of location and scale of the transformed feature $\phi^{\lambda, x_0} (x)$ [@Huber1981-su]. In short, weights for all standardised transformed feature values that exceed the value corresponding to the $99.5^\text{th}$ percentile of the normal distribution ($Q_{\mathcal{N}}(0.995)$) are set to 0, and remaining weights are left at 1. This works well if outliers are relatively rare in the transformed feature values, which we cannot *a priori* assume in our use case. Using lower quantile values comes at the cost of essentially trimming more and more instances from the dataset. -->

<!-- Another option is to weight instances based on their absolute residuals, $\left| \left( \phi^{\lambda, x_0}(x_i) - \mu_M)\right) / \sigma_M - Q_{\mathcal{N}}(q_i) \right|$, where $q_i$ is equispaced quantile probability for instance $i$. This weights instances by the distance from the expected normal distribution. -->

<!-- Though we refrain from specifying the exact function here, one would down-weight instances with large absolute residuals more strongly than small absolute residuals. While this approach is able to deal with outliers, there is no guarantee that large absolute residuals are strictly outliers. They may also be found in the central region, and down-weighting there might lead to suboptimal solutions. -->

<!-- We propose a practical approach. Let $\kappa$ be the fraction of central instances of feature $x$, i.e. those located between $q_i=(1-\kappa)/2$ and $q_i=(1 + \kappa)/2$. For example, if $\kappa = 0.80$, those instances with values between the $10^{\text{th}}$ and $90^{\text{th}}$ percentiles would be considered to be central. We then use Tukey's tapered cosine window to assign weights [@Tukey1967-eb]: -->

<!-- ```{=latex} -->
<!-- \begin{equation} -->
<!-- w_i =  -->
<!-- \begin{cases} -->
<!-- \frac{1}{2} \left[1 + \cos\left(2 \pi \frac{i - \frac{1}{2}\left[ n \left(1 - \kappa\right) + 1\right]} {n \left(1 - \kappa \right)} \right) \right] & \text{, } 0 \leq i - 0.5 < (1 - \kappa) /2\\ -->
<!-- 1 & \text{, } (1 - \kappa) /2 \leq i - 0.5 \leq (1 + \kappa) /2\\ -->
<!-- \frac{1}{2} \left[1 + \cos\left(2 \pi \frac{i - \frac{1}{2}\left[ n \left(1 + \kappa\right) + 1\right]} {n \left(1 - \kappa \right)} \right) \right]& \text{, } (1 + \kappa) /2 < i - 0.5 \leq 1 -->
<!-- \end{cases} -->
<!-- \end{equation} -->
<!-- ``` -->
<!-- Here, $i = 1, 2, \ldots n$, with $n$ the number of instances of feature $x$. -->



## Asymmetric generalised normal distributions

Modifications intended to make power transformations invariant to shifts and methods to improve their robustness against outliers need to be assessed using data drawn from a range of different distributions. Since the power transformations are intended for use with unimodal distributions, the generalised normal distribution [@Subbotin1923-qk, @Nadarajah2005-xe] is a suitable option to simulate realistic feature distributions. This distribution has the following probability density function $f_{\beta}$ for a value $x \in \mathbb{R}$:

```{=latex}
\begin{equation}
f_{\beta}(x) = \frac{\beta}{2\Gamma \left(1 / \beta \right)} e^{-\left| x \right|^\beta}
\end{equation}
```
Here, $\Gamma$ is the gamma function, and $\beta$ is a strictly positive shape parameter. For $\beta = 1$, the probability density function describes a Laplace distribution. A normal distribution is found for $\beta=2$, and for large $\beta$, the distribution approaches a uniform distribution. We will refrain from introducing scale and location parameters here directly.

Realistic feature distributions may be skewed. Gijbels et al. describe a recipe for introducing skewness into the otherwise symmetric generalised norm distribution [@Gijbels2019-te], leading to the following probability density function:

```{=latex}
\begin{equation}
f_{\alpha}(x; \mu, \sigma, \beta) = \frac{2 \alpha \left(1 - \alpha\right)}{\sigma}
\begin{cases}
f_{\beta}\left( \left(1 - \alpha \right) \frac{\left| x - \mu \right|}{\sigma} \right) & \text{, } x \leq \mu \\
f_{\beta}\left( \alpha \frac{\left| x - \mu \right|}{\sigma} \right) & \text{, } x > \mu
\end{cases}
\end{equation}
```
Here $\alpha \in (0,1)$ is a skewness parameter. $\alpha > 0.5$ creates a distribution with a negative skew, i.e. a left-skewed distribution. A right-skewed distribution is created for $\alpha < 0.5$. $\mu$ and $\sigma \in (0, \infty)$ are location and scale parameters, respectively. $f_{\alpha}$ thus describes the probability density function of an asymmetric generalised normal distribution, which we will refer to here and parametrise as $\mathcal{AGN}\left(\mu, \sigma, \alpha, \beta \right)$.

We require a quantile function (or an approximation thereof) to draw random values from an asymmetric generalised normal distribution using inverse transform sampling. Gijbels et al. derived the following quantile function $F_{\alpha}^{-1}(p)$:

```{=latex}
\begin{equation}
F_{\alpha}^{-1}(p; \mu, \sigma, \beta) =
\begin{cases}
\mu + \frac{\sigma}{1 - \alpha} F_{\beta}^{-1} \left( \frac{p}{2 \alpha}\right) & \text{, } p \leq \alpha \\
\mu + \frac{\sigma}{\alpha} F_{\beta}^{-1} \left( \frac{1 + p - 2 \alpha}{2 \left(1 - \alpha \right)} \right) & \text{, } p > \alpha
\end{cases}
\end{equation}
```
The quantile function for the asymmetric generalised normal distribution $F_{\alpha}^{-1}(p)$ thus incorporates the quantile function $F_{\beta}^{-1}(p^{*})$ of the symmetric generalised normal distribution. $F_{\beta}^{-1}(p^{*})$ was derived by Griffin to be [@Griffin2018-bf]:

```{=latex}
\begin{equation}
F_{\beta}^{-1}(p^{*}) = \sgn\left(p^{*} - 0.5 \right) F_{\Gamma}^{-1}\left(2 \left|p^{*} - 0.5 \right|; 1 / \beta \right)
\end{equation}
```
Here, $F_{\Gamma}^{-1}$ is the quantile function of the gamma distribution with shape $1 / \beta$, which can be numerically approximated. 

## Goodness of fit



# Simulation

## Modified power transformations are invariant to location

To assess whether the modified power transformations lead to $\lambda$ values that are invariant to shifts, we simulate three different distributions. The first distribution is a normal distribution, as previously shown in Figure **INSERT REFERENCE**. We first randomly draw $10000$ values from a normal distribution: $x_{\text{normal}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{N}\left(0, 1\right)$, or equivalently $x_{\text{normal}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{AGN}\left(0, 1/\sqrt{2}, 0.5, 2\right)$. The second distribution is a right-skewed normal distribution $x_{\text{right}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{AGN}\left(0, 1/\sqrt{2}, 0.2, 2\right)$. The third distribution is a left-skewed normal distribution $x_{\text{left}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{AGN}\left(0, 1/\sqrt{2}, 0.8, 2\right)$. We then compute transformation parameter $\lambda$ using the original definitions (Eq. **INSERT REFERENCE** and **INSERT REFERENCE**) and the modified definitions (Eq. **INSERT REFERENCE** and **INSERT REFERENCE**) for each distribution, after adding a positive value $d$ to each distribution. Here, $d \in [1, 10^6]$.

```{r shifted-distributions, echo=FALSE, warning=FALSE, message=FALSE, fig.cap=cap, eval = FALSE}
cap <- paste0(
  "Power transformations modified to be shift-sensitive produce transformation parameters are invariant to location. ")

# Non-standard evaluation
d <- distribution <- method <- mean_lambda <- sd_lambda <- NULL

# Density plot.
.create_density_plot <- function(x, plot_theme){
  p <- ggplot2::ggplot(
    data = data.table::data.table("x"=x),
    mapping = ggplot2::aes(
      x = x))
  p <- p + plot_theme
  p <- p + ggplot2::geom_density()
  p <- p + ggplot2::theme(
    axis.title = ggplot2::element_blank(),
    axis.line = ggplot2::element_blank(),
    axis.ticks = ggplot2::element_blank(),
    axis.text = ggplot2::element_blank(),
    panel.grid = ggplot2::element_blank(),
    panel.border = ggplot2::element_blank(),
    plot.title=ggplot2::element_text(
      hjust=0.5,
      size=ggplot2::rel(1.0)))
  p <- p + ggplot2::xlim(c(-5, 5))
  
  return(p)
}

# Lambda plot,
.create_lambda_shift_plot <- function(data, plot_theme, limits, guide=FALSE){
 
  p <- ggplot2::ggplot(
    data = data,
    mapping = ggplot2::aes(
      x = d,
      y = lambda,
      colour = method,
      shape = version))
  p <- p + plot_theme
  p <- p + ggplot2::geom_point()
  p <- p + ggplot2::scale_x_continuous(
    name = latex2exp::TeX("$d$"),
    labels = scales::math_format())
  p <- p + ggplot2::scale_y_continuous(
    name = latex2exp::TeX("$\\lambda$"),
    limits = limits)
  
  if(guide){
     p <- p + paletteer::scale_color_paletteer_d(
      palette = "ggthemes::Tableau_10",
      drop = FALSE)
     
  } else {
    p <- p + paletteer::scale_color_paletteer_d(
      palette = "ggthemes::Tableau_10",
      drop = FALSE,
      guide = "none")
    p <- p + ggplot2::scale_shape_discrete(guide="none")
  }
  
  return(p)
}

# Generator for creating parameters and data for all experiments.
generate_experiment_data <- coro::generator(
  function(
    x_normal,
    x_right_skewed,
    x_left_skewed){
    
    shift_range <- 10^seq(from=0, to=6, by=0.1)
    
    for(d in shift_range){
      for(distribution in c("normal", "right-skewed", "left-skewed")){
        if(distribution == "normal"){
          x <- x_normal
          
        } else if(distribution == "right-skewed"){
          x <- x_right_skewed
          
        } else if(distribution == "left-skewed"){
          x <- x_left_skewed
        }
        
        for(method in c("box_cox", "yeo_johnson")){
          for(shift in c(FALSE, TRUE)){
            for(robust in c(FALSE)){
            # for(robust in c(FALSE, TRUE)){
              # Only 
              # Skip if robust, but not shifted.
              if(robust && !shift) next
              
              if(!shift && !robust){
                version <- "original"
                
              } else if(!robust){
                version <- "shift-sensitive"
                
              } else {
                version <- "robust shift-sensitive"
              }
              
              yield(list(
                "x" = x + d,
                "d" = d,
                "distribution" = distribution,
                "method" = method,
                "shift" = shift,
                "robust" = robust,
                "version" = version
              ))
            }
          }
        }
      }
    }
  })

.compute_lambda <- function(parameter_set){
  # Create transformer object.
  transformer <- suppressWarnings(
    power.transform::find_transformation_parameters(
      x = parameter_set$x,
      method = parameter_set$method,
      robust = parameter_set$robust,
      shift = parameter_set$shift,
      lambda = NULL))
  
  return(
    data.table::data.table(
      "distribution" = parameter_set$distribution,
      "method" = parameter_set$method,
      "version" = parameter_set$version,
      "d" = log10(parameter_set$d),
      "lambda" = transformer@lambda
    ))
}

#### Computations --------------------------------------------------------------

# Set seed.
set.seed(19L)

# Normal distribution.
x_normal <- power.transform::ragn(10000L, location=0, scale=1/sqrt(2), alpha=0.5, beta=2)

# Right skewed data
x_right_skewed <- power.transform::ragn(10000L, location=0, scale=1/sqrt(2), alpha=0.2, beta=2)

# Left skewed data
x_left_skewed <- power.transform::ragn(10000L, location=0, scale=1/sqrt(2), alpha=0.8, beta=2)

# Generate all experiments.
experiments <- coro::collect(generate_experiment_data(
  x_normal = x_normal,
  x_right_skewed = x_right_skewed,
  x_left_skewed = x_left_skewed
))

# Start cluster
cl <- parallel::makeCluster(16L)

# Compute all data in parallel.
data <- parallel::parLapply(
  cl=cl,
  X=experiments,
  fun=.compute_lambda
)

# Stop cluster.
parallel::stopCluster(cl)

# Combine data into a single table.
data <- data.table::rbindlist(data)

# Update distribution, method and version to factors.
data$distribution <- factor(
  x = data$distribution,
  levels = c("normal", "right-skewed", "left-skewed"))
data$method <- factor(
  x = data$method,
  levels = c("box_cox", "yeo_johnson"),
  labels = c("Box-Cox", "Yeo-Johnson"))
data$version <- factor(
  x = data$version,
  levels = c("original", "shift-sensitive"))


#### Normal distribution -------------------------------------------------------
# Density
p_dens_normal <- .create_density_plot(
  x=x_normal,
  plot_theme=plot_theme)
p_dens_normal <- p_dens_normal + ggplot2::ggtitle(label="normal distribution")

# Box-Cox
p_bc_normal <- .create_lambda_shift_plot(
  data = data[distribution == "normal" & method == "Box-Cox"],
  plot_theme = plot_theme,
  limits = c(0.0, 35.0))

# Yeo-Johnson
p_yj_normal <- .create_lambda_shift_plot(
  data=data[distribution == "normal" & method == "Yeo-Johnson"],
  plot_theme = plot_theme,
  limits = c(0.0, 35.0))

#### Right skewed distribution -------------------------------------------------

# Density
p_dens_right <- .create_density_plot(
  x = x_right_skewed,
  plot_theme = plot_theme)
p_dens_right <- p_dens_right + ggplot2::ggtitle(label="right-skewed distribution")

# Box-Cox
p_bc_right <- .create_lambda_shift_plot(
  data =data[distribution == "right-skewed" & method == "Box-Cox"],
  plot_theme = plot_theme,
  limits = c(-9.0, 1.0))

# Yeo-Johnson
p_yj_right <- .create_lambda_shift_plot(
  data = data[distribution == "right-skewed" & method == "Yeo-Johnson"],
  plot_theme = plot_theme,
  limits = c(-9.0, 1.0))

#### Left skewed distribution --------------------------------------------------

# Density
p_dens_left <- .create_density_plot(
  x = x_left_skewed,
  plot_theme = plot_theme)
p_dens_left <- p_dens_left + ggplot2::ggtitle(label="left-skewed distribution")

# Box-Cox
p_bc_left <- .create_lambda_shift_plot(
  data = data[distribution == "left-skewed" & method == "Box-Cox"],
  plot_theme = plot_theme,
  limits = c(0.0, 60.0),
  guide = TRUE)

# Yeo-Johnson
p_yj_left <- .create_lambda_shift_plot(
  data = data[distribution == "left-skewed" & method == "Yeo-Johnson"],
  plot_theme = plot_theme,
  limits = c(0.0, 60.0))

#### Aggregate -----------------------------------------------------------------

aggregate_data <- data[, list("mean_lambda"=mean(lambda), "sd_lambda"=stats::sd(lambda)), by=c("method", "version", "distribution")]
aggregate_data$distribution <- factor(
  x=aggregate_data$distribution,
  levels = c("normal", "left-skewed", "right-skewed"))
aggregate_data[, "text":=parse_mean_sd_latex(mu=mean_lambda, sigma=sd_lambda)]

p <- egg::ggarrange(
  plots=list(
    p_dens_normal, p_bc_normal, p_yj_normal,
    p_dens_right, p_bc_right, p_yj_right,
    p_dens_left, p_bc_left, p_yj_left),
  ncol=3,
  heights=c(0.5, 1.0, 1.0),
  byrow=FALSE,
  draw=FALSE)
p
```

The result is shown in Figure **INSERT REFERENCE**, and summarised in Table **INSERT REFERENCE**. For each distribution, transformation parameter $\lambda$ varies with $d$ when estimated using the original definitions. In contrast, estimations of $\lambda$ modified power transformations were invariant to $d$.

```{r, echo=FALSE, results="asis", include=FALSE, eval = FALSE}
table_data <- data.table::dcast(
  aggregate_data,
  distribution ~ version + method,
  value.var = "text",
  sep = " ")

knitr::kable(
  table_data,
  caption = "Estimates and deviation of transformation parameter $\\lambda$ under location shifts",
  label = "shifted-distributions-table",
  format = "latex",
  escape = FALSE,
  booktabs = TRUE)
```

## Weighting based on feature values offers best robustness

Outliers may be presented in the data, and affect estimation of power transformation parameters. Such outliers should be down-weighted during the optimisation process. Weights can be assigned using three different data representations: 1. the original feature values; 2. the feature values after power transformation; 3. the residual errors between transformed feature values and their expected values according to the normal distribution.

```{r, echo=FALSE, warning=FALSE, message=FALSE, eval = FALSE}

set.seed(95)

.compute_lambda <- function(x){
  # Create transformer object.
  transformer <- suppressWarnings(
    power.transform::find_transformation_parameters(
      x = x,
      method = "yeo_johnson",
      robust = FALSE,
      shift = TRUE))
  
  return(transformer@lambda)
}

# Generate table of asymmetric generalised normal distribution parameters.
n_distributions <- 100

# Generate alpha, beta and n.
n <- stats::runif(n=n_distributions, min=2, max=4)
n <- round(10^n)

alpha <- stats::runif(n=n_distributions, min=0.01, max=0.99)
beta <- stats::runif(n=n_distributions, min=1.00, max=5.00)

# Generate corresponding distributions.
x <- mapply(
  power.transform::ragn,
  n = n,
  alpha = alpha,
  beta = beta)

# Compute lambda values without weighting for Yeo-Johnson.
target_lambda <- sapply(
  x,
  .compute_lambda)

# Add outliers, between 0.00 and 0.10.
n_outliers <- 10
outlier_fraction <- (seq_len(n_outliers) - 1)^2 /((n_outliers-1)^2 * 10)

# Find optimised k1 and k2 values for all combinations of weight sources and
# weighting functions.
x <- mapply(
  function(x, n, alpha, beta, target_lambda, outlier_fraction){
    
    parameters <- list("n" = n, "alpha"=alpha, "beta" = beta, "target_lambda"=target_lambda)
    
    x <- lapply(
      outlier_fraction,
      function(k, x, parameters){
        # Set parameters.
        parameters$k <- k
        
        # Compute interquartile range.
        interquartile_range <- stats::IQR(x)
        
        # Compute upper and lower quartiles.
        q_lower <- stats::quantile(x, probs=0.25, names=FALSE)
        q_upper <- stats::quantile(x, probs=0.75, names=FALSE)
        
        # Set data where the outliers will be copied into.
        x_outlier <- x
        
        if(k != 0.0){
          n_draw <- ceiling(k * 0.5 * length(x))
          
          # Generate outlier values that are smaller than Q1 - 1.5 IQR or larger
          # than Q3 + 1.5 IQR.
          outlier <- c(
            q_lower - 1.5 * interquartile_range - 3.0 * stats::runif(n_draw),
            q_upper + 1.5 * interquartile_range + 3.0 * stats::runif(n_draw))
          
          # Randomly insert outlier values.
          x_outlier[sample(seq_along(x), size=2*n_draw, replace=FALSE)] <- outlier
        }
        
        return(list(
          "x" = x_outlier,
          "parameters" = parameters))
      },
      x = x,
      parameters)
    
    return(x)
  },
  x = x,
  n = n,
  alpha = alpha,
  beta = beta,
  target_lambda = target_lambda,
  MoreArgs=list("outlier_fraction"=outlier_fraction),
  SIMPLIFY=FALSE,
  USE.NAMES=FALSE)

x <- unlist(x, recursive = FALSE)

# Optimise k1, k2 so that sum of absolute differences with target-lambda is
# minimised for each combination of source and weighting method.
experiment_args <- coro::generator(
  function(){
    
    # Non-robust transformation.
    fun_args <- list(
      "method" = "yeo_johnson",
      "robust" = FALSE,
      "shift" = TRUE)
    
    opt_args <- list()
    
    yield(list(
      "name" = "non-robust",
      "fun_args" = fun_args,
      "opt_args" = opt_args
    ))
    
    for(robustness_source in c("original", "transformed", "residual")){
      
      def_opt_limits <- list("k1"=c(0.0, 10.0), "k2"=c(0.0, 10.0))
      
      if(robustness_source == "original"){
        def_opt_limits$k1[2] <- 1.0
        def_opt_limits$k2[2] <- 1.0
        
        def_opt_init <- list("k1" = 0.8, "k2" = 0.95)
        
      } else if(robustness_source == "transformed"){
        def_opt_init <- list("k1" = 1.5, "k2" = 3.0)
        
      } else if(robustness_source == "residual"){
        def_opt_init <- list("k1" = 0.15, "k2" = 1.0)
      }
      
      for(robustness_weighting_function in c("step", "triangle", "cosine")){
        
        fun_args <- list(
          "method" = "yeo_johnson",
          "robust" = TRUE,
          "shift" = TRUE,
          "weight_method" = paste0(robustness_source, "_", robustness_weighting_function),
          "backup_use_default" = FALSE)
        
        opt_limits <- def_opt_limits
        opt_init <- def_opt_init
        
        # Step-weighting only has k1 as a parameter.
        if(robustness_weighting_function == "step"){
          opt_limits$k2 <- NULL
          opt_init$k2 <- NULL
        }
        
        yield(list(
          "name" = paste0(robustness_source, "-", robustness_weighting_function),
          "fun_args" = fun_args,
          "opt_args" = list("initial" = opt_init, "limits" = opt_limits)
        ))
      }
    }
  })

# Function for computing transformer parameters under optimisation constraints.
compute_robust_lambda <- function(
    x,
    fun_args,
    opt_args,
    cl=NULL){
  
  # Custom parser.
  ..outlier_parser <- function(
    x,
    fun_args,
    opt_args
  ){
    # Create transformer.
    transformer <- suppressWarnings(do.call(
      power.transform::find_transformation_parameters,
      args=c(
        list("x" = x$x),
        fun_args,
        opt_args)))
    
    parameter_data <- c(
      x$parameters,
      opt_args,
      list(
        "weight_method" = fun_args$weight_method,
        "lambda" = transformer@lambda,
        "shift" = transformer@shift)
    )
    
    return(parameter_data)
  }
  
  if(is.null(cl)){
    parameter_data <- lapply(
      x,
      ..outlier_parser,
      fun_args = fun_args,
      opt_args = opt_args)
    
  } else {
    parameter_data <- parallel::parLapply(
      cl = cl,
      X = x,
      fun = ..outlier_parser,
      fun_args = fun_args,
      opt_args = opt_args)
  }
  
  return(parameter_data)
}


# Inner optimisation function that sets the loss.
..optimisation_function <- function(
  opt_param,
  x,
  fun_args,
  cl = NULL){
  
  # Parse options.
  opt_args <- list()
  if(length(opt_param) >= 1) opt_args <- c(opt_args, list("k1"=opt_param[1]))
  if(length(opt_param) >= 2) opt_args <- c(opt_args, list("k2"=opt_param[2]))
  
  parameter_data <- compute_robust_lambda(
    x = x,
    fun_args = fun_args,
    opt_args = opt_args,
    cl = cl)
  
  lambda_error <- sapply(
    parameter_data,
    function(x) (abs(x$lambda - x$target_lambda)))
  
  return(sum(lambda_error))
}


# Outer optimisation function
.optimisation_function <- function(
    experiment,
    x,
    cl = NULL){
  
  # Run optimiser.
  if(length(experiment$opt_args) > 0){
    
    x0 <- unlist(experiment$opt_args$initial)
    
    lower <- experiment$opt_args$limits$k1[1]
    upper <- experiment$opt_args$limits$k1[2]
    
    if(length(experiment$opt_args$limits) > 1){
      lower <- c(lower, experiment$opt_args$limits$k2[1])
      upper <- c(upper, experiment$opt_args$limits$k2[2])
    }
    
    h <- nloptr::sbplx(
      x0 = x0,
      fn = ..optimisation_function,
      lower = lower,
      upper = upper,
      control=list("xtol_rel"=1e-3, "ftol_rel"=1e-4),
      x = x,
      fun_args = experiment$fun_args,
      cl = cl)
    
    if(length(h$par) == 1){
      return(list("name"=experiment$name, "k1" = h$par, "value" = h$value))
      
    } else {
      return(list("name"=experiment$name, "k1" = h$par[1], "k2" = h$par[2], "value" = h$value))
    }
    
  } else {
    value <- ..optimisation_function(
      opt_param=list(),
      x = x,
      fun_args=experiment$fun_args,
      cl = cl)
    
    return(list("name"=experiment$name, "value" = value))
  }
}


# Start cluster
cl <- parallel::makeCluster(16L)

experiment_results <- lapply(
  coro::collect(experiment_args()),
  .optimisation_function,
  x = x,
  cl = cl)

# Stop cluster.
parallel::stopCluster(cl)


```

For robustness simulate:
- A shifted normal distribution without, 0.1%, 0.2%, 0.5%, 1.0%, 2.0%, 5.0%, 10%, and 20% outliers mixed in.
- A shifted left-skewed distribution without, 0.1%, 0.2%, 0.5%, 1.0%, 2.0%, 5.0%, 10%, and 20% outliers mixed in.
- A shifted right-skewed distribution without, 0.1%, 0.2%, 0.5%, 1.0%, 2.0%, 5.0%, 10%, and 20% outliers mixed in.
- To show: lambda for modified, and robust modified Yeo-Johnson distribution for: Raymaekers trimmed, Tukey Window

```{r, eval = FALSE}
# Generator for creating parameters and data for all experiments.
generate_experiment_data <- coro::generator(
  function(
    x_normal,
    x_right_skewed,
    x_left_skewed){
    
    n <- 20
    k_range <- (seq_len(n) - 1)^2 /((n-1)^2 * 5)  
    # k_range <- c(0.0, 0.001, 0.002, 0.005, 0.010, 0.020, 0.050, 0.100, 0.200)
    
    for(distribution in c("normal", "right-skewed", "left-skewed")){
      if(distribution == "normal"){
        x <- x_normal
        
      } else if(distribution == "right-skewed"){
        x <- x_right_skewed
        
      } else if(distribution == "left-skewed"){
        x <- x_left_skewed
      }
      for(ii in seq_len(20)){
        for(k in k_range){
          # Compute interquartile range.
          interquartile_range <- stats::IQR(x)
          
          # Compute upper and lower quartiles.
          q_lower <- stats::quantile(x, probs=0.25, names=FALSE)
          q_upper <- stats::quantile(x, probs=0.75, names=FALSE)
          
          # Set data where the outliers will be copied into.
          x_outlier <- x
          
          if(k != 0.0){
            n_draw <- ceiling(k * 0.5 * length(x))
            
            # Generate outlier values that are smaller than Q1 - 1.5 IQR or larger
            # than Q3 + 1.5 IQR.
            outlier <- c(
              q_lower - 1.5 * interquartile_range - 3.0 * stats::runif(n_draw),
              q_upper + 1.5 * interquartile_range + 3.0 * stats::runif(n_draw))
            
            # Randomly insert outlier values.
            x_outlier[sample(seq_along(x), size=2*n_draw, replace=FALSE)] <- outlier
          }
          
          #for(method in c("box_cox", "yeo_johnson")){
          for(method in c("yeo_johnson")){
            for(weight_method in c("none", "tukey_window", "tapered_step_window")){
            # for(weight_method in c("none", "tukey_window", "step_window", "tapered_step_window", "trim_transformation", "trim_residual", "tukey_biweight_transformation", "tukey_biweight_residual", "huber_weight_transformed", "huber_weight_residual")){
              
              weight_method_name <- weight_method
              
              for(tukey_window_k in c(0.80, 0.60, 0.40, 0.20, 0.00)){
                if(weight_method == "none" && tukey_window_k != 0.00) next
                
                yield(list(
                  "ii" = ii,
                  "x" = x_outlier,
                  "k" = k,
                  "distribution" = distribution,
                  "method" = method,
                  "robust" = weight_method != "none",
                  "weight_method" = weight_method,
                  "weight_method_name" = paste0(weight_method_name, "_k_", tukey_window_k),
                  "tukey_window_k" = tukey_window_k
                ))
              }
              
              
              # if(weight_method == "tukey_window"){
              #   for(tukey_window_k in c(0.9, 0.8, 0.7, 0.6, 0.5)){
              #     yield(list(
              #       "x" = x_outlier,
              #       "k" = k,
              #       "distribution" = distribution,
              #       "method" = method,
              #       "robust" = weight_method != "none",
              #       "weight_method" = weight_method,
              #       "weight_method_name" = paste0(weight_method_name, "_k_", tukey_window_k),
              #       "tukey_window_k" = tukey_window_k
              #     ))
              #   }
              # } else {
              #   yield(list(
              #     "x" = x_outlier,
              #     "k" = k,
              #     "distribution" = distribution,
              #     "method" = method,
              #     "robust" = weight_method != "none",
              #     "weight_method" = weight_method,
              #     "weight_method_name" = weight_method_name
              #   ))
              # }
            }
          }
        }
      }
    }
  })

.compute_lambda <- function(parameter_set){
  # Create transformer object.
  transformer <- suppressWarnings(
    power.transform::find_transformation_parameters(
      x = parameter_set$x,
      method = parameter_set$method,
      robust = parameter_set$robust,
      shift = TRUE,
      weight_method = parameter_set$weight_method,
      k = parameter_set$tukey_window_k,
      lambda = NULL))
  
  return(
    data.table::data.table(
      "distribution" = parameter_set$distribution,
      "method" = parameter_set$method,
      "weight_method" = parameter_set$weight_method,
      "weight_method_name" = parameter_set$weight_method_name,
      "k" = parameter_set$k,
      "lambda" = transformer@lambda,
      "kappa" = parameter_set$tukey_window_k,
      "ii" = parameter_set$ii
    ))
}

# Set seed.
set.seed(19L)

# Normal distribution.
x_normal <- stats::rnorm(10000L)

# Right skewed data
x_right_skewed <- (x_normal - min(x_normal) + 1)^0.1
x_right_skewed <- (x_right_skewed - mean(x_right_skewed)) / sd(x_right_skewed)

# Left skewed data
x_left_skewed <- (x_normal - min(x_normal))^3
x_left_skewed <- (x_left_skewed - mean(x_left_skewed)) / sd(x_left_skewed)

# Generate all experiments.
experiments <- coro::collect(generate_experiment_data(
  x_normal = x_normal,
  x_right_skewed = x_right_skewed,
  x_left_skewed = x_left_skewed
))

# Start cluster
cl <- parallel::makeCluster(16L)

# Compute all data in parallel.
data <- parallel::parLapply(
  cl=cl,
  X=experiments,
  fun=.compute_lambda
)

# Stop cluster.
parallel::stopCluster(cl)

# Combine data into a single table.
data <- data.table::rbindlist(data)

# Update distribution, method and version to factors.
data$distribution <- factor(
  x = data$distribution,
  levels = c("normal", "right-skewed", "left-skewed"))
data$method <- factor(
  x = data$method,
  levels = c("box_cox", "yeo_johnson"),
  labels = c("Box-Cox", "Yeo-Johnson"))
data$kappa <- factor(data$kappa)

# Find target lambda values in the absence of any outliers.
target_data <- data[weight_method=="none" & k == 0.0, mget(c("distribution", "ii", "lambda", "method"))]
data.table::setnames(target_data, old="lambda", new="target_lambda")

# Merge target lambda values into the main data.
data <- data[target_data, on=.NATURAL]

# Compute difference with target lambda.
data[, "diff_lambda":=lambda - target_lambda]

summary_data[, list(deviation=sum(abs(diff_lambda))), by=c("distribution", "weight_method_name")][order(distribution, deviation)]

# Compute average.
new_data <- data[, list(lambda = mean(lambda)), by=c("k", "kappa", "weight_method", "distribution")]

p <- ggplot2::ggplot(
  data = new_data[distribution=="left-skewed"],
  mapping = ggplot2::aes(
    x = k,
    y = lambda,
    colour = kappa,
    linetype = weight_method))
p <- p + plot_theme
# p <- p + ggplot2::geom_point()
p <- p + ggplot2::geom_line()
p <- p + ggplot2::scale_x_continuous(
  name = latex2exp::TeX("$k$"),
  trans = "sqrt")
p <- p + ggplot2::scale_y_continuous(
  name = latex2exp::TeX("$\\lambda$"))

```


# Experimental Results

# Discussion

In their work, Box and Cox already mention a transformation with a shift parameter, but preferred the now well-known version in Eq. **INSERT REFERENCE** for the theoretical analysis in their paper [@Box1964-mz].

# Conclusion

# Code availability

Shift-sensitive power transformations are implemented in the `power.transform` package.

# Appendix A.

# References
