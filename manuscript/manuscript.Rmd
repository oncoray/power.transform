---
title: "Shift-sensitive Power Transformations for Transforming Data to Normality"
author: "Alex Zwanenburg"
date: "2023-01-06"
output:
  pdf_document:
    latex_engine: lualatex
bibliography: refs.bib
header-includes:
- \usepackage{amsmath}
- \usepackage{booktabs}
- \DeclareMathOperator*{\argmax}{argmax}
- \DeclareMathOperator*{\argmin}{argmin}
- \DeclareMathOperator{\sgn}{sgn}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  fig.align= "center")

# Allow for defining figure captions within a chunk.
knitr::opts_knit$set(
  eval.after = "fig.cap")

require(power.transform)
require(ggplot2)
require(data.table)
require(latex2exp)
require(coro)

manuscript_dir <- file.path("C:/Users/alexz/Documents/GitHub/power.transform/manuscript")
source(file.path(manuscript_dir, "simulations.R"))

# Base font size.
base_size <- 10

# Set general theme.
plot_theme <- ggplot2::theme_light(base_size=base_size)

get_annotation_settings <- function(ggtheme=NULL){
  # Import formatting settings from the provided ggtheme.
  
  # Find the text size for the table. This is based on text sizes in the
  # ggtheme.
  fontsize <- ggtheme$text$size
  fontsize_rel <- 1.0
  
  # Attempt to base the text size on the general axis.text attribute.
  if(!is.null(ggtheme$axis.text$size)){
    if(inherits(ggtheme$axis.text$size, "rel")){
      # Find the relative text size of axis text.
      fontsize_rel <- as.numeric(ggtheme$axis.text$size)
      
    } else {
      # Set absolute text size.
      fontsize <- ggtheme$axis.text$size
      fontsize_rel <- 1.0
    }
  }
  
  # Attempt to refine the text size using the axis.text.y attribute in
  # particular.
  if(!is.null(ggtheme$axis.text.y$size)){
    if(inherits(ggtheme$axis.text.y$size, "rel")){
      # Set relative text size of axis text
      fontsize_rel <- as.numeric(ggtheme$axis.text.y$size)
      
    } else {
      # Set absolute text size.
      fontsize <- as.numeric(ggtheme$axis.text.y$size)
      fontsize_rel <- 1.0
    }
  }
  
  # Update the text size using the magical ggplot2 point size (ggplot2:::.pt).
  geom_text_size <- fontsize * fontsize_rel / 2.845276
  
  # Obtain lineheight
  lineheight <- ggtheme$text$lineheight
  if(!is.null(ggtheme$axis.text$lineheight)) lineheight <- ggtheme$axis.text$lineheight
  if(!is.null(ggtheme$axis.text.y$lineheight)) lineheight <- ggtheme$axis.text.y$lineheight
  
  # Obtain family
  fontfamily <- ggtheme$text$family
  if(!is.null(ggtheme$axis.text$family)) fontfamily <- ggtheme$axis.text$family
  if(!is.null(ggtheme$axis.text.y$family)) fontfamily <- ggtheme$axis.text.y$family
  if(!is.null(ggtheme$axis.text.x$family)) fontfamily <- ggtheme$axis.text.x$family
  
  # Obtain face
  fontface <- ggtheme$text$face
  if(!is.null(ggtheme$axis.text$face)) fontface <- ggtheme$axis.text$face
  if(!is.null(ggtheme$axis.text.y$face)) fontface <- ggtheme$axis.text.y$face
  if(!is.null(ggtheme$axis.text.x$face)) fontface <- ggtheme$axis.text.x$face
  
  # Obtain colour
  colour <- ggtheme$text$colour
  if(!is.null(ggtheme$axis.text$colour)) colour <- ggtheme$axis.text$colour
  if(!is.null(ggtheme$axis.text.y$colour)) colour <- ggtheme$axis.text.y$colour
  if(!is.null(ggtheme$axis.text.x$colour)) colour <- ggtheme$axis.text.x$colour
  
  return(list(
    "geom_text_size"=geom_text_size,
    "fontsize"=fontsize,
    "fontsize_rel"=fontsize_rel,
    "colour"=colour,
    "family"=fontfamily,
    "face"=fontface,
    "lineheight"=lineheight))
}

parse_mean_sd_latex <- function(mu, sigma, digits=2L){
  return(paste0(
    "$", round(mu, digits=digits), " \\pm ", round(sigma, digits=digits), "$"
  ))
}

```

# Abstract

# Introduction

Numerical variables in datasets may strongly deviate from normal distributions, e.g. by being skewed. This may complicate further analysis. Power transformations [@Tukey1957-rt] can help improve normality of such features. The two most commonly used transformations are that of @Box1964-mz and @Yeo2000-vw. The Box-Cox transformation of a feature value $x_i$ of feature $\mathbf{X}=\left\{x_1, x_2, \ldots, x_n \right\}$ under the transformation parameter $\lambda$ is defined as:

```{=latex}
\begin{equation}
\phi_{\text{BC}}^\lambda (x_i) = 
\begin{cases}
\left(x_i^\lambda - 1 \right) / \lambda & \text{if } \lambda \neq 0\\
\log(x_i) & \text{if } \lambda = 0
\end{cases}
\end{equation}
```
One limitation of th Box-Cox transformation is that it is only defined for $x_i > 0$. In contrast, the Yeo-Johnson transformation under the transformation parameter $\lambda$ is defined for any $x_i \in \mathbb{R}$:

```{=latex}
\begin{equation}
\phi_{\text{YJ}}^\lambda (x_i) = 
\begin{cases}
\left( \left( 1 + x_i \right)^\lambda - 1\right) / \lambda & \text{if } \lambda \neq 0 \text{ and } x_i \geq 0\\
\log(1 + x_i) & \text{if } \lambda = 0 \text{ and } x_i \geq 0\\
-\left( \left( 1 - x_i\right)^{2 - \lambda} - 1 \right) / \left(2 - \lambda \right) & \text{if } \lambda \neq 2 \text{ and } x_i < 0\\
-\log(1 - x_i) & \text{if } \lambda = 2 \text{ and } x_i < 0
\end{cases}
\end{equation}
```
The $\lambda$-parameter is typically optimised using maximum likelihood estimation under the assumption that the transformed feature is normally distributed. As noted by Raymaekers and Rousseeuw, this approach is very sensitive to the presence of outliers, and robust versions of Box-Cox and Yeo-Johnson transformations were devised [@Raymaekers2021-kq].

Power transformation does not guarantee that transformed variables are normally distributed. In fact, depending on location and scale of the variable, power transformation may reduce normality, as shown in Figure **INSERT**. This is problematic for automated power transformations, for three reasons. First, normality is often desired in machine learning applications. Second, a large negative or positive $\lambda$-parameter may lead to numeric issues. Third, we currently do not have a test to automatically reject poor transformations. Statistical tests for normality exist, such as the Shapiro-Wilk test [@Shapiro1965-zd]. However, given sufficiently large sample sizes, these tests can detect trivial deviations from normality and are therefore not well-suited for applications of automated power transformations in practice.

```{r reduced-normality, echo=FALSE, fig.cap=cap, warning=FALSE}
cap <- paste0(
  "Power transformations may reduce normality depending on location. ",
  "Variable $x$ consists of $10000$ values are randomly drawn from a normal distribution $\\mathcal{N}(\\mu, 1)$. ",
  "For both Box-Cox and Yeo-Johnson transformations, we would expect $\\lambda = 1$, since $x$ is already normally distributed. ",
  "The figure shows that instead, $\\lambda$ is strongly dependent on the location $\\mu$, ",
  "and even breaks down due to numerical issues for $\\mu > 5 10^4$.")

compute_lambda <- function(mu, x, method){
  # Create transformer object.
  transformer <- suppressWarnings(
    power.transform::find_transformation_parameters(
      x = x + mu,
      method = method,
      robust = FALSE,
      shift = FALSE,
      lambda = NULL,
      optimiser_control = list("xtol_rel"=1e-3)))
  
  # Return lambda value.
  return(transformer@lambda)
}

# Non-standard evaluation
mu <- lambda <- method <- NULL

# Set seed.
set.seed(19L)

x <- power.transform::ragn(10000L, location=0, scale=1/sqrt(2), alpha=0.5, beta=2)
shift_range <- 10^seq(from=0, to=6, by=0.1)

box_cox_values <- sapply(
  shift_range,
  compute_lambda,
  x = x,
  method = "box_cox")

yeo_johnson_values <- sapply(
  shift_range,
  compute_lambda,
  x = x,
  method = "yeo_johnson")

data <- data.table::data.table(
  "mu" = log10(c(shift_range, shift_range)),
  "lambda" = c(box_cox_values, yeo_johnson_values),
  "method" = factor(
    x=rep(
      c("Box-Cox", "Yeo-Johnson"),
      each = length(shift_range)),
    levels=c("Box-Cox", "Yeo-Johnson")))

annotation_settings <- get_annotation_settings(ggplot2::theme_light(base_size = base_size))

p_bc <- ggplot2::ggplot(
  data = data[method == "Box-Cox"],
  mapping = ggplot2::aes(
    x = mu,
    y = lambda,
    colour = method))
p_bc <- p_bc + plot_theme
p_bc <- p_bc + ggplot2::geom_point()
p_bc <- p_bc + ggplot2::geom_hline(
  yintercept = 1.0, 
  linetype="longdash", 
  colour="grey40")
p_bc <- p_bc + ggplot2::annotate(
  geom = "text",
  x = 6,
  y = 1,
  label = "expected",
  colour=annotation_settings$colour,
  family=annotation_settings$family,
  fontface=annotation_settings$face,
  size=annotation_settings$geom_text_size,
  vjust=-1.0,
  hjust=1.0)
p_bc <- p_bc + ggplot2::scale_x_continuous(
  name = latex2exp::TeX("$\\mu$"),
  labels = scales::math_format())
p_bc <- p_bc + ggplot2::scale_y_continuous(
  name = latex2exp::TeX("$\\lambda$"),
  limits = c(0.0, 35.0))
p_bc <- p_bc + paletteer::scale_color_paletteer_d(
  palette="ggthemes::Tableau_10",
  drop=FALSE,
  guide="none")

p_yj <- ggplot2::ggplot(
  data = data[method == "Yeo-Johnson"],
  mapping = ggplot2::aes(
    x = mu,
    y = lambda,
    colour = method))
p_yj <- p_yj + plot_theme
p_yj <- p_yj + ggplot2::geom_point()
p_yj <- p_yj + ggplot2::geom_hline(
  yintercept = 1.0,
  linetype="longdash",
  colour="grey40")
p_yj <- p_yj + ggplot2::annotate(
  geom = "text",
  x = 6,
  y = 1,
  label = "expected",
  colour=annotation_settings$colour,
  family=annotation_settings$family,
  fontface=annotation_settings$face,
  size=annotation_settings$geom_text_size,
  vjust=-1.0,
  hjust=1.0)
p_yj <- p_yj + ggplot2::scale_x_continuous(
  name = latex2exp::TeX("$\\mu$"),
  labels = scales::math_format())
p_yj <- p_yj + ggplot2::scale_y_continuous(
  name = latex2exp::TeX("$\\lambda$"),
  limits = c(0.0, 35.0))
p_yj <- p_yj + paletteer::scale_color_paletteer_d(
  palette="ggthemes::Tableau_10",
  drop=FALSE)
p_yj <- p_yj + ggplot2::theme(
  axis.title.y=ggplot2::element_blank(),
  axis.text.y=ggplot2::element_blank())

p <- egg::ggarrange(plots=list(p_bc, p_yj), ncol=2, draw=FALSE)
p
```

Here we make the following contributions:

-   We devise shift-sensitive versions of the Box-Cox and Yeo-Johnson transformation, including versions robust to outliers.

-   We define an empirical measure of the goodness of fit for detecting cases when power transformations fail to yield a normally distributed transformed feature.

-   We assess the effect of power transformations on the performance of machine learning models.

# Methods

## Shift-sensitive power transformation

Box-Cox and Yeo-Johnson transformations are modified by adding a shift parameter $x_0$. The shift-sensitive Box-Cox transformation of a feature value $x_i$ of feature $\mathbf{X}$ under transformation parameter $\lambda$ and shift parameter $x_0$ is then:

```{=latex}
\begin{equation}
\phi_{\text{BC}}^{\lambda, x_0} (x_i) = 
\begin{cases}
\left( \left(x_i - x_0 \right)^\lambda - 1 \right) / \lambda & \text{if } \lambda \neq 0\\
\log(x_i - x_0) & \text{if } \lambda = 0
\end{cases}
\end{equation}
```
Where $x_i - x_0 > 0$. Likewise, the shift-sensitive Yeo-Johnson transformation of a feature value $x_i$ under transformation parameter $\lambda$ and shift parameter $x_0$ is:

```{=latex}
\begin{equation}
\phi_{\text{YJ}}^{\lambda, x_0} (x_i) = 
\begin{cases}
\left( \left( 1 + x_i - x_0\right)^\lambda - 1\right) / \lambda & \text{if } \lambda \neq 0 \text{ and } x_i - x_0 \geq 0\\
\log(1 + x_i - x_0) & \text{if } \lambda = 0 \text{ and } x_i - x_0 \geq 0\\
-\left( \left( 1 - x_i - x_0\right)^{2 - \lambda} - 1 \right) / \left(2 - \lambda \right) & \text{if } \lambda \neq 2 \text{ and } x_i - x_0< 0\\
-\log(1 - x_i - x_0) & \text{if } \lambda = 2 \text{ and } x_i - x_0 < 0
\end{cases}
\end{equation}
```
For both shift-sensitive transformations, $\lambda$ and $x_0$ are obtained by maximising the normal log-likelihood function. For the shift-sensitive Box-Cox transformation, and ignoring constant terms, this can be written as:

```{=latex}
\begin{equation}
\left\{ \hat{\lambda}^{\text{BC}}, \hat{x}_0^{\text{BC}} \right\} = \argmax_{\lambda, x_0} \sum_{i=1}^n -\frac{1}{2}\log(\sigma^2) + (\lambda - 1) \log(x_i - x_0)
\end{equation}
```
where $\sigma^2$ is the variance of the Box-Cox transformed feature $\phi_{\text{BC}}^{\lambda, x_0} (\mathbf{X})$. Similarly for the shift-sensitive Yeo-Johnson transformation:

```{=latex}
\begin{equation}
\left\{ \hat{\lambda}^{\text{YJ}}, \hat{x}_0^{\text{YJ}} \right\} = \argmax_{\lambda, x_0} \sum_{i=1}^n -\frac{1}{2}\log(\sigma^2) + (\lambda - 1) \sgn(x_i - x_0) \log(1 + |x_i - x_0|)
\end{equation}
```
where $\sigma^2$ is the variance of Yeo-Johnson transformed feature $\phi_{\text{YJ}}^{\lambda, x_0} (\mathbf{X})$.



## Robust shift-sensitive power transformations

Real-world data may contain outliers, to which maximum likelihood estimation can be sensitive. Their presence may lead to poor transformations to normality, as shown in Figure **INSERT**. As indicated by @Raymaekers2021-kq, the general aim of power transformations should be to transform non-outlier data to normality, i.e. achieve *central normality*. To achieve this, they devised an iterative procedure to find a robust estimate of the transformation parameter $\lambda$. Because $\lambda$ is updated during their procedure, simultaneous estimation of $\lambda$ and $x_0$ for shift-sensitive power transformations is not possible.

In essence, obtaining robust transformation parameters that provide central normality requires identifying outliers in the data and weighting such instances during the optimisation process. @Raymaekers2021-kq do this (during the final steps of their iterative procedure) through a weighted maximum likelihood estimation. This weighted maximum log-likelihood estimator is based on equations **INSERT**. Here, the estimator is further normalised by the total weight and extended to include the shift parameter. The shift-sensitive Box-Cox power transformation is:

```{=latex}
\begin{equation}
\left\{ \hat{\lambda}^{\text{rBC}}, \hat{x}_0^{\text{rBC}} \right\} = \argmax_{\lambda, x_0} \frac{1}{\sum_{i=1}^n w_u} \sum_{i=1}^n w_i \left[ -\frac{1}{2}\log(\sigma^2_w) + (\lambda - 1) \log(x_i - x_0) \right]
\end{equation}
```
where $\sigma^2_w$ is the weighted variance of the Box-Cox transformed feature $\phi_{\text{BC}}^{\lambda, x_0} (\mathbf{X})$:

```{=latex}
\begin{equation}
\sigma_w^2 = \frac{\sum_{i=1}^n w_i \left(\phi_{\text{BC}}^{\lambda, x_0} (x_i) - \mu_w \right)^2}{\sum_{i=1}^n w_i} \quad \text{with } \mu_w = \frac{\sum_{i=1}^n \phi_{\text{BC}}^{\lambda, x_0} (x_i)} {\sum_{i=1}^n w_i}
\end{equation}
```
Analogously, the weighted maximum log-likelihood estimator for the shift-sensitive Yeo-Johnson power transformation is:

```{=latex}
\begin{equation}
\left\{ \hat{\lambda}^{\text{rYJ}}, \hat{x}_0^{\text{rYJ}} \right\} = \argmax_{\lambda, x_0} \frac{1}{\sum_{i=1}^n w_u} \sum_{i=1}^n w_i \left[ -\frac{1}{2}\log(\sigma^2_w) + (\lambda - 1) \sgn(x_i - x_0) \log(1 + |x_i - x_0|) \right]
\end{equation}
```
where $\sigma^2_w$ is the weighted variance of the Yeo-Johnson transformed feature $\phi_{\text{YJ}}^{\lambda, x_0} (\mathbf{X})$:

```{=latex}
\begin{equation}
\sigma_w^2 = \frac{\sum_{i=1}^n w_i \left(\phi_{\text{YJ}}^{\lambda, x_0} (x_i) - \mu_w \right)^2}{\sum_{i=1}^n w_i} \quad \text{with } \mu_w = \frac{\sum_{i=1}^n \phi_{\text{YJ}}^{\lambda, x_0} (x_i)} {\sum_{i=1}^n w_i}
\end{equation}
```

There are a few options based on which weights may be set. Here, we investigate three:

- Set weights based on probabilities of the empirical distribution of the original feature $\mathbf{X}$. Probabilities are determined as $p_i = \frac{i - 1/3}{n + 1/3}$, with $i = 1, 2, \ldots n$, with $n$ the number of instances of feature $\mathbf{X}$.

- Set weights based on z-score of the transformed feature $\phi^{\lambda, x_0} (\mathbf{X})$. After @Raymaekers2021-kq, $z_i = \frac{\phi^{\lambda, x_0}(x_i) - \mu_M}{\sigma_M}$. Here, $\mu_M$ and $\sigma_M$ are robust Huber M-estimates of location and scale of the transformed feature $\phi^{\lambda, x_0} (\mathbf{X})$ [@Huber1981-su].

- Set weights based on the residual error between the z-score of the transformed feature $\phi^{\lambda, x_0} (\mathbf{X})$ and the theoretical z-score from a standard normal distribution: $r_i =\left| \left( \phi^{\lambda, x_0}(x_i) - \mu_M)\right) / \sigma_M - F^{-1}_{\mathcal{N}}(p_i) \right|$, with $\mu_M$, $\sigma_M$ and $p_i$ as defined above.

Based on either of the above, weights are set using a weighting function. We will investigate the following functions, where $\dot{x}_i$ can be either $p^{*}_i=2 \left( p_i - 0.5\right)$ (so that the quantiles are zero-centred), $z_i$ or $r_i$:

- A step function, with $k_1 \geq 0$ as threshold parameter:
```{=latex}
\begin{equation}
w_i =
\begin{cases}
1 & \text{if } \left| \dot{x}_i \right| \leq k_1\\
0 & \text{if } \left| \dot{x}_i \right| > k_1
\end{cases}
\end{equation}
```

- A triangle function (or generalised Huber weight), with $k_1 \geq 0$ and $k_2 \geq k_1$ as threshold parameters:
```{=latex}
\begin{equation}
w_i =
\begin{cases}
1 & \text{if } \left| \dot{x}_i \right| < k_1\\
1 - \frac{\left| \dot{x}_i \right| - k_1}{k_2 - k_1} & \text{if } k_1 \leq \left| \dot{x}_i \right| \leq k_2 \\
0 & \text{if } \left| \dot{x}_i \right| > k_2
\end{cases}
\end{equation}
```

- A tapered cosine function [@Tukey1967-eb], with $k_1 \geq 0$ and $k_2 \geq k_1$ as threshold parameters:
```{=latex}
\begin{equation}
w_i =
\begin{cases}
1 & \text{if } \left| \dot{x}_i \right| < k_1\\
0.5 + 0.5 \cos\left(\pi \frac{\left| \dot{x}_i \right| - k_1}{k_2 - k_1} \right) & \text{if } k_1 \leq \left| \dot{x}_i \right| \leq k_2 \\
0 & \text{if } \left| \dot{x}_i \right| > k_2
\end{cases}
\end{equation}
```

All weighting functions share the characteristic that for $\dot{x}_i < k_1$, instances are fully weighted, i.e. when $k_1 > 0$ the weighting functions are symmetric window functions with a flat top. The triangle and tapered cosine functions then gradually down-weight instances with $k_1 \leq \dot{x}_i \leq k_2$, and assign no weight to instances $x_i > k_2$.

```{r weight-functions, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "Weighting functions investigated in this study to make power transformations more robust against outliers. ",
  "The step function was parameterised with $k_1 = 0.60$. ",
  "The triangle and tapered cosine functions were both parameterised with $k_1 = 0.30$ and $k_2 = 0.90$.")

# Non-standard evaluation
d <- distribution <- method <- mean_lambda <- sd_lambda <- weight <- NULL

data_step <- data.table::data.table(x = seq(from=-1.0, to=1.0, by=0.001))
data_step[, "weight":=power.transform:::..weight_function_external(x, "step", k1=0.60)]

p_step <- ggplot2::ggplot(
  data = data_step,
  mapping = ggplot2::aes(
    x = x,
    y = weight))
p_step <- p_step + plot_theme
p_step <- p_step + ggplot2::geom_line()
p_step <- p_step + ggplot2::ggtitle(label="step")
p_step <- p_step + ggplot2::theme(
  plot.title=ggplot2::element_text(
    hjust=0.5,
    size=ggplot2::rel(1.0)))

data_triangle <- data.table::data.table(x = seq(from=-1.0, to=1.0, by=0.001))
data_triangle[, "weight":=power.transform:::..weight_function_external(x, "triangle", k1=0.30, k2=0.90)]

p_triangle <- ggplot2::ggplot(
  data = data_triangle,
  mapping = ggplot2::aes(
    x = x,
    y = weight))
p_triangle <- p_triangle + plot_theme
p_triangle <- p_triangle + ggplot2::geom_line()
p_triangle <- p_triangle + ggplot2::ggtitle(label="triangle")
p_triangle <- p_triangle + ggplot2::theme(
  plot.title=ggplot2::element_text(
    hjust=0.5,
    size=ggplot2::rel(1.0)),
  axis.text.y = ggplot2::element_blank(),
  axis.title.y = ggplot2::element_blank())

data_cosine <- data.table::data.table(x = seq(from=-1.0, to=1.0, by=0.001))
data_cosine[, "weight":=power.transform:::..weight_function_external(x, "cosine", k1=0.30, k2=0.90)]

p_cosine <- ggplot2::ggplot(
  data = data_cosine,
  mapping = ggplot2::aes(
    x = x,
    y = weight))
p_cosine <- p_cosine + plot_theme
p_cosine <- p_cosine + ggplot2::geom_line()
p_cosine <- p_cosine + ggplot2::ggtitle(label="tapered cosine")
p_cosine <- p_cosine + ggplot2::theme(
  plot.title = ggplot2::element_text(
    hjust = 0.5,
    size = ggplot2::rel(1.0)),
  axis.text.y = ggplot2::element_blank(),
  axis.title.y = ggplot2::element_blank())

p <- egg::ggarrange(plots=list(p_step, p_triangle, p_cosine), ncol=3, draw=FALSE)
p
```

## Asymmetric generalised normal distributions

Modifications intended to make power transformations invariant to shifts and methods to improve their robustness against outliers need to be assessed using data drawn from a range of different distributions. Since the power transformations are intended for use with unimodal distributions, the generalised normal distribution [@Subbotin1923-qk; @Nadarajah2005-xe] is a suitable option to simulate realistic feature distributions. This distribution has the following probability density function $f_{\beta}$ for a value $x \in \mathbb{R}$:

```{=latex}
\begin{equation}
f_{\beta}(x) = \frac{\beta}{2\Gamma \left(1 / \beta \right)} e^{-\left| x \right|^\beta}
\end{equation}
```
Here, $\Gamma$ is the gamma function, and $\beta$ is a strictly positive shape parameter. For $\beta = 1$, the probability density function describes a Laplace distribution. A normal distribution is found for $\beta=2$, and for large $\beta$, the distribution approaches a uniform distribution. We will refrain from introducing scale and location parameters here directly.

Realistic feature distributions may be skewed. Gijbels et al. describe a recipe for introducing skewness into the otherwise symmetric generalised norm distribution [@Gijbels2019-te], leading to the following probability density function:

```{=latex}
\begin{equation}
f_{\alpha}(x; \mu, \sigma, \beta) = \frac{2 \alpha \left(1 - \alpha\right)}{\sigma}
\begin{cases}
f_{\beta}\left( \left(1 - \alpha \right) \frac{\left| x - \mu \right|}{\sigma} \right) & \text{, } x \leq \mu \\
f_{\beta}\left( \alpha \frac{\left| x - \mu \right|}{\sigma} \right) & \text{, } x > \mu
\end{cases}
\end{equation}
```
Here $\alpha \in (0,1)$ is a skewness parameter. $\alpha > 0.5$ creates a distribution with a negative skew, i.e. a left-skewed distribution. A right-skewed distribution is created for $\alpha < 0.5$. $\mu \in \mathcal{R}$ and $\sigma \in (0, \infty)$ are location and scale parameters, respectively. $f_{\alpha}$ thus describes the probability density function of an asymmetric generalised normal distribution, which we will refer to here and parametrise as $\mathcal{AGN}\left(\mu, \sigma, \alpha, \beta \right)$.

We require a quantile function (or an approximation thereof) to draw random values from an asymmetric generalised normal distribution using inverse transform sampling. Gijbels et al. derived the following quantile function $F_{\alpha}^{-1}(p)$:

```{=latex}
\begin{equation}
F_{\alpha}^{-1}(p; \mu, \sigma, \beta) =
\begin{cases}
\mu + \frac{\sigma}{1 - \alpha} F_{\beta}^{-1} \left( \frac{p}{2 \alpha}\right) & \text{, } p \leq \alpha \\
\mu + \frac{\sigma}{\alpha} F_{\beta}^{-1} \left( \frac{1 + p - 2 \alpha}{2 \left(1 - \alpha \right)} \right) & \text{, } p > \alpha
\end{cases}
\end{equation}
```
The quantile function for the asymmetric generalised normal distribution $F_{\alpha}^{-1}(p)$ thus incorporates the quantile function $F_{\beta}^{-1}(p^{*})$ of the symmetric generalised normal distribution. $F_{\beta}^{-1}(p^{*})$ was derived by Griffin to be [@Griffin2018-bf]:

```{=latex}
\begin{equation}
F_{\beta}^{-1}(p^{*}) = \sgn\left(p^{*} - 0.5 \right) F_{\Gamma}^{-1}\left(2 \left|p^{*} - 0.5 \right|; 1 / \beta \right)
\end{equation}
```
Here, $F_{\Gamma}^{-1}$ is the quantile function of the gamma distribution with shape $1 / \beta$, which can be numerically approximated. 

## Goodness of fit

To be added.

# Simulation

## Shift-sensitive transformations are location-invariant

To assess whether the modified power transformations lead to $\lambda$ values that are invariant to shifts, we simulate three different distributions. The first distribution is a normal distribution, as previously shown in Figure **INSERT**.
We first randomly draw $10000$ values from a normal distribution: $\mathbf{X}_{\text{normal}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{N}\left(0, 1\right)$, or equivalently $\mathbf{X}_{\text{normal}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{AGN}\left(0, 1/\sqrt{2}, 0.5, 2\right)$.
The second distribution is a right-skewed normal distribution $\mathbf{X}_{\text{right}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{AGN}\left(0, 1/\sqrt{2}, 0.2, 2\right)$.
The third distribution is a left-skewed normal distribution $\mathbf{X}_{\text{left}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{AGN}\left(0, 1/\sqrt{2}, 0.8, 2\right)$.
We then compute transformation parameter $\hat{\lambda}$ using the original definitions (Eq. **INSERT** and **INSERT**) and the shift-sensitive definitions (Eq. **INSERT** and **INSERT**) for each distribution, after adding a positive value $d$ to each distribution. Here, $d \in [1, 10^6]$.

```{r shifted-distributions, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "Shift-sensitive power transformation produces transformation parameters that are invariant to location. ",
  "Samples were drawn from normal, right-skewed and left-skewed distributions, respectively,  which then underwent a shift $d$. ",
  "Estimates of transformation parameter $\\lambda$ with the original power transformations (circle) show strong dependency on the overall location of the distribution, ",
  "whereas estimates with the shift-sensitive power transformations (triangle) are constant.")

# Non-standard evaluation
d <- distribution <- method <- mean_lambda <- sd_lambda <- estimation_method <- NULL

# Density plot.
.create_density_plot <- function(x, plot_theme, limits=c(-5, 5)){
  p <- ggplot2::ggplot(
    data = data.table::data.table("x"=x),
    mapping = ggplot2::aes(
      x = x))
  p <- p + plot_theme
  p <- p + ggplot2::geom_density()
  p <- p + ggplot2::theme(
    axis.title = ggplot2::element_blank(),
    axis.line = ggplot2::element_blank(),
    axis.ticks = ggplot2::element_blank(),
    axis.text = ggplot2::element_blank(),
    panel.grid = ggplot2::element_blank(),
    panel.border = ggplot2::element_blank(),
    plot.title=ggplot2::element_text(
      hjust=0.5,
      size=ggplot2::rel(1.0)))
  p <- p + ggplot2::xlim(limits)
  
  return(p)
}

# Lambda plot,
.create_lambda_shift_plot <- function(data, plot_theme, limits, guide=FALSE){
  
  p <- ggplot2::ggplot(
    data = data,
    mapping = ggplot2::aes(
      x = d,
      y = lambda,
      colour = method,
      shape = version))
  p <- p + plot_theme
  p <- p + ggplot2::geom_point()
  p <- p + ggplot2::scale_x_continuous(
    name = latex2exp::TeX("$d$"),
    labels = scales::math_format())
  p <- p + ggplot2::scale_y_continuous(
    name = latex2exp::TeX("$\\lambda$"),
    limits = limits)
  
  if(guide){
    p <- p + paletteer::scale_color_paletteer_d(
      palette = "ggthemes::Tableau_10",
      drop = FALSE)
    
  } else {
    p <- p + paletteer::scale_color_paletteer_d(
      palette = "ggthemes::Tableau_10",
      drop = FALSE,
      guide = "none")
    p <- p + ggplot2::scale_shape_discrete(guide="none")
  }
  
  return(p)
}

# Set seed.
set.seed(19L)

# Normal distribution.
x_normal <- power.transform::ragn(10000L, location=0, scale=1/sqrt(2), alpha=0.5, beta=2)

# Right skewed data
x_right_skewed <- power.transform::ragn(10000L, location=0, scale=1/sqrt(2), alpha=0.2, beta=2)

# Left skewed data
x_left_skewed <- power.transform::ragn(10000L, location=0, scale=1/sqrt(2), alpha=0.8, beta=2)

# Load data.
data <- .get_shifted_distribution_data(manuscript_dir=manuscript_dir)
data[estimation_method == "MLE"]

#### Normal distribution -------------------------------------------------------
# Density
p_dens_normal <- .create_density_plot(
  x = x_normal,
  plot_theme = plot_theme)
p_dens_normal <- p_dens_normal + ggplot2::ggtitle(label="normal distribution")

# Box-Cox
p_bc_normal <- .create_lambda_shift_plot(
  data = data[distribution == "normal" & method == "Box-Cox"],
  plot_theme = plot_theme,
  limits = c(0.0, 35.0))

# Yeo-Johnson
p_yj_normal <- .create_lambda_shift_plot(
  data=data[distribution == "normal" & method == "Yeo-Johnson"],
  plot_theme = plot_theme,
  limits = c(0.0, 35.0))

#### Right skewed distribution -------------------------------------------------

# Density
p_dens_right <- .create_density_plot(
  x = x_right_skewed,
  plot_theme = plot_theme)
p_dens_right <- p_dens_right + ggplot2::ggtitle(label="right-skewed distribution")

# Box-Cox
p_bc_right <- .create_lambda_shift_plot(
  data = data[distribution == "right-skewed" & method == "Box-Cox"],
  plot_theme = plot_theme,
  limits = c(-9.0, 1.0))

# Yeo-Johnson
p_yj_right <- .create_lambda_shift_plot(
  data = data[distribution == "right-skewed" & method == "Yeo-Johnson"],
  plot_theme = plot_theme,
  limits = c(-9.0, 1.0))

#### Left skewed distribution --------------------------------------------------

# Density
p_dens_left <- .create_density_plot(
  x = x_left_skewed,
  plot_theme = plot_theme)
p_dens_left <- p_dens_left + ggplot2::ggtitle(label="left-skewed distribution")

# Box-Cox
p_bc_left <- .create_lambda_shift_plot(
  data = data[distribution == "left-skewed" & method == "Box-Cox"],
  plot_theme = plot_theme,
  limits = c(0.0, 60.0),
  guide = TRUE)

# Yeo-Johnson
p_yj_left <- .create_lambda_shift_plot(
  data = data[distribution == "left-skewed" & method == "Yeo-Johnson"],
  plot_theme = plot_theme,
  limits = c(0.0, 60.0))

#### Aggregate -----------------------------------------------------------------

aggregate_data <- data[, list("mean_lambda"=mean(lambda), "sd_lambda"=stats::sd(lambda)), by=c("method", "version", "distribution")]
aggregate_data$distribution <- factor(
  x=aggregate_data$distribution,
  levels = c("normal", "left-skewed", "right-skewed"))
aggregate_data[, "text":=parse_mean_sd_latex(mu=mean_lambda, sigma=sd_lambda)]

p <- egg::ggarrange(
  plots=list(
    p_dens_normal, p_bc_normal, p_yj_normal,
    p_dens_right, p_bc_right, p_yj_right,
    p_dens_left, p_bc_left, p_yj_left),
  ncol=3,
  heights=c(0.5, 1.0, 1.0),
  byrow=FALSE,
  draw=FALSE)
p
```

The result is shown in Figure **INSERT**, and summarised in Table **INSERT**. For each distribution, transformation parameter $\hat{\lambda}$ varies with $d$ when estimated using the original definitions. In contrast, estimation of $\lambda$ using shift-sensitive power transformations was invariant to $d$.

```{r, results="asis"}
table_data <- data.table::dcast(
  aggregate_data,
  distribution ~ version + method,
  value.var = "text",
  sep = " ")

data.table::setnames(
  table_data,
  old = c("original Box-Cox", "original Yeo-Johnson", "shift-sensitive Box-Cox", "shift-sensitive Yeo-Johnson"),
  new = c("orig. Box-Cox", "orig. Yeo-Johnson", "shift-sens. Box-Cox", "shift-sens. Yeo-Johnson"))

knitr::kable(
  table_data,
  align = c("l", "c", "c", "c", "c"),
  caption = paste0(
    "Estimates and variability of transformation parameter $\\lambda$ under location shifts. ",
    "Whereas the original (orig.) power transformations show considerable variability, ",
    "whereas for shift-sensitive (shift-sens.) power transformations variability in the value of $\\lambda$ is negligible."),
  label = "shifted-distributions-table",
  format = "latex",
  escape = FALSE,
  booktabs = TRUE)
```

## Empirical distribution yields consistent robust transformations

Outliers may be presented in the data, and affect estimation of power transformation parameters.
The log-likelihood function is weighted to assign less weight to outlier instances.
Weighting is based on either probabilities of the empirical distribution of the original feature, the z-score of the transformed feature values, or the residual error between the z-score of the transformed feature values and their expected z-score based on the normal distribution.
A weight is then assigned to each instance using one of three weighting functions: step, triangle and tapered cosine.
The weighting functions have one (step) or two (triangle, tapered cosine) parameters.
Here we investigate how these parameters should be set to find transformation parameter $\hat{\lambda}^r$ that are robust to outliers.

Let us randomly draw $m_d=100$ asymmetric generalised normal distributions. Each distribution is parametrised with a randomly chosen skewness parameter $\alpha \sim U\left(0.01, 0.99\right)$ and shape parameter $\beta \sim U\left(1.00, 5.00 \right)$. Location and scale parameters are set as $\mu = 0$ and $\sigma = 1$, respectively. $n = \lceil 10^\gamma \rceil$ values are then randomly drawn, with $\gamma \sim U\left(2, 4\right)$. That is between $100$ and $10000$ values are drawn to create $\mathbf{X}_i$. 

Outlier values are then drawn to randomly replace a fraction of the values of $\mathbf{X}_i$.
This is repeated $m_{out} = 10$ times, with outlier fractions regularly spaced in $[0.00, 0.10]$.
Thus up to 10 percent of the values may be replaced by outliers.
Outlier values are set according to @Tukey1977-xm.
Let $x^{*} \sim U\left(-2, 2\right)$.
Then the corresponding outlier value is:

```{=latex}
\begin{equation}
x_{out} =
\begin{cases}
Q_1 - \left(1.5 - x^{*} \right) \text{IQR} & \text{if } x^{*} < 0 \\
Q_3 + \left(1.5 + x^{*} \right) \text{IQR} & \text{if } x^{*} \geq 0
\end{cases}
\end{equation}
```

Here, $Q_1$, $Q_3$ and $\text{IQR}$ are the first quartile, third quartile and interquartile range of $\mathbf{X}_i$, respectively.
Outlier values randomly replace values in $\mathbf{X}_i$ to create $\mathbf{X}_{i,j}$.

To find the optimal values for the weighting function parameters $k_1$ and $k_2$ (if applicable), we minimise the absolute difference between the $\hat{\lambda}^r$ parameter obtained for transformation in the presence of outliers, and the $\hat{\lambda}_0$ parameter obtained using the non-robust modified transformation in absence of outliers:

```{=latex}
\begin{equation}
\left\{ \hat{k}_1, \hat{k}_2 \right\} = \argmin_{k_1, k_2} \sum_{i=1}^{m_d} \sum_{j=1}^{m_{out}} \left| \hat{\lambda}^r \left(\mathbf{X}_{i, j}; k_1, k_2 \right) - \hat{\lambda}_{0} \left(\mathbf{X}_i \right) \right|
\end{equation}
```
The double sum in right-hand side of the above equation can be considered as the loss that should be minimised.
Minimisation is performed using the `sbplx` algorithm implemented in the `nloptr` package for R [@Rowan1990-sn; @Ypma2022-tc].

```{r, echo=FALSE, warning=FALSE, message=FALSE, eval=FALSE}
two_sided_function_parameters <- .get_optimised_weighting_function_parameters_two_sided(manuscript_dir = manuscript_dir)

```



```{r, echo=FALSE, warning=FALSE, message=FALSE, eval=FALSE}
two_sided_transformation_parameters <- .get_transformation_parameters_two_sided(manuscript_dir = manuscript_dir)

```



```{=latex}
\begin{table}
\begin{center}
\caption{Optimal weighting parameters and corresponding loss for \textbf{shift-sensitive Box-Cox} power transformations. $p^{*}$ indicates use of the empirical distribution of feature values, $z$ the z-score of the transformed feature values, and $r$ the residual error between the z-score of transformed feature values and the expected z-score according to the normal distribution. The \textit{initial} column shows the starting parameter value for the optimisation process, with the corresponding boundary values in the \textit{limits} column. The {optimal} column shows the optimal parameter values. The \textit{loss} column shows the loss achieved by each method, under optimised parameters. Lower loss indicates better robustness against outliers.}
\begin{tabular}{l r r r r r r r}

\toprule
method & \multicolumn{3}{c}{$k_1$} & \multicolumn{3}{c}{$k_2$} & loss \\
& initial & limits & optimal & initial & limits & optimal & \\

\midrule
non-robust               & ---  & ---       & ---  & ---  & ---       & ---  & 783 \\
$p^{*}$ (step)           & 0.80 & $(0, 1]$  & 0.84 & ---  & ---       & ---  & 587 \\
$p^{*}$ (triangle)       & 0.80 & $(0, 1]$  & 0.78 & 0.95 & $(0, 1]$  & 0.99 & 587 \\
$p^{*}$ (tapered cosine) & 0.80 & $(0, 1]$  & 0.76 & 0.95 & $(0, 1]$  & 0.98 & 583 \\
$z$ (step)               & 1.28 & $(0, 10]$ & 1.09 & ---  & ---       & ---  & 1135 \\
$z$ (triangle)           & 1.28 & $(0, 10]$ & 0.26 & 1.96 & $(0, 10]$ & 5.47 & 1813 \\
$z$ (tapered cosine)     & 1.28 & $(0, 10]$ & 0.03 & 1.96 & $(0, 10]$ & 6.91 & 1844 \\
$r$ (step)               & 0.50 & $(0, 10]$ & 1.56 & ---  & ---       & ---  & 1837 \\
$r$ (triangle)           & 0.50 & $(0, 10]$ & 1.49 & 1.00 & $(0, 10]$ & 1.51 & 1877 \\
$r$ (tapered cosine)     & 0.50 & $(0, 10]$ & 1.32 & 1.00 & $(0, 10]$ & 1.65 & 1834 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}
```


```{=latex}
\begin{table}
\begin{center}
\caption{Optimal weighting parameters and corresponding loss for \textbf{shift-sensitive Yeo-Johnson} power transformations. $p^{*}$ indicates use of the empirical distribution of feature values, $z$ the z-score of the transformed feature values, and $r$ the residual error between the z-score of transformed feature values and the expected z-score according to the normal distribution. The \textit{initial} column shows the starting parameter value for the optimisation process, with the corresponding boundary values in the \textit{limits} column. The {optimal} column shows the optimal parameter values. The \textit{loss} column shows the loss achieved by each method, under optimised parameters. Lower loss indicates better robustness against outliers.}
\begin{tabular}{l r r r r r r r}

\toprule
method & \multicolumn{3}{c}{$k_1$} & \multicolumn{3}{c}{$k_2$} & loss \\
& initial & limits & optimal & initial & limits & optimal & \\

\midrule
non-robust               & ---  & ---       & ---   & ---  & ---       & ---   & 225 \\
$p^{*}$ (step)           & 0.80 & $(0, 1]$  & 0.92  & ---  & ---       & ---   & 125 \\
$p^{*}$ (triangle)       & 0.80 & $(0, 1]$  & 0.88  & 0.95 & $(0, 1]$  & 0.93  & 126 \\
$p^{*}$ (tapered cosine) & 0.80 & $(0, 1]$  & 0.89  & 0.95 & $(0, 1]$  & 0.93  & 124 \\
$z$ (step)               & 1.28 & $(0, 10]$ & 1.04  & ---  & ---       & ---   & 333 \\
$z$ (triangle)           & 1.28 & $(0, 10]$ & 0.12  & 1.96 & $(0, 10]$ & 9.91  & 620 \\
$z$ (tapered cosine)     & 1.28 & $(0, 10]$ & 0.23  & 1.96 & $(0, 10]$ & 5.63  & 716 \\
$r$ (step)               & 0.50 & $(0, 10]$ & 10.00 & ---  & ---       & ---   & 485 \\
$r$ (triangle)           & 0.50 & $(0, 10]$ & 9.96  & 1.00 & $(0, 10]$ & 10.00 & 483 \\
$r$ (tapered cosine)     & 0.50 & $(0, 10]$ & 9.97  & 1.00 & $(0, 10]$ & 10.00 & 484 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}
```


Table **INSERT** shows weighting parameters and corresponding loss, after optimisation. Loss without any mitigation of outliers was $225$. Only weighting based on empirical probabilities ($p^(*)$) achieved better loss under optimised weighting parameters. For these data, all three window functions achieved a similar loss. Weighting based on the z-score of the transformed feature, or on their residual error, performed worse than non-robust shift-sensitive power transformation. Thus weighting of outliers should not be based on either of these two options. The likely reason for their poor performance is that they allow for a direct interaction with estimation of the $\lambda$ parameter. Because they both use transformed feature values, weights actually vary during the optimisation process for maximising the weighted log-likelihood function. This complicates the parameter search space, and leads to different parameter estimates. Weighting based on empirical probabilities does not suffer from this issue, because all weights are fixed during optimising the weighted log-likelihood function.
For weighting based on the z-score of the transformed feature, or on their residual error, we can see two extreme strategies emerging. Firstly, the strategy to down-weight most instances ($r$ (step), and to a lesser degree $z$ (step)) and thereby force normality on the most central parts of the distribution. Second is an attempt to assign full-weight to almost all instances, with large values for $k_1$ and $k_2$, which leads to behaviour similar to non-robust power transformation.

```{r, warning=FALSE, message=FALSE}
# Only process data if the file has not been created.
if(!file.exists(file.path(manuscript_dir, "robustness_comparison_plot.RDS"))){
  
  # Generator for creating parameters and data for all experiments.
  generate_experiment_data <- coro::generator(
    function(
    x_normal,
    x_right_skewed,
    x_left_skewed){
      
      n <- 20
      k_range <- (seq_len(n) - 1)^2 /((n-1)^2 * 10)  
      
      for(distribution in c("normal", "right-skewed", "left-skewed")){
        if(distribution == "normal"){
          x <- x_normal
          
        } else if(distribution == "right-skewed"){
          x <- x_right_skewed
          
        } else if(distribution == "left-skewed"){
          x <- x_left_skewed
        }
        for(ii in seq_len(100)){
          for(k in k_range){
            # Compute interquartile range.
            interquartile_range <- stats::IQR(x)
            
            # Compute upper and lower quartiles.
            q_lower <- stats::quantile(x, probs=0.25, names=FALSE)
            q_upper <- stats::quantile(x, probs=0.75, names=FALSE)
            
            # Set data where the outliers will be copied into.
            x_outlier <- x
            
            if(k != 0.0){
              n_draw <- ceiling(k * length(x))
              
              # Generate outlier values that are smaller than Q1 - 1.5 IQR or larger
              # than Q3 + 1.5 IQR.
              x_random <- stats::runif(n_draw, min=-2.0, max=2.0)
              outlier <- numeric(n_draw)
              if(any(x_random < 0)){
                outlier[x_random < 0] <- q_lower - 1.5 * interquartile_range + x_random[x_random < 0] * interquartile_range
              }
              
              if(any(x_random >= 0)){
                outlier[x_random >= 0] <- q_upper + 1.5 * interquartile_range + x_random[x_random >= 0] * interquartile_range
              }
              
              # Randomly insert outlier values.
              x_outlier[sample(seq_along(x), size=n_draw, replace=FALSE)] <- outlier
            }
            
            for(method in c("box_cox", "yeo_johnson")){
              for(weight_method in c("none", "original_step", "original_cosine", "transformed_step", "transformed_cosine", "residual_step", "residual_cosine")){
                
                k1 <- k2 <- NULL
                
                # Optimal parameter values from the previous experiment.
                if(weight_method == "original_step"){
                  k1 <- ifelse(method == "box_cox", 0.84, 0.92)
                  
                } else if(weight_method == "original_cosine"){
                  k1 <- ifelse(method == "box_cox", 0.76, 0.89)
                  k2 <- ifelse(method == "box_cox", 0.98, 0.93)
                  
                } else if(weight_method == "transformed_step"){
                  k1 <- ifelse(method == "box_cox", 1.09, 1.04)
                  
                } else if(weight_method == "transformed_cosine"){
                  k1 <- ifelse(method == "box_cox", 0.03, 0.12)
                  k2 <- ifelse(method == "box_cox", 6.91, 5.63)
                  
                } else if(weight_method == "residual_step"){
                  k1 <- ifelse(method == "box_cox", 1.56, 10.00)
                  
                } else if(weight_method == "residual_cosine"){
                  k1 <- ifelse(method == "box_cox", 1.32, 9.97)
                  k2 <- ifelse(method == "box_cox", 1.65, 10.0)
                }
                
                yield(list(
                  "ii" = ii,
                  "x" = x_outlier,
                  "k" = k,
                  "distribution" = distribution,
                  "method" = method,
                  "robust" = weight_method != "none",
                  "weight_method" = weight_method,
                  "k1" = k1,
                  "k2" = k2))
              }
            }
          }
        }
      }
    })
  
  .compute_lambda <- function(parameter_set){
    # Create transformer object.
    transformer <- suppressWarnings(
      power.transform::find_transformation_parameters(
        x = parameter_set$x,
        method = parameter_set$method,
        robust = parameter_set$robust,
        shift = TRUE,
        weight_method = parameter_set$weight_method,
        k1 = parameter_set$k1,
        k2 = parameter_set$k2))
    
    return(
      data.table::data.table(
        "distribution" = parameter_set$distribution,
        "method" = parameter_set$method,
        "weight_method" = parameter_set$weight_method,
        "k" = parameter_set$k,
        "lambda" = transformer@lambda,
        "ii" = parameter_set$ii))
  }
  
  
  # Computations ---------------------------------------------------------------
  
  # Set seed.
  set.seed(19L)
  
  # Normal distribution.
  x_normal <- power.transform::ragn(10000L, location=0, scale=1/sqrt(2), alpha=0.5, beta=2)
  
  # Right skewed data
  x_right_skewed <- power.transform::ragn(10000L, location=0, scale=1/sqrt(2), alpha=0.2, beta=2)
  
  # Left skewed data
  x_left_skewed <- power.transform::ragn(10000L, location=0, scale=1/sqrt(2), alpha=0.8, beta=2)
  
  # Generate all experiments.
  experiments <- coro::collect(generate_experiment_data(
    x_normal = x_normal,
    x_right_skewed = x_right_skewed,
    x_left_skewed = x_left_skewed
  ))
  
  # Start cluster
  cl <- parallel::makeCluster(18L)
  
  # Compute all data in parallel.
  data <- parallel::parLapply(
    cl=cl,
    X=experiments,
    fun=.compute_lambda
  )
  
  # Stop cluster.
  parallel::stopCluster(cl)
  
  # Combine data into a single table.
  data <- data.table::rbindlist(data)
  
  saveRDS(data, file.path(manuscript_dir, "robustness_comparison_plot.RDS"))
  
}
```

```{r outlier-values_yeo-johnson, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "Estimated $\\lambda$ values for \\textbf{shift-sensitive Yeo-Johnson} power transformations as a function of the fraction of outlier values. ",
  "Features were randomly drawn from centered, right-skewed and left-skewed normal distributions, and a fraction of values was randomly replaced by outlier values.",
  "The top row shows weighting methods that perform better than the non-robust version, ",
  "namely empirical probabilities weighted using step (emp. prob. (step)) and tapered cosine (emp. prob. (tap. cos.)) weighting functions. ",
  "The bottom row shows weighting methods that perform worse than the non-robust version. ",
  "Note that the range of $\\lambda$ values differs between top and bottom rows.")

# Prevent warnings due to non-standard evaluation.
weight_method <- distribution <- k <- NULL

.create_outlier_fraction_plot <- function(
    data,
    plot_theme,
    limits,
    lambda_target,
    guide=FALSE){
  
  # Prevent warnings due to non-standard evaluation.
  weight_method <- k <- mean_lambda <- min_lambda <- max_lambda <- NULL
  
  # Annotation settings.
  annotation_settings <- get_annotation_settings(ggplot2::theme_light(base_size = base_size))
  
  p <- ggplot2::ggplot(
    data = data,
    mapping = ggplot2::aes(
      x = k,
      y = mean_lambda,
      ymin = min_lambda,
      ymax = max_lambda))
  p <- p + plot_theme
  
  p <- p + ggplot2::geom_ribbon(mapping=ggplot2::aes(fill=weight_method), alpha=0.10)
  p <- p + ggplot2::geom_line(mapping=ggplot2::aes(colour=weight_method))
  p <- p + ggplot2::scale_x_continuous(
    name = latex2exp::TeX("outlier fraction"),
    trans = "sqrt")
  p <- p + ggplot2::scale_y_continuous(
    name = latex2exp::TeX("$\\lambda$"),
    limits = limits)
  
  p <- p + ggplot2::geom_hline(
    yintercept = lambda_target, 
    linetype="longdash", 
    colour="grey40")
  
  p <- p + ggplot2::annotate(
    geom = "text",
    x = 0.1,
    y = lambda_target,
    label = "target",
    colour=annotation_settings$colour,
    family=annotation_settings$family,
    fontface=annotation_settings$face,
    size=annotation_settings$geom_text_size,
    vjust=-1.0,
    hjust=1.0)
  
  if(guide){
    p <- p + paletteer::scale_color_paletteer_d(
      palette="ggthemes::Tableau_10",
      drop=FALSE,
      guide=ggplot2::guide_legend("weighting method"))
    
    p <- p + paletteer::scale_fill_paletteer_d(
      palette="ggthemes::Tableau_10",
      drop=FALSE,
      guide=ggplot2::guide_legend("weighting method"))
    
  } else {
    p <- p + paletteer::scale_color_paletteer_d(
      palette="ggthemes::Tableau_10",
      drop=FALSE,
      guide="none")
    
    p <- p + paletteer::scale_fill_paletteer_d(
      palette="ggthemes::Tableau_10",
      drop=FALSE,
      guide="none")
  }
  
  p <- p + ggplot2::scale_alpha(guide="none")
  
  # Update title.
  p <- p + ggplot2::theme(
    plot.title=ggplot2::element_text(
      hjust=0.5,
      size=ggplot2::rel(1.0)))
  
  return(p)
}

data <- readRDS(file.path(manuscript_dir, "robustness_comparison_plot.RDS"))

# Update distribution, method and version to factors.
data$distribution <- factor(
  x = data$distribution,
  levels = c("normal", "right-skewed", "left-skewed"))
data$method <- factor(
  x = data$method,
  levels = c("box_cox", "yeo_johnson"),
  labels = c("Box-Cox", "Yeo-Johnson"))
data$weight_method <- factor(
  x = data$weight_method,
  levels = c("none", "original_step", "original_cosine", "transformed_step", "transformed_cosine", "residual_step", "residual_cosine"),
  labels = c("non-robust", "emp. prob. (step)", "emp. prob. (tap. cos.)", "z-score (step)", "z-score (tap. cos.)", "residual (step)", "residual (tap. cos.)"))

data <- data[method == "Yeo-Johnson", list(
  "mean_lambda"=mean(lambda),
  "min_lambda"=min(lambda),
  "max_lambda"=max(lambda)),
  by=c("distribution", "method", "weight_method", "k")]

top_group <- c("non-robust", "emp. prob. (step)", "emp. prob. (tap. cos.)")
bottom_group <- c("z-score (step)", "z-score (tap. cos.)", "residual (step)", "residual (tap. cos.)")

top_limits <- c(0.2, 1.8)
bottom_limits <- c(-2.5, 4.0)

# Normal distribution ----------------------------------------------------------
p_normal_t <- .create_outlier_fraction_plot(
  data = data[distribution == "normal" & weight_method %in% top_group],
  plot_theme = plot_theme,
  limits = top_limits,
  lambda_target = data[distribution == "normal" & weight_method == "non-robust" & k == 0.0]$mean_lambda)
p_normal_t <- p_normal_t + ggplot2::ggtitle(label="normal distribution")
p_normal_t <- p_normal_t + ggplot2::theme(
  axis.title.x = ggplot2::element_blank(),
  axis.text.x = ggplot2::element_blank())

p_normal_b <- .create_outlier_fraction_plot(
  data = data[distribution == "normal" & weight_method %in% bottom_group],
  plot_theme = plot_theme,
  limits = bottom_limits,
  lambda_target = data[distribution == "normal" & weight_method == "non-robust" & k == 0.0]$mean_lambda)
p_normal_b <- p_normal_b + ggplot2::theme(
  axis.text.x = ggplot2::element_text(
    angle=60.0,
    hjust=1.0))

# Right skewed distribution ----------------------------------------------------
p_right_t <- .create_outlier_fraction_plot(
  data = data[distribution == "right-skewed" & weight_method %in% top_group],
  plot_theme = plot_theme,
  limits= top_limits,
  lambda_target = data[distribution == "right-skewed" & weight_method == "non-robust" & k == 0.0]$mean_lambda)
p_right_t <- p_right_t + ggplot2::ggtitle(label="right-skewed distribution")
p_right_t <- p_right_t + ggplot2::theme(
  axis.title.x = ggplot2::element_blank(),
  axis.text.x = ggplot2::element_blank(),
  axis.title.y = ggplot2::element_blank(),
  axis.text.y = ggplot2::element_blank())

p_right_b <- .create_outlier_fraction_plot(
  data = data[distribution == "right-skewed" & weight_method %in% bottom_group],
  plot_theme = plot_theme,
  limits = bottom_limits,
  lambda_target = data[distribution == "right-skewed" & weight_method == "non-robust" & k == 0.0]$mean_lambda)
p_right_b <- p_right_b + ggplot2::theme(
  axis.title.y = ggplot2::element_blank(),
  axis.text.y = ggplot2::element_blank(),
  axis.text.x = ggplot2::element_text(
    angle=60.0,
    hjust=1.0))

# Left skewed distribution -----------------------------------------------------
p_left_t <- .create_outlier_fraction_plot(
  data = data[distribution == "left-skewed" & weight_method %in% top_group],
  plot_theme = plot_theme,
  limits = top_limits,
  lambda_target = data[distribution == "left-skewed" & weight_method == "non-robust" & k == 0.0]$mean_lambda,
  guide = TRUE)
p_left_t <- p_left_t + ggplot2::ggtitle(label="left-skewed distribution")
p_left_t <- p_left_t + ggplot2::theme(
  axis.title.x = ggplot2::element_blank(),
  axis.text.x = ggplot2::element_blank(),
  axis.title.y = ggplot2::element_blank(),
  axis.text.y = ggplot2::element_blank())

p_left_b <- .create_outlier_fraction_plot(
  data = data[distribution == "left-skewed" & weight_method %in% bottom_group],
  plot_theme = plot_theme,
  limits = bottom_limits,
  lambda_target = data[distribution == "left-skewed" & weight_method == "non-robust" & k == 0.0]$mean_lambda)
p_left_b <- p_left_b + ggplot2::theme(
  axis.title.y = ggplot2::element_blank(),
  axis.text.y = ggplot2::element_blank(),
  axis.text.x = ggplot2::element_text(
    angle=60.0,
    hjust=1.0))

# Aggregate --------------------------------------------------------------------

p <- egg::ggarrange(
  plots=list(
    p_normal_t, p_right_t, p_left_t,
    p_normal_b, p_right_b, p_left_b),
  ncol=3,
  heights=c(1.0, 1.0),
  byrow=TRUE,
  draw=FALSE)
p
```

```{r outlier-values-box-cox, warning=FALSE, message=FALSE, fig.cap=cap}

data <- readRDS(file.path(manuscript_dir, "robustness_comparison_plot.RDS"))

cap <- paste0(
  "Estimated $\\lambda$ values for \\textbf{shift-sensitive Box-Cox} power transformations as a function of the fraction of outlier values. ",
  "Features were randomly drawn from centered, right-skewed and left-skewed normal distributions, and a fraction of values was randomly replaced by outlier values.",
  "The top row shows weighting methods that perform better than the non-robust version, ",
  "namely empirical probabilities weighted using step (emp. prob. (step)) and tapered cosine (emp. prob. (tap. cos.)) weighting functions. ",
  "The bottom row shows weighting methods that perform worse than the non-robust version. ",
  "Note that the range of $\\lambda$ values differs between top and bottom rows.")

# Update distribution, method and version to factors.
data$distribution <- factor(
  x = data$distribution,
  levels = c("normal", "right-skewed", "left-skewed"))
data$method <- factor(
  x = data$method,
  levels = c("box_cox", "yeo_johnson"),
  labels = c("Box-Cox", "Yeo-Johnson"))
data$weight_method <- factor(
  x = data$weight_method,
  levels = c("none", "original_step", "original_cosine", "transformed_step", "transformed_cosine", "residual_step", "residual_cosine"),
  labels = c("non-robust", "emp. prob. (step)", "emp. prob. (tap. cos.)", "z-score (step)", "z-score (tap. cos.)", "residual (step)", "residual (tap. cos.)"))

data <- data[method == "Box-Cox", list(
  "mean_lambda"=mean(lambda),
  "min_lambda"=min(lambda),
  "max_lambda"=max(lambda)),
  by=c("distribution", "method", "weight_method", "k")]

top_group <- c("non-robust", "emp. prob. (step)", "emp. prob. (tap. cos.)")
bottom_group <- c("z-score (step)", "z-score (tap. cos.)", "residual (step)", "residual (tap. cos.)")

top_limits <- c(-3, 4.5)
bottom_limits <- c(-4.01, 6.01)

# Normal distribution ----------------------------------------------------------
p_normal_t <- .create_outlier_fraction_plot(
  data = data[distribution == "normal" & weight_method %in% top_group],
  plot_theme = plot_theme,
  limits = top_limits,
  lambda_target = data[distribution == "normal" & weight_method == "non-robust" & k == 0.0]$mean_lambda)
p_normal_t <- p_normal_t + ggplot2::ggtitle(label="normal distribution")
p_normal_t <- p_normal_t + ggplot2::theme(
  axis.title.x = ggplot2::element_blank(),
  axis.text.x = ggplot2::element_blank())

p_normal_b <- .create_outlier_fraction_plot(
  data = data[distribution == "normal" & weight_method %in% bottom_group],
  plot_theme = plot_theme,
  limits = bottom_limits,
  lambda_target = data[distribution == "normal" & weight_method == "non-robust" & k == 0.0]$mean_lambda)
p_normal_b <- p_normal_b + ggplot2::theme(
  axis.text.x = ggplot2::element_text(
    angle=60.0,
    hjust=1.0))

# Right skewed distribution ----------------------------------------------------
p_right_t <- .create_outlier_fraction_plot(
  data = data[distribution == "right-skewed" & weight_method %in% top_group],
  plot_theme = plot_theme,
  limits= top_limits,
  lambda_target = data[distribution == "right-skewed" & weight_method == "non-robust" & k == 0.0]$mean_lambda)
p_right_t <- p_right_t + ggplot2::ggtitle(label="right-skewed distribution")
p_right_t <- p_right_t + ggplot2::theme(
  axis.title.x = ggplot2::element_blank(),
  axis.text.x = ggplot2::element_blank(),
  axis.title.y = ggplot2::element_blank(),
  axis.text.y = ggplot2::element_blank())

p_right_b <- .create_outlier_fraction_plot(
  data = data[distribution == "right-skewed" & weight_method %in% bottom_group],
  plot_theme = plot_theme,
  limits = bottom_limits,
  lambda_target = data[distribution == "right-skewed" & weight_method == "non-robust" & k == 0.0]$mean_lambda)
p_right_b <- p_right_b + ggplot2::theme(
  axis.title.y = ggplot2::element_blank(),
  axis.text.y = ggplot2::element_blank(),
  axis.text.x = ggplot2::element_text(
    angle=60.0,
    hjust=1.0))

# Left skewed distribution -----------------------------------------------------
p_left_t <- .create_outlier_fraction_plot(
  data = data[distribution == "left-skewed" & weight_method %in% top_group],
  plot_theme = plot_theme,
  limits = top_limits,
  lambda_target = data[distribution == "left-skewed" & weight_method == "non-robust" & k == 0.0]$mean_lambda,
  guide = TRUE)
p_left_t <- p_left_t + ggplot2::ggtitle(label="left-skewed distribution")
p_left_t <- p_left_t + ggplot2::theme(
  axis.title.x = ggplot2::element_blank(),
  axis.text.x = ggplot2::element_blank(),
  axis.title.y = ggplot2::element_blank(),
  axis.text.y = ggplot2::element_blank())

p_left_b <- .create_outlier_fraction_plot(
  data = data[distribution == "left-skewed" & weight_method %in% bottom_group],
  plot_theme = plot_theme,
  limits = bottom_limits,
  lambda_target = data[distribution == "left-skewed" & weight_method == "non-robust" & k == 0.0]$mean_lambda)
p_left_b <- p_left_b + ggplot2::theme(
  axis.title.y = ggplot2::element_blank(),
  axis.text.y = ggplot2::element_blank(),
  axis.text.x = ggplot2::element_text(
    angle=60.0,
    hjust=1.0))

# Aggregate --------------------------------------------------------------------

p <- egg::ggarrange(
  plots=list(
    p_normal_t, p_right_t, p_left_t,
    p_normal_b, p_right_b, p_left_b),
  ncol=3,
  heights=c(1.0, 1.0),
  byrow=TRUE,
  draw=FALSE)
p

```

## Empirical goodness of fit test

To answer:
1. What is the residual threshold and the volume required to avoid rejecting transformations? Optimise: 1. window; 2. threshold; 3. threshold fraction. ? Create a figure that summarises residual error thresholds (50%, 80%, 90%, 95%, 99%, 100%), below which x% of the data falls.
2. How does this correspond to distance between bimodal normal distributions?

```{r goodness-of-fit, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "CHECK. ")

residual <- residual_error <- threshold <- NULL

if(!file.exists(file.path(manuscript_dir, "residual_plot.RDS"))){
  
  set.seed(95)
  
  # Generate table of asymmetric generalised normal distribution parameters.
  n_distributions <- 1000
  
  # Generate alpha, beta and n.
  n <- stats::runif(n=n_distributions, min=2, max=4)
  n <- ceiling(10^n)
  
  alpha <- stats::runif(n=n_distributions, min=0.01, max=0.99)
  beta <- stats::runif(n=n_distributions, min=1.00, max=5.00)
  
  # Generate corresponding distributions.
  x <- mapply(
    power.transform::ragn,
    n = n,
    alpha = alpha,
    beta = beta)
  
  # Add outliers, between 0.00 and 0.10.
  n_outliers <- 10
  outlier_fraction <- (seq_len(n_outliers) - 1)^2 /((n_outliers-1)^2 * 10)
  
  x <- mapply(
    function(x, n, alpha, beta, outlier_fraction){
      
      parameters <- list("n" = n, "alpha"=alpha, "beta" = beta)
      
      x <- lapply(
        outlier_fraction,
        function(k, x, parameters){
          # Set parameters.
          parameters$k <- k
          
          # Compute interquartile range.
          interquartile_range <- stats::IQR(x)
          
          # Compute upper and lower quartiles.
          q_lower <- stats::quantile(x, probs=0.25, names=FALSE)
          q_upper <- stats::quantile(x, probs=0.75, names=FALSE)
          
          # Set data where the outliers will be copied into.
          x_outlier <- x
          
          if(k != 0.0){
            n_draw <- ceiling(k * length(x))
            
            # Generate outlier values that are smaller than Q1 - 1.5 IQR or larger
            # than Q3 + 1.5 IQR.
            x_random <- stats::runif(n_draw, min=-2.0, max=2.0)
            outlier <- numeric(n_draw)
            if(any(x_random < 0)){
              outlier[x_random < 0] <- q_lower - 1.5 * interquartile_range + x_random[x_random < 0] * interquartile_range
            }
            
            if(any(x_random >= 0)){
              outlier[x_random >= 0] <- q_upper + 1.5 * interquartile_range + x_random[x_random >= 0] * interquartile_range
            }
            
            # Randomly insert outlier values.
            x_outlier[sample(seq_along(x), size=n_draw, replace=FALSE)] <- outlier
          }
          
          return(list(
            "x" = x_outlier,
            "parameters" = parameters))
        },
        x = x,
        parameters)
      
      return(x)
    },
    x = x,
    n = n,
    alpha = alpha,
    beta = beta,
    MoreArgs=list("outlier_fraction"=outlier_fraction),
    SIMPLIFY=FALSE,
    USE.NAMES=FALSE)
  
  x <- unlist(x, recursive = FALSE)
  x <- mapply(
    function(ii, x){
      x$distribution_id <- ii
      
      return(x)},
    ii = seq_along(x),
    x = x,
    SIMPLIFY = FALSE)
  
  compute_residuals <- function(
    x,
    n_sample = 200){
    
    # Set interpolation points.
    p_sample <- (seq_len(n_sample) - 1/3) / (n_sample + 1/3)
    
    # Determine residuals for Box-Cox transformations.
    transformer_bc <- suppressWarnings(power.transform::find_transformation_parameters(
      x = x$x,
      method = "box_cox",
      shift = TRUE,
      robust = TRUE,
      weight_method = "original_cosine",
      k1 = 0.76,
      k2 = 0.95))
    
    residual_data <- power.transform::get_residuals(x = x$x, transformer = transformer_bc)
    
    residual <- stats::approx(
      x = residual_data$p,
      y = abs(residual_data$residual),
      xout = p_sample,
      rule = 2,
      ties = max)$y
    
    residual_bc <- data.table::data.table(
      "distribution_id" = x$distribution_id,
      "p" = p_sample,
      "residual" = residual,
      "method" = "box_cox",
      "has_outliers" = x$parameters$k > 0)
    
    # Determine residuals for Yeo-Johnson transformations.
    transformer_yj <- suppressWarnings(power.transform::find_transformation_parameters(
      x = x$x,
      method = "yeo_johnson",
      shift = TRUE,
      robust = TRUE,
      weight_method = "original_cosine",
      k1 = 0.89,
      k2 = 0.93))
    
    residual_data <- power.transform::get_residuals(x = x$x, transformer = transformer_yj)
    
    residual <- stats::approx(
      x = residual_data$p,
      y = abs(residual_data$residual),
      xout = p_sample,
      rule = 2,
      ties = max)$y
    
    residual_yj <- data.table::data.table(
      "distribution_id" = x$distribution_id,
      "p" = p_sample,
      "residual" = residual,
      "method" = "yeo_johnson",
      "has_outliers" = x$parameters$k > 0)
    
    return(data.table::rbindlist(list(residual_bc, residual_yj)))
  }
  
  # Start cluster
  cl <- parallel::makeCluster(18L)
  
  # Compute all data in parallel.
  data <- parallel::parLapply(
    cl=cl,
    X=x,
    fun=compute_residuals)
  
  # Stop cluster.
  parallel::stopCluster(cl)
  
  # data <- lapply(
  #   x,
  #   compute_residuals)
  
  data <- data.table::rbindlist(data)
  
  saveRDS(
    object=data,
    file=file.path(manuscript_dir, "residual_plot.RDS"))
  
} else {
  data <- readRDS(file.path(manuscript_dir, "residual_plot.RDS"))
}

data$method <- factor(
  x = data$method,
  levels = c("box_cox", "yeo_johnson"),
  labels = c("Box-Cox", "Yeo-Johnson"))

outlier_free_data <- data[has_outliers == FALSE, list("residual_error"=stats::quantile(residual, probs=0.99)), by=c("p", "method")]

data <- data[, list(
  "residual_50"=stats::quantile(residual, probs=0.50),
  "residual_90"=stats::quantile(residual, probs=0.90),
  "residual_95"=stats::quantile(residual, probs=0.95),
  "residual_99"=stats::quantile(residual, probs=0.99),
  "residual_max"=max(residual)),
  by=c("p", "method")]

data <- data.table::melt(
  data = data,
  id.vars = c("p", "method"),
  variable.name = "threshold",
  value.name = "residual_error")

data$threshold <- factor(
  x = data$threshold,
  levels = c("residual_50", "residual_90", "residual_95", "residual_99", "residual_max"),
  labels = c("50 %", "90 %", "95 %", "99 %", "100 %"))

p_bc <- ggplot2::ggplot(
  data = data[method == "Box-Cox"],
  mapping = ggplot2::aes(
    x = p,
    y = residual_error,
    colour = threshold))
p_bc <- p_bc + plot_theme
p_bc <- p_bc + ggplot2::geom_line()
p_bc <- p_bc + ggplot2::geom_line(
  data = outlier_free_data[method == "Box-Cox"],
  mapping = ggplot2::aes(
    x = p,
    y = residual_error),
  colour = "gray40",
  linetype = "dashed")
p_bc <- p_bc + ggplot2::scale_colour_discrete(
  name = "Box-Cox threshold",
  type=c("50 %" = "#ABC6E2", "90 %" = "#779EC6", "95 %" = "#4E79A7", "99 %" = "#346394", "100 %" = "#1D4D7E"))
p_bc <- p_bc + ggplot2::xlab("empirical probability")
p_bc <- p_bc + ggplot2::ylab("absolute residual error")
p_bc <- p_bc + ggplot2::coord_cartesian(
  xlim = c(0, 1),
  ylim = c(0, 1))
p_bc <- p_bc + ggplot2::theme(
  axis.title.x = ggplot2::element_blank(),
  axis.text.x = ggplot2::element_blank())

p_yj <- ggplot2::ggplot(
  data = data[method == "Yeo-Johnson"],
  mapping = ggplot2::aes(
    x = p,
    y = residual_error,
    colour = threshold))
p_yj <- p_yj + plot_theme
p_yj <- p_yj + ggplot2::geom_line()
p_yj <- p_yj + ggplot2::geom_line(
  data = outlier_free_data[method == "Yeo-Johnson"],
  mapping = ggplot2::aes(
    x = p,
    y = residual_error),
  colour = "gray40",
  linetype = "dashed")
p_yj <- p_yj + ggplot2::scale_colour_discrete(
  name = "Yeo-Johnson threshold",
  type=c("50 %" = "#FFBD7D", "90 %" = "#FFA954", "95 %" = "#F28E2B", "99 %" = "#CD6B0B", "100 %" = "#A25000"))
p_yj <- p_yj + ggplot2::xlab("empirical probability")
p_yj <- p_yj + ggplot2::ylab("absolute residual error")
p_yj <- p_yj + ggplot2::coord_cartesian(
  xlim = c(0, 1),
  ylim = c(0, 1))

p <- egg::ggarrange(
  plots=list(
    p_bc,
    p_yj),
  ncol=1,
  heights=c(1.0, 1.0),
  byrow=TRUE,
  draw=FALSE)
p
```


```{r goodness-of-fit, warning=FALSE, message=FALSE, fig.cap=cap}

if(!file.exists(file.path(manuscript_dir, "residual_plot.RDS"))) stop("Residual data for goodness-of-fit test does not exist.")

data <- readRDS(file.path(manuscript_dir, "residual_plot.RDS"))

central_width <- c(0.60, 0.70, 0.80, 0.90, 0.95, 1.00)

p_lower <- 0.50 - central_width / 2
p_upper <- 0.50 + central_width / 2

x <- data.table::copy(data)

# Clip empirical probabilities to centre.
x <- x[p >= p_lower & p <= p_upper]

# Compute mean absolute residual error per feature.
x <- x[, list("mare"=mean(residual)), by=c("distribution_id", "method")]
x <- x[, list("n"=.N), by=c("mare", "method")][order(mare, method)]
x[, "rejected":= 1.0 - cumsum(n) / sum(n)]
x <- rbind(data.table::data.table("mare"=c(0.0, 0.0), "method"=c("box_cox", "yeo_johnson"), "n"=0L, "rejected"=0.0))

p <- ggplot2::ggplot(
  data = x,
  mapping = ggplot2::aes(
    x = mare,
    y = rejected,
    colour = method))
p <- p + plot_theme
p <- p + ggplot2::geom_step()
p <- p + ggplot2::xlab("mean absolute residual error")
p <- p + paletteer::scale_color_paletteer_d(
  palette="ggthemes::Tableau_10",
  drop=FALSE)
```

# Experimental Results

# Discussion

In their work, Box and Cox already mention a transformation with a shift parameter, but preferred the now well-known version in Eq. **INSERT REFERENCE** for the theoretical analysis in their paper [@Box1964-mz].

# Conclusion

# Code availability

Shift-sensitive power transformations are implemented in the `power.transform` package.






# Appendix A: Non-MLE-based optimisation of transformation parameters

Maximum likelihood estimation (MLE) is commonly used to optimise parameters for power transformation. Generally, optimisation requires minimisation or maximisation of a criterion. In MLE, the maximised criterion is the log-likelihood function of the normal distribution. Here, we investigate power transformation using optimisation criteria that are closely related to test statistics for normality tests.

Let $\mathbf{X}$ be a feature with ordered feature values, and $\mathbf{Y}^\lambda =\psi^{\lambda} \left(\mathbf{X} \right)$ and $\mathbf{Y}^{\lambda, x_0} =\psi^{\lambda, x_0} \left(\mathbf{X} \right)$ its transformed values using conventional and shift-sensitive power transformations, respectively. Since power transformations are monotonic, $\mathbf{Y}$ will likewise be ordered.

Below we will focus on criteria based on empirical density function, and on skewness and kurtosis of the transformed featured. Other potential criteria, such as the Shapiro-Wilk test statistic [@Shapiro1965-zd] are not investigated here. In the case of the Shapiro-Wilk test statistic this is because of scalability to features with many ($> 5000$) instances, and because adapting the test statistic to include weights is not straightforward.

## Empirical density function-based criteria

The first class of criteria is based on the empirical distribution function (EDF). Transformation parameters are then fit through minimisation of the distance between the empirical distribution function $F_{\epsilon}$ and the cumulative density function (CDF) of the normal distribution $F_{\mathcal{N}}$. Let $F_{\epsilon}\left(x_i \right) = \frac{i - 1/3}{n + 1/3}$ be the empirical probability of instance $i$. The normal distribution is parametrised by location parameter $\mu$ and scale parameter $\sigma$, both of which have to be estimated from the data. For non-robust power transformation, $\mu$ and $\sigma$ are sample mean and sample standard deviation, respectively. For robust power transformations, we estimate $\mu$ and $\sigma$ as Huber M-estimates of location and scale of the transformed feature $\phi^{\lambda, x_0} (\mathbf{X})$ [@Huber1981-su].

### Anderson-Darling criterion

The Anderson-Darling criterion is based on the empirical distribution function of $\mathbf{X}$. We define this criterion as follows:

```{=latex}
\begin{equation}
U_{\text{AD}} \left(\mathbf{X}, \lambda, x_0 \right) = \frac{1}{\sum_{i=1}^n w_i} \sum_{i=1}^n w_i \frac{\left( F_{\epsilon}\left(x_i \right) - F_{\mathcal{N}} \left(\psi^{\lambda, x_0} \left(x_i \right); \mu, \sigma \right) \right)^2} {F_{\mathcal{N}} \left(\psi^{\lambda, x_0} \left(x_i \right); \mu, \sigma \right) \left(1 - F_{\mathcal{N}} \left(\psi^{\lambda, x_0} \left(x_i \right); \mu, \sigma \right) \right) }
\end{equation}
```

Here $\mu$ and $\sigma$ are location and scale parameters as described above, and $w_i$ are weights as described in section **Insert**. For non-robust power transformations, all $w_i = 1$. Note that this criterion is not the same as the Anderson-Darling test statistic [@Anderson1952-gz], which involves solving (or approximating) an integral function, contains an extra scalar multiplication term, and does not include any extraneous weights $w_i$. The Anderson-Darling criterion seeks to minimise the squared Euclidean distance between the EDF and the normal CDF, with differences at the upper and lower end of the normal CDF receiving more weight than those at the the centre of the CDF.

### Cramr-von Mises criterion

The Cramr-von Mises criterion is also based on the empirical distribution function of $\mathbf{X}$. We define the Cramr-von Mises criterion as follows:
```{=latex}
\begin{equation}
U_{\text{CvM}} \left(\mathbf{X}, \lambda, x_0 \right) = \frac{1}{\sum_{i=1}^n w_i} \sum_{i=1}^n w_i \left( F_{\epsilon}\left(x_i \right) - F_{\mathcal{N}} \left(\psi^{\lambda, x_0} \left(x_i \right); \mu, \sigma \right) \right)^2}
\end{equation}
```

Here $\mu$ and $\sigma$ are location and scale parameters as described above, and $w_i$ are weights as described in section **Insert**. For non-robust power transformations, all $w_i = 1$. The criterion is similar to the Cramr-von Mises test statistic [@Cramer1928-rc, @Von_Mises1928-ef], aside from a additive scalar value and the introduction of extraneous weights. This criterion, like the Anderson-Darling criterion, seeks to minimise the squared Euclidean distance between the EDF and the normal CDF. Unlike the Anderson-Darling criterion, this criterion weights all instances equally.

For conventional power transformations with a fixed shift parameter, the transformation $\phi^{\lambda, x_0} (\mathbf{X})$ may be substituted by $\phi^{\lambda} (\mathbf{X})$ in the definition of the Cramr-von Mises criterion.

## Skewness-kurtosis-based criteria

The second class of criteria seeks to reduce skewness and (excess) kurtosis of the transformed feature $\mathbf{Y}$. We will first define the location $\mu$ and scale $\sigma$ of the the transformed as these are required for computing skewness and kurtosis. Here, $\mu$ is defined as:

```{=latex}
\begin{equation}
\mu = \frac{\sum_{i=1}^n \phi^{\lambda, x_0} \left(x_i \right)} {\sum_{i=1}^n w_i}
\end{equation}
```

The location, or mean, is weighted using weights $w_i$, as described in section **Insert**. For non-robust transformations, $w_i = 1$. Then, $sigma^2$ is defined as:
```{=latex}
\begin{equation}
\sigma^2 = \frac{\sum_{i=1}^n w_i \left(\phi^{\lambda, x_0} \left( x_i \right) - \mu \right)^2}{\sum_{i=1}^n w_i}
\end{equation}
```

Skewness is defined as:
```{=latex}
\begin{equation}
s = \frac{\sum_{i=1}^n w_i \left(\phi^{\lambda, x_0} \left( x_i \right) - \mu \right)^3}{\sigma^3 \sum_{i=1}^n w_i}
\end{equation}
```

Kurtosis is defined as:
```{=latex}
\begin{equation}
k = \frac{\sum_{i=1}^n w_i \left(\phi^{\lambda, x_0} \left( x_i \right) - \mu \right)^4}{\sigma^4 \sum_{i=1}^n w_i}
\end{equation}
```

### D'Agostino criterion

The D'Agostino criterion defined here follows the D'Agostino $K^2$ test statistic [@DAgostino1990-kp]. This test statistic is composed of two separate test statistics, one of which is related to skewness, and the other to kurtosis. Both test statistics are computed in several steps. Let us first define $\nu=\sum_{i=1}^n w_i$. Thus for non-robust power transformations, $\nu = n$.

For the skewness test statistic we first compute [@DAgostino1990-kp]:

```{=latex}
\begin{equation}
\beta_1 = s \sqrt{ \frac{\left(\nu + 1\right) \left(\nu + 3\right)} {6 \left(\nu - 2\right)} }
\end{equation}

\begin{equation}
\beta_2 = 3 \frac{\left(\nu^2 + 27\nu - 70\right) \left(\nu + 1\right) \left(\nu + 3\right)} {\left(\nu - 2\right) \left(\nu + 5\right) \left(\nu + 7\right) \left(\nu + 9\right)}
\end{equation}

\begin{equation}
\alpha = \sqrt{\frac{2} {\sqrt{2 \beta_2 - 1} - 2}}
\end{equation}
```

The skewness test statistic is then:

```{=latex}
\begin{equation}
Z_s^2 = \frac{2 \left[\log\left(\beta_1 / \alpha + \sqrt{\beta_1^2 / \alpha^2 + 1} \right) \right]^2}{\log\left(\sqrt{2 \beta_2 - 1} - 1 \right)}
\end{equation}
```

For the kurtosis test statistic we first compute [@DAgostino1990-kp, @Anscombe1983-nz]:

```{=latex}
\begin{equation}
\beta_1 = 3 \frac{\nu - 1}{\nu + 1}
\end{equation}

\begin{equation}
\beta_2 = 24 \nu \frac{\left(\nu - 2\right)\left(\nu - 3\right)}{\left(\nu + 1\right)^2 \left(\nu + 3\right) \left(\nu + 5\right)}
\end{equation}

\begin{equation}
\beta_3 = 6 \frac{\nu^2 - 5 \nu + 2}{\left(\nu + 7\right) \left(\nu + 9\right)} \sqrt{6 \frac{\left(\nu + 3\right) \left(\nu + 5\right)}{\nu \left(\nu - 2\right) \left(\nu - 3 \right)}}
\end{equation}

\begin{equation}
\alpha_1 = 6 + \frac{8}{\beta_3} \left[\frac{2}{\beta_3} + \sqrt{1 + \frac{4}{\beta_3^2}} \right]
\end{equation}

\begin{equation}
\alpha_2 = \frac{k - \beta_1}{\sqrt{\beta_2}}
\end{equation}
```

The kurtosis test statistic is then:

```{=latex}
\begin{equation}
Z_k^2 = \frac{9 \alpha_1}{2} \left[ 1 - \frac{2}{9 \alpha_1} - \left(\frac{1 - 2 / \alpha_1}{1 + \alpha_2 \sqrt{2 / \left(\alpha_1 - 4 \right)}} \right)^{1 / 3}  \right]^2
\end{equation}
```

The D'Agostino $K^2$ test statistic and our criterion are the same, and are defined as:
```{=latex}
\begin{equation}
U_{\text{DA}} \left(\mathbf{X}, \lambda, x_0 \right) = Z_s^2 + Z_k^2
\end{equation}
```

The main difference between the test statistic as originally formulated, and the criterion proposed here is the presence of weights for robust power transformation.

### Jarque-Bera criterion

The second criterion based on skewness and kurtosis is the Jarque-Bera criterion. It is relatively simple to compute, compared to the D'Agostino criterion:
```{=latex}
\begin{equation}
U_{\text{JB}} \left(\mathbf{X}, \lambda, x_0 \right) = s^2 + \left(k - 3\right)^2 / 4
\end{equation}
```

The main difference between the above criterion and the Jarque-Bera test statistic [@Jarque1980-hw] is that a scalar multiplication is absent.

## Optimisation using non-MLE criteria

Each of the criteria above can be used for optimisation, i.e.:

```{=latex}
\begin{equation}
\left\{ \hat{\lambda}, \hat{x}_0 \right\} = \argmin_{\lambda, x_0} U\left(\mathbf{X}, \lambda, x_0 \right)
\end{equation}
```

For conventional power transformations with a fixed shift parameter, the transformation $\phi^{\lambda, x_0} (\mathbf{X})$ may be substituted by $\phi^{\lambda} (\mathbf{X})$, or equivalently, $x_0$ is fixed:
```{=latex}
\begin{equation}
\left\{ \hat{\lambda}\right\} = \argmin_{\lambda} U\left(\mathbf{X}, \lambda; x_0 \right)
\end{equation}
```



```{r shifted-distributions-other-criteria, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "Shift-sensitive power transformation produces transformation parameters that are invariant to location. ",
  "Samples were drawn from normal, right-skewed and left-skewed distributions, respectively,  which then underwent a shift $d$. ",
  "Estimates of transformation parameter $\\lambda$ with the original power transformations (circle) show strong dependency on the overall location of the distribution, ",
  "whereas estimates with the shift-sensitive power transformations (triangle) are constant.")

# Lambda plot,
.create_lambda_shift_plot <- function(data, plot_theme, limits, guide=FALSE){
  
  p <- ggplot2::ggplot(
    data = data,
    mapping = ggplot2::aes(
      x = d,
      y = lambda,
      colour = estimation_method))
  p <- p + plot_theme
  p <- p + ggplot2::geom_point()
  p <- p + ggplot2::scale_x_continuous(
    name = latex2exp::TeX("$d$"),
    labels = scales::math_format())
  p <- p + ggplot2::scale_y_continuous(
    name = latex2exp::TeX("$\\lambda$"),
    limits = limits)
  p <- p + ggplot2::theme(
    plot.title=ggplot2::element_text(
      hjust=0.5,
      size=ggplot2::rel(1.0)))
  
  if(guide){
    p <- p + paletteer::scale_color_paletteer_d(
      palette = "ggthemes::Tableau_10",
      drop = FALSE)
    p <- p + guides(colour = ggplot2::guide_legend(title = "Optimisation criterion"))
    
  } else {
    p <- p + paletteer::scale_color_paletteer_d(
      palette = "ggthemes::Tableau_10",
      drop = FALSE,
      guide = "none")
  }
  
  return(p)
}

data <- .get_shifted_distribution_data(manuscript_dir = manuscript_dir)

#### Normal distribution -------------------------------------------------------

# Box-Cox-original
p_bc_normal <- .create_lambda_shift_plot(
  data = data[distribution == "normal" & method == "Box-Cox" & version == "original"],
  plot_theme = plot_theme,
  limits = c(-20.0, 35.0))
p_bc_normal <- p_bc_normal + ggplot2::ggtitle(label="normal distribution")

# Yeo-Johnson
p_yj_normal <- .create_lambda_shift_plot(
  data=data[distribution == "normal" & method == "Yeo-Johnson" & version == "original"],
  plot_theme = plot_theme,
  limits = c(-20.0, 35.0))

#### Right skewed distribution -------------------------------------------------

# Box-Cox
p_bc_right <- .create_lambda_shift_plot(
  data = data[distribution == "right-skewed" & method == "Box-Cox" & version == "original"],
  plot_theme = plot_theme,
  limits = c(-15.0, 1.0))
p_bc_right <- p_bc_right + ggplot2::ggtitle(label="right-skewed distribution")

# Yeo-Johnson
p_yj_right <- .create_lambda_shift_plot(
  data = data[distribution == "right-skewed" & method == "Yeo-Johnson" & version == "original"],
  plot_theme = plot_theme,
  limits = c(-15.0, 1.0))

#### Left skewed distribution --------------------------------------------------

# Box-Cox
p_bc_left <- .create_lambda_shift_plot(
  data = data[distribution == "left-skewed" & method == "Box-Cox" & version == "original"],
  plot_theme = plot_theme,
  limits = c(-50.0, 60.0),
  guide = TRUE)
p_bc_left <- p_bc_left + ggplot2::ggtitle(label="left-skewed distribution")

# Yeo-Johnson
p_yj_left <- .create_lambda_shift_plot(
  data = data[distribution == "left-skewed" & method == "Yeo-Johnson" & version == "original"],
  plot_theme = plot_theme,
  limits = c(-50.0, 60.0))

p <- egg::ggarrange(
  plots=list(
    p_bc_normal, p_yj_normal,
    p_bc_right, p_yj_right,
    p_bc_left, p_yj_left),
  ncol=3,
  heights=c(1.0, 1.0),
  byrow=FALSE,
  draw=FALSE)
p

p <- ggplot2::ggplot(
  data = data,
  mapping = ggplot2::aes(
    x = d,
    y = lambda,
    colour = estimation_method))
p <- p + plot_theme
p <- p + ggplot2::geom_point()
p <- p + ggplot2::scale_x_continuous(
  name = latex2exp::TeX("$d$"),
  labels = scales::math_format())
p <- p + ggplot2::scale_y_continuous(
  name = latex2exp::TeX("$\\lambda$"))
p <- p + paletteer::scale_color_paletteer_d(
  palette = "ggthemes::Tableau_10",
  drop = FALSE)
p <- p + guides(colour = ggplot2::guide_legend(title = "optimisation criterion"))

p <- p + ggplot2::facet_grid(
  method + version ~ distribution,
  scales = "free")


```



```{r eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE}

if(!file.exists(file.path(manuscript_dir, "robustness_comparison_rightside_outlier.RDS"))){
  set.seed(95)
  
  .compute_lambda <- function(x, method){
    # Create transformer object.
    transformer <- suppressWarnings(
      power.transform::find_transformation_parameters(
        x = x,
        method = method,
        robust = FALSE,
        shift = TRUE))
    
    return(transformer@lambda)
  }
  
  # Generate table of asymmetric generalised normal distribution parameters.
  n_distributions <- 100
  
  # Generate alpha, beta and n.
  n <- stats::runif(n=n_distributions, min=2, max=4)
  n <- ceiling(10^n)
  
  alpha <- stats::runif(n=n_distributions, min=0.01, max=0.99)
  beta <- stats::runif(n=n_distributions, min=1.00, max=5.00)
  
  # Generate corresponding distributions.
  x <- mapply(
    power.transform::ragn,
    n = n,
    alpha = alpha,
    beta = beta)
  
  # Compute lambda values without weighting for Box-Cox.
  target_lambda_bc <- sapply(
    x,
    .compute_lambda,
    method = "box_cox")
  
  # Compute lambda values without weighting for Yeo-Johnson
  target_lambda_yj <- sapply(
    x,
    .compute_lambda,
    method = "yeo_johnson")
  
  # Add outliers, between 0.00 and 0.10.
  n_outliers <- 10
  outlier_fraction <- (seq_len(n_outliers) - 1)^2 /((n_outliers-1)^2 * 20)
  
  # Find optimised k1 and k2 values for all combinations of weight sources and
  # weighting functions.
  x <- mapply(
    function(x, n, alpha, beta, target_lambda_bc, target_lambda_yj, outlier_fraction){
      
      parameters <- list("n" = n, "alpha"=alpha, "beta" = beta, "target_lambda_bc"=target_lambda_bc, "target_lambda_yj"=target_lambda_yj)
      
      x <- lapply(
        outlier_fraction,
        function(k, x, parameters){
          # Set parameters.
          parameters$k <- k
          
          # Compute interquartile range.
          interquartile_range <- stats::IQR(x)
          
          # Compute upper and lower quartiles.
          q_lower <- stats::quantile(x, probs=0.25, names=FALSE)
          q_upper <- stats::quantile(x, probs=0.75, names=FALSE)
          
          # Set data where the outliers will be copied into.
          x_outlier <- x
          
          if(k != 0.0){
            n_draw <- ceiling(k * length(x))
            
            # Generate outlier values that are smaller than Q1 - 1.5 IQR or larger
            # than Q3 + 1.5 IQR.
            x_random <- stats::runif(n_draw, min=0.0, max=2.0)
            outlier <- q_upper + 1.5 * interquartile_range + x_random * interquartile_range
            
            # Randomly insert outlier values.
            x_outlier[sample(seq_along(x), size=n_draw, replace=FALSE)] <- outlier
          }
          
          return(list(
            "x" = x_outlier,
            "parameters" = parameters))
        },
        x = x,
        parameters)
      
      return(x)
    },
    x = x,
    n = n,
    alpha = alpha,
    beta = beta,
    target_lambda_bc = target_lambda_bc,
    target_lambda_yj = target_lambda_yj,
    MoreArgs=list("outlier_fraction"=outlier_fraction),
    SIMPLIFY=FALSE,
    USE.NAMES=FALSE)
  
  x <- unlist(x, recursive = FALSE)
  
  # Optimise k1, k2 so that sum of absolute differences with target-lambda is
  # minimised for each combination of source and weighting method.
  experiment_args <- coro::generator(
    function(){
      
      for(method in c("box_cox", "yeo_johnson")){
        # Non-robust transformation.
        fun_args <- list(
          "method" = method,
          "robust" = FALSE,
          "shift" = TRUE)
        
        opt_args <- list()
        
        yield(list(
          "name" = "non-robust",
          "method" = method,
          "fun_args" = fun_args,
          "opt_args" = opt_args
        ))
      }
      
      for(robustness_source in c("original", "transformed", "residual")){
        
        def_opt_limits <- list("k1"=c(0.0, 10.0), "k2"=c(0.0, 10.0))
        
        if(robustness_source == "original"){
          def_opt_limits$k1[2] <- 1.0
          def_opt_limits$k2[2] <- 1.0
          
          def_opt_init <- list("k1" = 0.80, "k2" = 0.95)
          
        } else if(robustness_source == "transformed"){
          def_opt_init <- list("k1" = 1.28, "k2" = 1.96)
          
        } else if(robustness_source == "residual"){
          def_opt_init <- list("k1" = 0.50, "k2" = 1.0)
        }
        
        for(method in c("box_cox", "yeo_johnson")){
          for(robustness_weighting_function in c("step", "triangle", "cosine")){
            
            fun_args <- list(
              "method" = method,
              "robust" = TRUE,
              "shift" = TRUE,
              "weight_method" = paste0(robustness_source, "_", robustness_weighting_function),
              "backup_use_default" = FALSE)
            
            opt_limits <- def_opt_limits
            opt_init <- def_opt_init
            
            # Step-weighting only has k1 as a parameter.
            if(robustness_weighting_function == "step"){
              opt_limits$k2 <- NULL
              opt_init$k2 <- NULL
            }
            
            yield(list(
              "name" = paste0(robustness_source, "-", robustness_weighting_function),
              "method" = method,
              "fun_args" = fun_args,
              "opt_args" = list("initial" = opt_init, "limits" = opt_limits)
            ))
          }
        }
      }
    })
  
  # Function for computing transformer parameters under optimisation constraints.
  compute_robust_lambda <- function(
    x,
    fun_args,
    opt_args,
    cl_internal=NULL){
    
    # Custom parser.
    ..outlier_parser <- function(
    x,
    fun_args,
    opt_args
    ){
      # Create transformer.
      transformer <- suppressWarnings(do.call(
        power.transform::find_transformation_parameters,
        args=c(
          list("x" = x$x),
          fun_args,
          opt_args)))
      
      parameter_data <- c(
        x$parameters,
        opt_args,
        list(
          "method" = fun_args$method,
          "weight_method" = fun_args$weight_method,
          "lambda" = transformer@lambda,
          "shift" = transformer@shift)
      )
      
      return(parameter_data)
    }
    
    if(is.null(cl_internal)){
      parameter_data <- lapply(
        x,
        ..outlier_parser,
        fun_args = fun_args,
        opt_args = opt_args)
      
    } else {
      parameter_data <- parallel::parLapply(
        cl = cl_internal,
        X = x,
        fun = ..outlier_parser,
        fun_args = fun_args,
        opt_args = opt_args)
    }
    
    return(parameter_data)
  }
  
  
  # Inner optimisation function that sets the loss.
  ..optimisation_function <- function(
    opt_param,
    x,
    fun_args,
    cl_internal = NULL){
    
    # Parse options.
    opt_args <- list()
    if(length(opt_param) >= 1) opt_args <- c(opt_args, list("k1"=opt_param[1]))
    if(length(opt_param) >= 2) opt_args <- c(opt_args, list("k2"=opt_param[2]))
    
    parameter_data <- compute_robust_lambda(
      x = x,
      fun_args = fun_args,
      opt_args = opt_args,
      cl_internal = cl_internal)
    
    if(fun_args$method == "box_cox"){
      lambda_error <- sapply(
        parameter_data,
        function(x) (abs(x$lambda - x$target_lambda_bc)))
      
    } else {
      # Yeo-Johnson.
      lambda_error <- sapply(
        parameter_data,
        function(x) (abs(x$lambda - x$target_lambda_yj)))
    }
    
    return(sum(lambda_error))
  }
  
  
  # Outer optimisation function
  .optimisation_function <- function(
    experiment,
    data,
    cl_internal = NULL){
    
    set.seed(9)
    
    # Run optimiser.
    if(length(experiment$opt_args) > 0){
      
      x0 <- unlist(experiment$opt_args$initial)
      
      lower <- experiment$opt_args$limits$k1[1]
      upper <- experiment$opt_args$limits$k1[2]
      
      if(length(experiment$opt_args$limits) > 1){
        lower <- c(lower, experiment$opt_args$limits$k2[1])
        upper <- c(upper, experiment$opt_args$limits$k2[2])
      }
      
      h <- nloptr::sbplx(
        x0 = x0,
        fn = .optimisation_inner,
        lower = lower,
        upper = upper,
        control=list("xtol_rel"=1e-3, "ftol_rel"=1e-4),
        x = data,
        fun_args = experiment$fun_args,
        cl_internal = cl_internal)
      
      if(length(h$par) == 1){
        return(list("name"=experiment$name, "method"=experiment$method, "k1" = h$par, "value" = h$value))
        
      } else {
        return(list("name"=experiment$name, "method"=experiment$method, "k1" = h$par[1], "k2" = h$par[2], "value" = h$value))
      }
      
    } else {
      value <- ..optimisation_function(
        opt_param=list(),
        x = data,
        fun_args=experiment$fun_args,
        cl = cl_internal)
      
      return(list("name"=experiment$name, "method"=experiment$method, "value" = value))
    }
  }
  
  
  # Start cluster
  cl <- parallel::makeCluster(18L)
  
  parallel::clusterExport(
    cl=cl,
    varlist = c("compute_robust_lambda", "..optimisation_function")
  )
  
  experiment_results <- parallel::parLapplyLB(
    cl = cl,
    X = coro::collect(experiment_args()),
    fun = .optimisation_function,
    data = x,
    chunk.size = 1L)
  
  # Stop cluster.
  parallel::stopCluster(cl)
  
  saveRDS(experiment_results, file = file.path(manuscript_dir, "robustness_comparison_rightside_outlier.RDS"))
}
```

# References
