---
title: "Location and Scale-Invariant Power Transformations for Transforming Data to Normality"
author: "Alex Zwanenburg"
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: lualatex
bibliography: refs.bib
header-includes:
- \usepackage{amsmath}
- \usepackage{booktabs}
- \DeclareMathOperator*{\argmax}{argmax}
- \DeclareMathOperator*{\argmin}{argmin}
- \DeclareMathOperator{\sgn}{sgn}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  fig.align= "center")

# Allow for defining figure captions within a chunk.
knitr::opts_knit$set(
  eval.after = "fig.cap")

require(power.transform)
require(ggplot2)
require(patchwork)
require(data.table)
require(latex2exp)
require(coro)
require(paletteer)
require(robustHD)
require(modeldata)

manuscript_dir <- file.path("C:/Users/alexz/Documents/GitHub/power.transform/manuscript")
source(file.path(manuscript_dir, "simulations.R"))
source(file.path(manuscript_dir, "plots.R"))

# Base font size.
base_size <- 10

# Set general theme.
plot_theme <- ggplot2::theme_light(base_size=base_size)
plot_theme$plot.title$size <- ggplot2::rel(1.0)
plot_theme$strip.background <- ggplot2::element_blank()
plot_theme$strip.text$colour <- NULL
plot_theme$plot.margin <- grid::unit(c(2, 2, 2, 2), "points")

parse_mean_sd_latex <- function(mu, sigma, digits=2L){
  return(paste0(
    "$", round(mu, digits=digits), " \\pm ", round(sigma, digits=digits), "$"
  ))
}

```

# Abstract

# Introduction

Numerical variables in datasets may strongly deviate from normal distributions, e.g. by being skewed. This may complicate further analysis. Power transformations [@Tukey1957-rt] can help improve normality of such features. The two most commonly used transformations are that of @Box1964-mz and @Yeo2000-vw. The Box-Cox transformation of a feature value $x_i$ of feature $\mathbf{X}=\left\{x_1, x_2, \ldots, x_n \right\}$ under the transformation parameter $\lambda$ is defined as:

```{=latex}
\begin{equation}
\label{eqn:box-cox-original}
\phi_{\text{BC}}^\lambda (x_i) = 
\begin{cases}
\left(x_i^\lambda - 1 \right) / \lambda & \text{if } \lambda \neq 0\\
\log(x_i) & \text{if } \lambda = 0
\end{cases}
\end{equation}
```
One limitation of the Box-Cox transformation is that it is only defined for $x_i > 0$. In contrast, the Yeo-Johnson transformation under the transformation parameter $\lambda$ is defined for any $x_i \in \mathbb{R}$:

```{=latex}
\begin{equation}
\label{eqn:yeo-johnson-original}
\phi_{\text{YJ}}^\lambda (x_i) = 
\begin{cases}
\left( \left( 1 + x_i \right)^\lambda - 1\right) / \lambda & \text{if } \lambda \neq 0 \text{ and } x_i \geq 0\\
\log(1 + x_i) & \text{if } \lambda = 0 \text{ and } x_i \geq 0\\
-\left( \left( 1 - x_i\right)^{2 - \lambda} - 1 \right) / \left(2 - \lambda \right) & \text{if } \lambda \neq 2 \text{ and } x_i < 0\\
-\log(1 - x_i) & \text{if } \lambda = 2 \text{ and } x_i < 0
\end{cases}
\end{equation}
```
The $\lambda$-parameter is typically optimised using maximum likelihood estimation under the assumption that the transformed feature is normally distributed. As noted by Raymaekers and Rousseeuw, this approach is very sensitive to the presence of outliers, and robust versions of Box-Cox and Yeo-Johnson transformations were devised [@Raymaekers2021-kq].

Power transformation does not guarantee that transformed variables are normally distributed. In fact, depending on location and scale of the variable, power transformation may decrease normality, as shown in Figure \ref{fig:decreased-normality}. This is problematic for automated power transformations, for three reasons. First, normality is often desired in machine learning applications. Second, a large negative or positive $\lambda$-parameter may lead to numeric issues. Third, we currently do not have a test to automatically reject poor transformations. Statistical tests for normality exist, such as the Shapiro-Wilk test [@Shapiro1965-zd]. However, given sufficiently large sample sizes, these tests can detect trivial deviations from normality and are therefore not well-suited for applications of automated power transformations in practice.

```{r decreased-normality, echo=FALSE, fig.cap=cap, fig.height=4.0, warning=FALSE}
cap <- paste0(
  "Power transformations may reduce normality depending on location. ",
  "Variable $x$ consists of $10000$ values randomly drawn from a normal distribution. ",
  "For the left-hand panels, that distribution is parameterised as $\\mathcal{N}(\\mu, 1)$, ",
  "whereas for the right-hand row, the distribution is $\\mathcal{N}(0, \\sigma)$.",
  "For both Box-Cox and Yeo-Johnson transformations, we would expect $\\lambda = 1$, since $x$ is already normally distributed. ",
  "The figure shows that instead, $\\lambda$ is strongly dependent on the location $\\mu$, ",
  "and breaks down due to numerical issues for $\\mu > 5 \\cdot 10^3$ (both Box-Cox and Yeo-Johnson).",
  "Scale did not affect Box-Cox and Yeo-Johnson transformations of variables drawn from normal distributions."
)

.plot_reduced_normality(plot_theme = plot_theme, manuscript_dir = manuscript_dir)
```

Here we make the following contributions:

-   We devise shift and scale invariant versions of the Box-Cox and Yeo-Johnson transformation, including versions robust to outliers.

-   We define an empirical central normality test for detecting cases where power transformations fail to yield an approximately normally distributed transformed feature.

-   We assess the effect of power transformations on the performance of machine learning models.

# Methods

## Location and scale invariant power transformation

Box-Cox and Yeo-Johnson transformations are modified by introducing shift parameter $x_0$ and scale parameter $s$ into equations \ref{eqn:box-cox-original} and \ref{eqn:yeo-johnson-original}. The location and scale invariant Box-Cox transformation of a feature value $x_i$ of feature $\mathbf{X}$ under transformation parameter $\lambda$, shift parameter $x_0$ and scale parameter $s$ is then:

```{=latex}
\begin{equation}
\label{eqn:box-cox-shift}
\phi_{\text{BC}}^{\lambda, x_0, s} (x_i) = 
\begin{cases}
\left( \left(\frac{x_i - x_0}{s} \right)^\lambda - 1 \right) / \lambda & \text{if } \lambda \neq 0\\
\log\left[\frac{x_i - x_0}{s}\right] & \text{if } \lambda = 0
\end{cases}
\end{equation}
```
where $x_i - x_0 > 0$. Likewise, the shift and scale invariant Yeo-Johnson transformation of a feature value $x_i$ under transformation parameter $\lambda$, shift parameter $x_0$ and scale parameter $s$ is:

```{=latex}
\begin{equation}
\label{eqn:yeo-johnson-shift}
\phi_{\text{YJ}}^{\lambda, x_0, s} (x_i) = 
\begin{cases}
\left( \left( 1 + \frac{x_i - x_0}{s}\right)^\lambda - 1\right) / \lambda & \text{if } \lambda \neq 0 \text{ and } x_i - x_0 \geq 0\\
\log\left[1 + \frac{x_i - x_0}{s}\right] & \text{if } \lambda = 0 \text{ and } x_i - x_0 \geq 0\\
-\left( \left( 1 - \frac{x_i - x_0}{s}\right)^{2 - \lambda} - 1 \right) / \left(2 - \lambda \right) & \text{if } \lambda \neq 2 \text{ and } x_i - x_0 < 0\\
-\log\left[1 - \frac{x_i - x_0}{s}\right] & \text{if } \lambda = 2 \text{ and } x_i - x_0 < 0
\end{cases}
\end{equation}
```

For both invariant transformations, $\lambda$, $x_0$ and $s$ parameters are obtained by maximising the log-likelihood function. A full derivation is shown in Appendix A. The invariant Box-Cox log-likelihood function is:

```{=latex}
\begin{equation}
\label{eqn:box-cox-invariant-log-likelihood}
\begin{split}
\mathcal{l}_{\text{BC}}^{\lambda, x_0, s} = & -\frac{n}{2} \log \left[2 \pi \sigma^2 \right] -\frac{1}{2 \sigma^2} \sum_{i=1}^n \left( \phi^{\lambda, x_0, s}(x_i) - \mu \right)^2 \\
& -n \lambda \log s + \left( \lambda - 1 \right) \sum_{i=1}^n \log \left[ x_i - x_0 \right]
\end{split}
\end{equation}
```
where $\mu$ and $\sigma^2$ are the mean and variance of the Box-Cox transformed feature $\phi_{\text{BC}}^{\lambda, x_0, s} (\mathbf{X})$, respectively. Similarly, for the invariant Yeo-Johnson log-likelihood function is:

```{=latex}
\begin{equation}
\label{eqn:yeo-johnson-invariant-log-likelihood}
\begin{split}
\mathcal{l}_{\text{YJ}}^{\lambda, x_0, s} = & -\frac{n}{2} \log\left[2 \pi \sigma^2\right] -\frac{1}{2 \sigma^2} \sum_{i=1}^n \left( \phi^{\lambda, x_0, s}(x_i) - \mu \right)^2 \\
& - n \log s + (\lambda - 1) \sum_{i=1}^n \sgn(x_i - x_0) \log \left[1 + \frac{|x_i - x_0|}{s} \right]
\end{split}
\end{equation}
```
where $\mu$ and $\sigma^2$ are the mean and variance of the Box-Cox transformed feature $\phi_{\text{YJ}}^{\lambda, x_0, s} (\mathbf{X})$, respectively.


## Robust location and scale invariant power transformations

```{r sensitivity-to-outliers, echo=FALSE, fig.cap=cap, fig.height=2.0, warning=FALSE}
cap <- paste0(
  "Power transformations are affected by outliers. ",
  "Variable $x$ consists of $1000$ values randomly drawn from a normal distribution $\\mathcal{N}(0, 1)$, ",
  "with a single outlier with value $d$. ",
  "For both Box-Cox and Yeo-Johnson transformations, we would expect $\\lambda = 1$, since $x$ is already normally distributed. ",
  "The figure shows that instead, $\\lambda$ is strongly affected by the presence of the outlier at $d$.")

.plot_sensitivity_to_outlier(plot_theme = plot_theme, manuscript_dir = manuscript_dir)

```

Real-world data may contain outliers, to which maximum likelihood estimation can be sensitive. Their presence may lead to poor transformations to normality, as shown in Figure \ref{fig:sensitivity-to-outliers}. As indicated by @Raymaekers2021-kq, the general aim of power transformations should be to transform non-outlier data to normality, i.e. achieve *central normality*. To achieve this, they devised an iterative procedure to find a robust estimate of the transformation parameter $\lambda$. Because $\lambda$ is updated during their procedure, it can not be used simultaneously estimate $\lambda$, $x_0$ and $s$ for invariant power transformations.

In essence, obtaining robust transformation parameters that provide central normality requires identifying outliers in the data and weighting such instances during the optimisation process. @Raymaekers2021-kq do this (during the final steps of their iterative procedure) through a weighted maximum likelihood estimation. Here, weighted maximum likelihood estimation is based on equations \ref{eqn:box-cox-invariant-log-likelihood} and \ref{eqn:yeo-johnson-invariant-log-likelihood}. Compared to @Raymaekers2021-kq, these log-likelihood functions includes additional terms to accommodate estimation of $x_0$ and $s$. The weighted invariant Box-Cox log-likelihood function is:

```{=latex}
\begin{equation}
\begin{split}
\mathcal{l}_{\text{rBC}}^{\lambda, x_0, s} = & -\frac{1}{2} \left(\sum_{i=1}^n w_i \right) \log \left[ 2 \pi \sigma_w^2 \right] -\frac{1}{2 \sigma_w^2} \sum_{i=1}^n w_i \left( \phi^{\lambda, x_0, s}(x_i) - \mu_w \right)^2 \\
& - \lambda \left( \sum_{i=1}^n w_i \right) \log s + \left( \lambda - 1 \right) \sum_{i=1}^n w_i \log \left[ x_i - x_0 \right]
\end{split}
\end{equation}
```
where $\mu_w$ and $\sigma^2_w$ are the weighted mean and weighted variance of the Box-Cox transformed feature $\phi_{\text{BC}}^{\lambda, x_0, s} (\mathbf{X})$:

```{=latex}
\begin{equation}
\sigma_w^2 = \frac{\sum_{i=1}^n w_i \left(\phi_{\text{BC}}^{\lambda, x_0, s} (x_i) - \mu_w \right)^2}{\sum_{i=1}^n w_i} \quad \text{with } \mu_w = \frac{\sum_{i=1}^n \phi_{\text{BC}}^{\lambda, x_0, s} (x_i)} {\sum_{i=1}^n w_i}
\end{equation}
```

Analogously, the weighted invariant Yeo-Johnson log-likelihood function is:

```{=latex}
\begin{equation}
\begin{split}
\mathcal{l}_{\text{rYJ}}^{\lambda, x_0, s} = & -\frac{1}{2} \left(\sum_{i=1}^n w_i \right) \log \left[ 2 \pi \sigma_w^2 \right] -\frac{1}{2 \sigma_w^2} \sum_{i=1}^n w_i \left( \phi^{\lambda, x_0, s}(x_i) - \mu_w \right)^2 \\
& - \left( \sum_{i=1}^n w_i \right) \log s + (\lambda - 1) \sum_{i=1}^n w_i \sgn(x_i - x_0) \log \left[1 + \frac{|x_i - x_0|}{s} \right]
\end{split}
\end{equation}
```
where $\mu_w$ and $\sigma^2_w$ are the weighted mean and weighted variance of the Yeo-Johnson transformed feature $\phi_{\text{YJ}}^{\lambda, x_0, s} (\mathbf{X})$:

```{=latex}
\begin{equation}
\sigma_w^2 = \frac{\sum_{i=1}^n w_i \left(\phi_{\text{YJ}}^{\lambda, x_0, s} (x_i) - \mu_w \right)^2}{\sum_{i=1}^n w_i} \quad \text{with } \mu_w = \frac{\sum_{i=1}^n \phi_{\text{YJ}}^{\lambda, x_0, s} (x_i)} {\sum_{i=1}^n w_i}
\end{equation}
```

Other criteria aside from MLE can be used for finding robust transformation parameters, see Appendix B. For the remainder, we will use MLE as the optimisation criterion. 

There are a few options based on which weights may be set. Here, we investigate three:

- Set weights based on probabilities of the empirical distribution of the original feature $\mathbf{X}$. Probabilities are determined as $p_i = \frac{i - 1/3}{n + 1/3}$, with $i = 1, 2, \ldots n$, with $n$ the number of instances of feature $\mathbf{X}$.

- Set weights based on the z-score of the transformed feature $\phi^{\lambda, x_0, s} (\mathbf{X})$. After @Raymaekers2021-kq, $z_i = \frac{\phi^{\lambda, x_0, s}(x_i) - \mu_M}{\sigma_M}$. Here, $\mu_M$ and $\sigma_M$ are robust Huber M-estimates of location and scale of the transformed feature $\phi^{\lambda, x_0, s} (\mathbf{X})$ [@Huber1981-su].

- Set weights based on the residual error between the z-score of the transformed feature $\phi^{\lambda, x_0, s} (\mathbf{X})$ and the theoretical z-score from a standard normal distribution: $r_i =\left| \left( \phi^{\lambda, x_0, s}(x_i) - \mu_M)\right) / \sigma_M - F^{-1}_{\mathcal{N}}(p_i) \right|$, with $\mu_M$, $\sigma_M$ and $p_i$ as defined above.

Based on either of the above, weights are set using a weighting function. We will investigate the following functions, where $\dot{x}_i$ can be either $p^{*}_i=2 \left( p_i - 0.5\right)$ (so that the quantiles are zero-centred), $z_i$ or $r_i$:

- A step function, with $k_1 \geq 0$ as threshold parameter:
```{=latex}
\begin{equation}
w_i =
\begin{cases}
1 & \text{if } \left| \dot{x}_i \right| \leq k_1\\
0 & \text{if } \left| \dot{x}_i \right| > k_1
\end{cases}
\end{equation}
```

- A triangle function (or generalised Huber weight), with $k_1 \geq 0$ and $k_2 \geq k_1$ as threshold parameters:
```{=latex}
\begin{equation}
w_i =
\begin{cases}
1 & \text{if } \left| \dot{x}_i \right| < k_1\\
1 - \frac{\left| \dot{x}_i \right| - k_1}{k_2 - k_1} & \text{if } k_1 \leq \left| \dot{x}_i \right| \leq k_2 \\
0 & \text{if } \left| \dot{x}_i \right| > k_2
\end{cases}
\end{equation}
```

- A tapered cosine function [@Tukey1967-eb], with $k_1 \geq 0$ and $k_2 \geq k_1$ as threshold parameters:
```{=latex}
\begin{equation}
w_i =
\begin{cases}
1 & \text{if } \left| \dot{x}_i \right| < k_1\\
0.5 + 0.5 \cos\left(\pi \frac{\left| \dot{x}_i \right| - k_1}{k_2 - k_1} \right) & \text{if } k_1 \leq \left| \dot{x}_i \right| \leq k_2 \\
0 & \text{if } \left| \dot{x}_i \right| > k_2
\end{cases}
\end{equation}
```

All weighting functions share the characteristic that for $\left| \dot{x}_i \right|< k_1$, instances are fully weighted, i.e. when $k_1 > 0$ the weighting functions are symmetric window functions with a flat top. The triangle and tapered cosine functions then gradually down-weight instances with $k_1 \leq \left| \dot{x}_i \right| \leq k_2$, and assign no weight to instances $\left| x_i \right| > k_2$. Examples of the three types of weighting function are shown in Figure \ref{fig:weighting-functions}.

```{r weighting-functions, warning=FALSE, message=FALSE, fig.cap=cap, fig.height=2.5}
cap <- paste0(
  "Weighting functions investigated in this study to make power transformations more robust against outliers. ",
  "The step function was parameterised with $k_1 = 0.60$. ",
  "The triangle and tapered cosine functions were both parameterised with $k_1 = 0.30$ and $k_2 = 0.90$.")

.plot_weighting_functions(plot_theme = plot_theme)
```

## Asymmetric generalised normal distributions

Modifications intended to make power transformations invariant to location and scale of a feature and methods to improve their robustness against outliers need to be assessed using data drawn from a range of different distributions. Since the power transformations are intended for use with unimodal distributions, the generalised normal distribution [@Subbotin1923-qk; @Nadarajah2005-xe] is a suitable option for simulating realistic feature distributions. This distribution has the following probability density function $f_{\beta}$ for a value $x \in \mathbb{R}$:

```{=latex}
\begin{equation}
f_{\beta}(x) = \frac{\beta}{2\Gamma \left(1 / \beta \right)} e^{-\left| x \right|^\beta}
\end{equation}
```
Here, $\Gamma$ is the gamma function, and $\beta$ is a strictly positive shape parameter. For $\beta = 1$, the probability density function describes a Laplace distribution. A normal distribution is found for $\beta=2$, and for large $\beta$, the distribution approaches a uniform distribution. We will refrain from introducing scale and location parameters here directly.

Realistic feature distributions may be skewed. Gijbels et al. describe a recipe for introducing skewness into the otherwise symmetric generalised normal distribution [@Gijbels2019-te], leading to the following probability density function:

```{=latex}
\begin{equation}
f_{\alpha}(x; \mu, \sigma, \beta) = \frac{2 \alpha \left(1 - \alpha\right)}{\sigma}
\begin{cases}
f_{\beta}\left( \left(1 - \alpha \right) \frac{\left| x - \mu \right|}{\sigma} \right) & \text{, } x \leq \mu \\
f_{\beta}\left( \alpha \frac{\left| x - \mu \right|}{\sigma} \right) & \text{, } x > \mu
\end{cases}
\end{equation}
```
Here $\alpha \in (0,1)$ is a skewness parameter. $\alpha > 0.5$ creates a distribution with a negative skew, i.e. a left-skewed distribution. A right-skewed distribution is created for $\alpha < 0.5$. $\mu \in \mathcal{R}$ and $\sigma \in (0, \infty)$ are location and scale parameters, respectively. $f_{\alpha}$ thus describes the probability density function of an asymmetric generalised normal distribution, which we will refer to here and parametrise as $\mathcal{AGN}\left(\mu, \sigma, \alpha, \beta \right)$.

We require a quantile function (or an approximation thereof) to draw random values from an asymmetric generalised normal distribution using inverse transform sampling. Gijbels et al. derived the following quantile function $F_{\alpha}^{-1}(p)$:

```{=latex}
\begin{equation}
F_{\alpha}^{-1}(p; \mu, \sigma, \beta) =
\begin{cases}
\mu + \frac{\sigma}{1 - \alpha} F_{\beta}^{-1} \left( \frac{p}{2 \alpha}\right) & \text{, } p \leq \alpha \\
\mu + \frac{\sigma}{\alpha} F_{\beta}^{-1} \left( \frac{1 + p - 2 \alpha}{2 \left(1 - \alpha \right)} \right) & \text{, } p > \alpha
\end{cases}
\end{equation}
```
The quantile function for the asymmetric generalised normal distribution $F_{\alpha}^{-1}(p)$ thus incorporates the quantile function $F_{\beta}^{-1}(p^{*})$ of the symmetric generalised normal distribution. $F_{\beta}^{-1}(p^{*})$ was derived by Griffin to be [@Griffin2018-bf]:

```{=latex}
\begin{equation}
F_{\beta}^{-1}(p^{*}) = \sgn\left(p^{*} - 0.5 \right) F_{\Gamma}^{-1}\left(2 \left|p^{*} - 0.5 \right|; 1 / \beta \right)
\end{equation}
```
Here, $F_{\Gamma}^{-1}$ is the quantile function of the gamma distribution with shape $1 / \beta$, which can be numerically approximated. 

## Empirical central normality test

Power transformations aim to transform features to a normal distribution. However, this may not always be successful or possible. Deviations from normality can be detected by normality tests, such as the Shapiro-Wilk test [@Shapiro1965-zd]. In practice, normality tests may be too stringent to assess goodness of fit in presence of large sample sizes, outliers or a combination of the two. Here we develop an empirical test for central normality. The null hypothesis $H_0$ is that the distribution is centrally normal. The alternative hypothesis $H_1$ is that the distribution is not centrally normal.

Let central normality be defined as normality of central portion $\kappa$ of the data, i.e. $\mathbf{X}_{\text{central}} = \left\{x_i \in \mathbf{X} \, | \,  \frac{1-\kappa}{2} \leq  p_i \leq \frac{1 + \kappa}{2}\right\}$, with $p_i$ probabilities of the empirical distribution, as previously.
We then compute the residual errors between the z-scores of the transformed feature $\phi^{\lambda, x_0, s} (\mathbf{X})$ and the expected z-scores from a standard normal distribution: $r_i =\left| \left( \phi^{\lambda, x_0, s}(x_i) - \mu_M)\right) / \sigma_M - F^{-1}_{\mathcal{N}}(p_i) \right|$, with $\mu_M$ and $\sigma_M$ robust Huber M-estimates of location and scale of the transformed feature $\phi^{\lambda, x_0, s} (\mathbf{X})$ [@Huber1981-su].
The set of residual errors for the central portion of the data is then $\mathbf{R}_{\text{central}} = \left\{ r_i \in \left\{ r_1, r_2, \ldots, r_n\right\} \, | \,  \frac{1-\kappa}{2} \leq  p_i \leq \frac{1 + \kappa}{2}\right\}$.

The test statistic $\tau_{\text{ecn}}$ is then defined as:
```{=latex}
\begin{equation}
\tau_{\text{ecn}} = \frac{\sum_{i=1}^{N} r_i \left[r_i \in \mathbf{R}_{\text{central}}\right]}{\sum_{i=1}^N \left[r_i \in \mathbf{R}_{\text{central}}\right]} 
\end{equation}
```
Here $[\quad]$ denotes an Iverson bracket. The test statistic is equal to the mean of the residual errors of the central portion of the data. 

# Simulation

In this section we simulate data to assess invariance to location and scale of the proposed power transformations, weighting for robust transformations, and to develop the empirical central normality test.
The $\lambda$ parameter for conventional power transformations (Eqn. \ref{eqn:box-cox-original} and \ref{eqn:yeo-johnson-original}) and $\lambda$, $x_0$ and $s$ parameters for location and scale invariant power transformations (Eqn. \ref{eqn:box-cox-invariant} and \ref{eqn:yeo-johnson-invariant}) are estimated using the BOBYQA algorithm for derivative-free bound constraint optimisation [@Powell2009-zb] through maximum likelihood estimation. Appendix B describes other criteria, and Appendix C the results of simulations using these criteria.

## Invariance to location and scale

To assess whether the proposed power transformations lead to $\lambda$ values that are invariant to location and scale of the distribution, we simulate three different distributions. The first distribution is a normal distribution, as previously shown in Figure \ref{fig:decreased-normality}.
We first randomly draw $10000$ values from a normal distribution: $\mathbf{X}_{\text{normal}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{N}\left(0, 1\right)$, or equivalently $\mathbf{X}_{\text{normal}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{AGN}\left(0, 1/\sqrt{2}, 0.5, 2\right)$.
The second distribution is a right-skewed normal distribution $\mathbf{X}_{\text{right}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{AGN}\left(0, 1/\sqrt{2}, 0.2, 2\right)$.
The third distribution is a left-skewed normal distribution $\mathbf{X}_{\text{left}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{AGN}\left(0, 1/\sqrt{2}, 0.8, 2\right)$.
We then compute transformation parameter $\hat{\lambda}$ using the original definitions (equations \ref{eqn:box-cox-original} and \ref{eqn:yeo-johnson-original}) and the location and scale invariant definitions (equations \ref{eqn:box-cox-invariant} and \ref{eqn:yeo-johnson-invariant}) for each distribution.
To assess location invariance, a positive value $d_{\text{shift}}$ is added to each distribution with $d_{\text{shift}} \in [1, 10^6]$. Similarly, to assess scale invariance, each distribution is multiplied by a positive value $d_{\text{scale}}$, where $d_{\text{scale}} \in [1, 10^6]$.

```{r shifted-distributions, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "Invariant power transformation produces transformation parameters that are invariant to location and shape. ",
  "Samples were drawn from normal, right-skewed and left-skewed distributions, respectively, which then underwent a shift $d_{\\text{shift}}$ or multiplication by $d_{\\text{scale}}$. ",
  "Estimates of transformation parameter $\\lambda$ with the conventional power transformations (circle) show strong dependency on the overall location and scale of the distribution, ",
  "whereas estimates with the invariant power transformations (triangle) are constant."
)

.plot_shifted_distributions(manuscript_dir = manuscript_dir, plot_theme = plot_theme)
```

The result is shown in Figure \ref{fig:shifted-distributions}. For each distribution, transformation parameter $\hat{\lambda}$ varies with $d_{\text{shift}}$ and $d_{\text{scale}}$ when estimated using the original definitions.
In contrast, estimation of $\lambda$ using invariant power transformations was invariant to $d_{\text{shift}}$ and $d_{\text{scale}}$. 
While the effect of location on the estimated value of $\lambda$ using conventional power transformations is noticeable, scale also affects $\lambda$ for features that are not normally distributed.
Figure \ref{fig:shifted-distributions} also shows that estimated $\lambda$ for Box-Cox power transformations in left-skewed distributions diverges from that found for invariant Box-Cox power transformations.

## Empirical distribution yields consistent robust transformations

Outliers may be presented in the data, and affect estimation of power transformation parameters.
The log-likelihood function is weighted to assign less weight to outlier instances.
Weighting is based on either probabilities of the empirical distribution of the original feature, the z-score of the transformed feature values, or the residual error between the z-score of the transformed feature values and their expected z-score based on the normal distribution.
A weight is then assigned to each instance using one of three weighting functions: step, triangle and tapered cosine.
The weighting functions have one (step) or two (triangle, tapered cosine) parameters.
Here we investigate how these parameters should be set to find transformation parameter $\hat{\lambda}^r$ that are robust to outliers.

Let us randomly draw $m_d=100$ asymmetric generalised normal distributions.
Each distribution is parametrised with a randomly chosen skewness parameter $\alpha \sim U\left(0.01, 0.99\right)$ and shape parameter $\beta \sim U\left(1.00, 5.00 \right)$.
Location and scale parameters are set as $\mu = 0$ and $\sigma = 1$, respectively.
$n = \lceil 10^\gamma \rceil$ values are then randomly drawn, with $\gamma \sim U\left(2, 4\right)$, i.e., between $100$ and $10000$ values are drawn to create $\mathbf{X}_i$. 

Outlier values are then drawn to randomly replace a fraction of the values of $\mathbf{X}_i$.
This is repeated $m_{out} = 10$ times, with outlier fractions regularly spaced in $[0.00, 0.10]$.
Thus up to 10 percent of the values may be replaced by outliers.
Outlier values are set according to @Tukey1977-xm.
Let $x^{*} \sim U\left(-2, 2\right)$.
Then the corresponding outlier value is:

```{=latex}
\begin{equation}
x_{out} =
\begin{cases}
Q_1 - \left(1.5 - x^{*} \right) \text{IQR} & \text{if } x^{*} < 0 \\
Q_3 + \left(1.5 + x^{*} \right) \text{IQR} & \text{if } x^{*} \geq 0
\end{cases}
\end{equation}
```

Here, $Q_1$, $Q_3$ and $\text{IQR}$ are the first quartile, third quartile and interquartile range of $\mathbf{X}_i$, respectively.
Outlier values randomly replace values in $\mathbf{X}_i$ to create $\mathbf{X}_{i,j}$.

To find the optimal values for the weighting function parameters $k_1$ and $k_2$ (if applicable), we minimise the absolute difference between the $\hat{\lambda}^r$ parameter obtained for transformation in the presence of outliers, and the $\hat{\lambda}_0$ parameter obtained using the non-robust modified transformation in absence of outliers:

```{=latex}
\begin{equation}
\left\{ \hat{k}_1, \hat{k}_2 \right\} = \argmin_{k_1, k_2} \sum_{i=1}^{m_d} \sum_{j=1}^{m_{out}} \left| \hat{\lambda}^r \left(\mathbf{X}_{i, j}; k_1, k_2 \right) - \hat{\lambda}_{0} \left(\mathbf{X}_i \right) \right|
\end{equation}
```
The double sum in right-hand side of the above equation can be considered as the loss that should be minimised.
As mentioned earlier, transformation parameters are estimated using the BOBYQA algorithm for derivative-free bound constraint optimisation [@Powell2009-zb] through maximum likelihood estimation.

The optimal weighting function parameters for weighted MLE are shown in Tables \ref{tab:optimal-weighting-parameters-box-cox} and \ref{tab:optimal-weighting-parameters-yeo-johnson} for robust, shift-sensitive Box-Cox and Yeo-Johnson transformations, respectively.

```{r include = FALSE, eval = FALSE}
# This writes the lookup table for two-sided weighting parameters.
# power.transform package.
load(file = "./R/sysdata.rda")

two_sided_function_parameters <- .get_optimised_weighting_function_parameters(
  manuscript_dir = manuscript_dir,
  side = "both"
)

usethis::use_data(
  gof_lookup_table, 
  two_sided_function_parameters, 
  internal = TRUE, 
  overwrite = TRUE)
```

```{=latex}
\begin{table}
\begin{center}
\caption{Optimal weighting parameters and corresponding loss for \textbf{shift-sensitive Box-Cox} power transformations. $p^{*}$ indicates use of the empirical distribution of feature values, $z$ the z-score of the transformed feature values, and $r$ the residual error between the z-score of transformed feature values and the expected z-score according to the normal distribution. The \textit{initial} column shows the starting parameter value for the optimisation process, with the corresponding boundary values in the \textit{limits} column. The {optimal} column shows the optimal parameter values. The \textit{loss} column shows the loss achieved by each method, under optimised parameters. Lower loss indicates better robustness against outliers.}
\label{tab:optimal-weighting-parameters-box-cox}
\begin{tabular}{l r r r r r r r}

\toprule
method & \multicolumn{3}{c}{$k_1$} & \multicolumn{3}{c}{$k_2$} & loss \\
& initial & limits & optimal & initial & limits & optimal & \\

\midrule
non-robust               & ---  & ---       & ---  & ---  & ---       & ---  & 771 \\
$p^{*}$ (step)           & 0.80 & $(0, 1]$  & 0.86 & ---  & ---       & ---  & 561 \\
$p^{*}$ (triangle)       & 0.80 & $(0, 1]$  & 0.83 & 0.95 & $(0, 1]$  & 0.92 & 564 \\
$p^{*}$ (tapered cosine) & 0.80 & $(0, 1]$  & 0.76 & 0.95 & $(0, 1]$  & 0.95 & 560 \\
$z$ (step)               & 1.28 & $(0, 10]$ & 1.12 & ---  & ---       & ---  & 1153 \\
$z$ (triangle)           & 1.28 & $(0, 10]$ & 0.38 & 1.96 & $(0, 10]$ & 4.48 & 1178 \\
$z$ (tapered cosine)     & 1.28 & $(0, 10]$ & 1.21 & 1.96 & $(0, 10]$ & 4.92 & 1135 \\
$r$ (step)               & 0.50 & $(0, 10]$ & 2.18 & ---  & ---       & ---  & 1839 \\
$r$ (triangle)           & 0.50 & $(0, 10]$ & 1.29 & 1.00 & $(0, 10]$ & 1.47 & 1764 \\
$r$ (tapered cosine)     & 0.50 & $(0, 10]$ & 1.38 & 1.00 & $(0, 10]$ & 1.41 & 1583 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}
```


```{=latex}
\begin{table}
\begin{center}
\caption{Optimal weighting parameters and corresponding loss for \textbf{invariant Yeo-Johnson} power transformations. $p^{*}$ indicates use of the empirical distribution of feature values, $z$ the z-score of the transformed feature values, and $r$ the residual error between the z-score of transformed feature values and the expected z-score according to the normal distribution. The \textit{initial} column shows the starting parameter value for the optimisation process, with the corresponding boundary values in the \textit{limits} column. The {optimal} column shows the optimal parameter values. The \textit{loss} column shows the loss achieved by each method, under optimised parameters. Lower loss indicates better robustness against outliers.}
\label{tab:optimal-weighting-parameters-yeo-johnson}
\begin{tabular}{l r r r r r r r}

\toprule
method & \multicolumn{3}{c}{$k_1$} & \multicolumn{3}{c}{$k_2$} & loss \\
& initial & limits & optimal & initial & limits & optimal & \\

\midrule
non-robust               & ---  & ---       & ---  & ---  & ---       & ---  & 364 \\
$p^{*}$ (step)           & 0.80 & $(0, 1]$  & 0.95 & ---  & ---       & ---  & 224 \\
$p^{*}$ (triangle)       & 0.80 & $(0, 1]$  & 0.88 & 0.95 & $(0, 1]$  & 0.97 & 212 \\
$p^{*}$ (tapered cosine) & 0.80 & $(0, 1]$  & 0.94 & 0.95 & $(0, 1]$  & 0.95 & 218 \\
$z$ (step)               & 1.28 & $(0, 10]$ & 1.09 & ---  & ---       & ---  & 392 \\
$z$ (triangle)           & 1.28 & $(0, 10]$ & 0.24 & 1.96 & $(0, 10]$ & 4.62 & 396 \\
$z$ (tapered cosine)     & 1.28 & $(0, 10]$ & 0.15 & 1.96 & $(0, 10]$ & 6.05 & 413 \\
$r$ (step)               & 0.50 & $(0, 10]$ & 1.63 & ---  & ---       & ---  & 746 \\
$r$ (triangle)           & 0.50 & $(0, 10]$ & 1.50 & 1.00 & $(0, 10]$ & 1.79 & 731 \\
$r$ (tapered cosine)     & 0.50 & $(0, 10]$ & 1.51 & 1.00 & $(0, 10]$ & 1.57 & 612 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}
```

Figure \ref{fig:optimised-weighting-function-parameters} shows the distribution of errors $\left| \hat{\lambda}^r - \hat{\lambda}_0 \right|$ for non-robust transformations, as well as robust transformations using the optimal weighting function parameters. In the presence of outliers on either side of the distribution, the Yeo-Johnson transformation yields smaller errors than the Box-Cox transformation. For both transformations, weighting based on empirical probabilities yields the most consistent $\lambda$ parameter estimates in the presence of outliers. Other methods did not outperform the non-robust power transformation method.

```{r optimised-weighting-function-parameters, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "Robustness of power transformations after optimising weighting function parameters. ",
  "The distribution of errors, i.e. the difference between robustly fitted $\\hat{\\lambda}^r$ ",
  "and expected $\\hat{\\lambda}_{0}$ found in the absence of outliers, is shown for 1000 randomly ",
  "parametrised asymmetric generalised normal distributions with up to 10% outliers on either side ",
  "of the distribution. The top and bottom panel show errors for the invariant Box-Cox ",
  "and invariant Yeo-Johnson transformations, respectively. ",
  "In each panel, the error distribution for the non-robust transformation is shown to the left ",
  "for comparison with different weighting methods. ",
  "Note that a square root transformation was used along the $y$-axis. Median errors are shown above each distribution."
)

.plot_optimised_weighting_function_parameters(
  plot_theme = plot_theme,
  manuscript_dir = manuscript_dir
)
```


## Empirical central normality test

To develop an empirical test for central normality we need to consider two parameters: the central portion $\kappa$ as a fixed parameter, and test statistic $\tau_{\text{gof}}$. We will first define the central portion $\kappa$.

First we draw $m_d=10000$ random asymmetric generalised normal distributions. As before, each distribution is parametrised with a randomly chosen skewness parameter $\alpha \sim U\left(0.01, 0.99\right)$ and shape parameter $\beta \sim U\left(1.00, 5.00 \right)$. Location and scale parameters are set as $\mu = 0$ and $\sigma = 1$, respectively. $n = \lceil 10^\gamma \rceil$ values are then randomly drawn, with $\gamma \sim U\left(1.47, 3.00\right)$, which leads to between $30$ and $1000$ values being drawn to create $\mathbf{X}_i$. As before, for each distribution, up to $10 \%$ of instances are replaced by outlier values, and this is repeated $10$ times. We then compute residuals after optimising robust shift-sensitive transformations with the empirical tapered cosine weighting method.

Figure \ref{fig:empirical-central-normality-residual-error} shows the residual errors of the transformed distribution as function of the empirical probability. As expected, the largest deviations from normality appear on the extremities of this range. For Box-Cox, the effect of outliers on the lower end of the distribution leads to noticeable asymmetry. Between very low and very high empirical probabilities, residual errors are constrained and relatively flat. This is the candidate range for the central portion of the data.

```{r empirical-central-normality-residual-error, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "Residual error as a function of empirical probability. ",
  "Percentiles of the error are shown for robust, shift-sensitive transformations of 10000 randomly drawn asymmetric ",
  "generalised normal distributions, with 10 randomly drawn outliers distributions, ",
  "(up to 10% of samples) each. Larger errors occur for more extreme values at the edges of each distribution. ",
  "For comparison, the 99th percentile of residual errors for transformations of outlier-free distributions are shown (grey dashed line).")

.plot_residuals(
  plot_theme = plot_theme,
  manuscript_dir = manuscript_dir)

```
We then consider the empirical probability of type I errors: the probability of incorrectly classifying a centrally normal distribution as being non-normal based on the test statistic. Under the assumption that the asymmetric generalised normal distributions are centrally normal after robust transformation, this produces the relationship shown in Figure \ref{fig:empirical-central-normality-type-1-error-rate}. Since for $\kappa \leq 0.80$ error curves for both transformation methods are similar, we fix the value of the central portion $\kappa$ to 0.80. In the remaining we will use the empirical central normality test statistic values defined using robust shift-sensitive Yeo-Johnson transformations, as it is slightly more strict. This leads to test statistic values listed in Table \ref{tab:empirical-central-normality}

```{r empirical-central-normality-type-1-error-rate, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "Type 1 error rate of transformed asymmetric generalised normal distributions as function of ",
  "the test statistic for the central portion $\\kappa$ of the distribution.")

.plot_type_1_error_rate(
  plot_theme = plot_theme,
  manuscript_dir = manuscript_dir)

```

```{r include = FALSE, eval = FALSE}
# This writes the lookup table for the goodness of fit test to the
# power.transform package.
load(file = "./R/sysdata.rda")

gof_lookup_table <- .get_test_statistics_data(
  manuscript_dir = manuscript_dir,
  reduce = TRUE,
  as_table = FALSE)
gof_lookup_table[, "method" := NULL]

usethis::use_data(
  gof_lookup_table, 
  two_sided_function_parameters, 
  internal = TRUE, 
  overwrite = TRUE)
```

```{r include = FALSE, eval = FALSE}
# This generates the data for the table below.
table_data <- .get_test_statistics_data(manuscript_dir = manuscript_dir)
```

```{=latex}
\begin{table}
\begin{center}
\caption{Test statistic for empirical central normality at $\kappa = 0.80$ as a function of Type I error rate.}
\label{tab:empirical-central-normality}
\begin{tabular}{c c c c c c c}

\toprule
0.50 & 0.20 & 0.10 & 0.05 & 0.02 & 0.01 & 0.001 \\

\midrule
0.041 & 0.062 & 0.075 & 0.088 & 0.103 & 0.115 & 0.154 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}
```

In Figure \ref{fig:empirical-central-normality-examples} we apply the empirical central normality test to assess central normality of features that are composed of a mixture of samples drawn from two normal distributions. With increased separation of the underlying normal distributions, the probability of the feature being centrally normal decreases, as expected.

```{r empirical-central-normality-examples, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "Bi-modal distributions and empirical goodness of fit test results. ",
  "The feature (black) is a mixture of two identical sample sets (blue and orange) ",
  "drawn from normal distributions that are offset by a distance $d$. ",
  "We use the empirical goodness of fit test to compute p-values for the hypothesis that ",
  "the distribution is centrally normal. As may be observed, with increasing offset $d$ ",
  "the probability that the feature is centrally normal decreases. ",
  "Quantile-quantile plots are drawn below each distribution."
)

.plot_bimodal_distribution_test(
  plot_theme = plot_theme,
  manuscript_dir = manuscript_dir)

```

# Experimental Results

In the previous section, we simulated data to assess shift-sensitive power transformations and their robustness against outliers. In this section, we assess these aspects using real datasets.

## Datasets with outliers



### Top Gear dataset

```{r top-gear, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "Quantile-quantile plots for the fuel consumption data in the Top Gear dataset. ",
  "This data contains three strong outliers for highly efficient vehicles. ",
  "The left panel shows the quantile-quantile plot for the original data. ",
  "The middle panel shows the quantile-quantile plots for data transformed using ",
  "the original Yeo-Johnson transformation, and Raymaekers and Rousseeuw's robust adaptation \\cite{Raymaekers2021-kq}. ",
  "For these, the empirical central normality test yields $p=0.01$, and $p=0.58$, respectively. ",
  "The right panels shows the quantile-quantile plots for data transformed using ",
  "the non-robust and robust shift-sensitive transformations. ",
  "For these, the empirical central normality test yields $p=0.01$, and $p=0.36$, respectively. ",
  "Samples with observed quantiles below $-3.0$ or above $3.0$ are indicated by crosses."
)

.plot_top_gear(
  plot_theme = plot_theme
)
```

The Top Gear dataset contains data on 297 cars that appeared on the BBC television show *Top Gear*. Fuel consumption contains outliers. Figure \ref{fig:top-gear} shows that the outliers cause non-robust transformations to fail to transform the data to a centrally normal distribution (empirical central normality test $p = 0.01$). Robust transformations produce distributions that are centrally normal (empirical central normality test $p > 0.05$).

### Housing dataset

```{r housing-data, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "Quantile-quantile plots for the fuel consumption data in the Top Gear dataset. ",
  "This data contains three strong outliers for highly efficient vehicles. ",
  "The left panel shows the quantile-quantile plot for the original data. ",
  "The middle panel shows the quantile-quantile plots for data transformed using ",
  "the original Yeo-Johnson transformation, and Raymaekers and Rousseeuw's robust adaptation [@Raymaekers2021-kq]. ",
  "For these, the empirical central normality test yields $p=0.01$, and $p=0.58$, respectively. ",
  "The right panels shows the quantile-quantile plots for data transformed using ",
  "the non-robust and robust shift-sensitive transformations. ",
  "For these, the empirical central normality test yields $p=0.01$, and $p=0.36$, respectively. ",
  "Samples with observed quantiles below $-3.0$ or above $3.0$ are indicated by crosses."
)

.plot_sacramento_house(
  plot_theme = plot_theme
)
```

modeldata::Sacramento$price

Ames housing data set, year built

### Other data ???

## Integration into end-to-end machine learning

We used 285 datasets from the Penn Machine Learning Benchmarks collection [@Romano2022-gq].
In this collection, 122 datasets correspond to regression tasks and 163 datasets
to classification tasks. Using the familiar auto-machine learning library [@Zwanenburg2021-so] (version 1.5.0),
each dataset was used to train a model for each of 16 process configurations.
Each process configuration specifies the learner (generalised linear model or
random forest), transformation method (none, conventional Yeo-Johnson, robust
invariant Yeo-Johnson, robust invariant Yeo-Johnson with empirical central
normality test ($p = 0.01$)), and normalisation method (none,
$z$-standardisation), yielding 16 distinct configurations.
Before each experiment, each dataset was randomly split into a training (70%)
and holdout test (30%) set five times. Thus, a total of 22800 models were
created. Each model was then evaluated using the holdout test set using one of
two metrics, i.e. the root relative squared error (RRSE) for regression tasks and the
area under the receiver operating characteristic curve (AUC) for classification tasks.

For the purpose of assessing the effect of the difficulty of the task, we computed the median performance score over all models for each dataset and assigned one the following categories:

* very easy: $\text{AUC} \geq 0.90$ or $\text{RRSE} \leq 0.10$ (87 datasets)
* easy: $0.90 > \text{AUC} \geq 0.80$ or $0.30 \geq \text{RRSE} > 0.10$ (46 datasets)
* intermediate: $0.80 > \text{AUC} \geq 0.70$ or $0.60 \geq \text{RRSE} > 0.30$ (72 datasets)
* difficult: $0.70 > \text{AUC} \geq 0.60$ or $0.80 \geq \text{RRSE} > 0.60$ (60 datasets)
* very difficult: $0.60 > \text{AUC} \geq 0.50$ or $1.00 \geq \text{RRSE} > 0.80$ (12 datasets)
* unsolvable: $\text{AUC} < 0.50$ or $\text{RRSE} > 1.00$ (8 datasets)

To remove the effect of the dataset, and allow for comparing metrics, we ranked all performance scores for each dataset so that a higher rank corresponds to better performance.
Experiments yielding the same score received the same, average, rank.
Subsequently ranks were normalised to the $[0.0, 1.0]$ range.

```{r include = FALSE, eval = TRUE}

data <- .get_ml_experiment_data(manuscript_dir = manuscript_dir)

# Convert to data matrix. First aggregate within each experiment because the
# Friedman test is not for repeated measurements.
data <- data.table::dcast(
  data,
  dataset ~ learner + transformation_method + normalisation_method, value.var="value_rank",
  fun.aggregate = mean
)

friedman_test_result <- friedman.test(as.matrix(data[, "dataset" := NULL]))
```

Significant differences exist between process configurations (Friedman test: $p < 10^{-5})$.

```{r include = FALSE, eval = TRUE}

data <- .get_ml_experiment_data(manuscript_dir = manuscript_dir)

# Convert to data matrix. First aggregate within each experiment because the
# Wilcoxon signed rank test is not for repeated measurements.
data <- data.table::dcast(
  data,
  dataset ~ learner + transformation_method + normalisation_method, value.var="value_rank",
  fun.aggregate = mean
)

data[, "dataset" := NULL]
configurations <- colnames(data)
```

Something about direct comparisons.

To assess the marginal effects of process parameters, including transformation method, we first fit a regression random forest (ranger package [@Wright2017-rf] (version 0.16.0); 2000 trees, minimum node size 2, other hyperparameters default) with process parameters and task difficulty as predictors and normalised rank as response variable.

- Plot differences vs. none transformation method.
- Plot differences vs. none normalisation method and conventional transformation.

- Assess whether shift-invariant robust transformation improves machine learning results.

  - No transformation.
  - Conventional Yeo-Johnson transformation.
  - Shift sensitive Yeo-Johnson transformation.
  - Shift sensitive, robust Yeo-Johnson transformation.
  - Shift sensitive, robust Yeo-Johnson with central normality test.
  
  TODO: 
  - implement into familiar
  - update familiar.experiment to allow for setting external configurations (or maybe make that the default -- its definitely easier and more transparent.)
  
  - Add tests for out-of-bound actions (power_transform) and GOF test rejection (find_parameters).

# Discussion

In their work, Box and Cox already mention a transformation with a shift parameter, but preferred the now well-known version in Eq. **INSERT REFERENCE** for the theoretical analysis in their paper [@Box1964-mz].

# Conclusion

# Code availability

Shift-sensitive power transformations are implemented in the `power.transform` package. This manuscript was created using R Markdown, and is available from [https://github.com/alexzwanenburg/power.transform].






```{r optimal_weighting_function_all_criteria_box_cox, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "Things."
)
# This finds lambda values at the optimised weighting parameter settings.
data <- .get_transformation_parameters(manuscript_dir = manuscript_dir)

data <- data[method == "Box-Cox"]
data <- data[, "lambda_error" := abs(lambda - target_lambda)]
non_robust_data <- data[weight_method == "non-robust", mget(c("method", "estimation_method", "k", "ii", "lambda_error"))]
data[non_robust_data, on=c("method", "estimation_method", "k", "ii"), "non_robust_lambda_error" := i.lambda_error]

data[, "better_than_non_robust" := lambda_error <= non_robust_lambda_error]
data[k == 0.0, "better_than_non_robust" := NA_integer_]
data[weight_method == "non-robust", "better_than_non_robust" := NA_integer_]

pass_rate_data <- data[, list(
  "pass_rate" = sum(better_than_non_robust, na.rm=TRUE),
  "n" = sum(!is.na(better_than_non_robust)),
  "median_error" = stats::median(lambda_error)),
  by=c("method", "estimation_method", "weight_method")]
pass_rate_data <- pass_rate_data[, "pass_rate" := round(pass_rate / n * 100.0, 1)]

p <- ggplot2::ggplot(
  data = data,
  mapping = ggplot2::aes(
    x = weight_method,
    y = lambda_error,
    fill = estimation_method))
p <- p + plot_theme
p <- p + ggplot2::geom_violin(draw_quantiles = 0.5)
p <- p + ggplot2::facet_wrap("estimation_method", nrow=nlevels(data$estimation_method))
p <- p + paletteer::scale_fill_paletteer_d(
  palette = "ggthemes::Tableau_10",
  drop = FALSE)
p <- p + ggplot2::scale_y_sqrt(name = latex2exp::TeX("$| \\hat{\\lambda}^r - \\hat{\\lambda}_{0}|$"))
p <- p + ggplot2::xlab("weighting method")
p <- p + ggplot2::ggtitle("shift-sensitive Box-Cox transformation")
p <- p + ggplot2::theme(
  axis.text.x = ggplot2::element_text(
    angle=30.0,
    hjust=1.0))
p
```


```{r optimal_weighting_function_all_criteria_yeo_johnson, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "Things."
)
# This finds lambda values at the optimised weighting parameter settings.
data <- .get_transformation_parameters(manuscript_dir = manuscript_dir)

data <- data[method == "Yeo-Johnson"]
data <- data[, "lambda_error" := abs(lambda - target_lambda)]
non_robust_data <- data[weight_method == "non-robust", mget(c("method", "estimation_method", "k", "ii", "lambda_error"))]
data[non_robust_data, on=c("method", "estimation_method", "k", "ii"), "non_robust_lambda_error" := i.lambda_error]

data[, "better_than_non_robust" := lambda_error <= non_robust_lambda_error]
data[k == 0.0, "better_than_non_robust" := NA_integer_]
data[weight_method == "non-robust", "better_than_non_robust" := NA_integer_]

pass_rate_data <- data[, list(
  "pass_rate" = sum(better_than_non_robust, na.rm=TRUE),
  "n" = sum(!is.na(better_than_non_robust)),
  "median_error" = stats::median(lambda_error)),
  by=c("method", "estimation_method", "weight_method")]
pass_rate_data[, "pass_rate" := round(pass_rate / n * 100.0, 1)][order(pass_rate)]

p <- ggplot2::ggplot(
  data = data,
  mapping = ggplot2::aes(
    x = weight_method,
    y = lambda_error,
    fill = estimation_method))
p <- p + plot_theme
p <- p + ggplot2::geom_violin(draw_quantiles = 0.5)
p <- p + ggplot2::facet_wrap("estimation_method", nrow=nlevels(data$estimation_method))
p <- p + paletteer::scale_fill_paletteer_d(
  palette = "ggthemes::Tableau_10",
  drop = FALSE)
p <- p + guides(fill = ggplot2::guide_legend(title = "optimisation criterion"))
p <- p + ggplot2::scale_y_sqrt(name = latex2exp::TeX("$| \\hat{\\lambda}^r - \\hat{\\lambda}_{0}|$"))
p <- p + ggplot2::xlab("weighting method")
p <- p + ggplot2::ggtitle("shift-sensitive Yeo-Johnson transformation")
p <- p + ggplot2::theme(
  axis.text.x = ggplot2::element_text(
    angle=30.0,
    hjust=1.0))
p

```
# Appendix C: Right-sided outliers

```{r eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE}

.get_optimised_weighting_function_parameters(manuscript_dir = manuscript_dir, side = "right")

if(!file.exists(file.path(manuscript_dir, "robustness_comparison_rightside_outlier.RDS"))){
  set.seed(95)
  
  .compute_lambda <- function(x, method){
    # Create transformer object.
    transformer <- suppressWarnings(
      power.transform::find_transformation_parameters(
        x = x,
        method = method,
        robust = FALSE,
        shift = TRUE))
    
    return(transformer@lambda)
  }
  
  # Generate table of asymmetric generalised normal distribution parameters.
  n_distributions <- 100
  
  # Generate alpha, beta and n.
  n <- stats::runif(n=n_distributions, min=2, max=4)
  n <- ceiling(10^n)
  
  alpha <- stats::runif(n=n_distributions, min=0.01, max=0.99)
  beta <- stats::runif(n=n_distributions, min=1.00, max=5.00)
  
  # Generate corresponding distributions.
  x <- mapply(
    power.transform::ragn,
    n = n,
    alpha = alpha,
    beta = beta)
  
  # Compute lambda values without weighting for Box-Cox.
  target_lambda_bc <- sapply(
    x,
    .compute_lambda,
    method = "box_cox")
  
  # Compute lambda values without weighting for Yeo-Johnson
  target_lambda_yj <- sapply(
    x,
    .compute_lambda,
    method = "yeo_johnson")
  
  # Add outliers, between 0.00 and 0.10.
  n_outliers <- 10
  outlier_fraction <- (seq_len(n_outliers) - 1)^2 /((n_outliers-1)^2 * 20)
  
  # Find optimised k1 and k2 values for all combinations of weight sources and
  # weighting functions.
  x <- mapply(
    function(x, n, alpha, beta, target_lambda_bc, target_lambda_yj, outlier_fraction){
      
      parameters <- list("n" = n, "alpha"=alpha, "beta" = beta, "target_lambda_bc"=target_lambda_bc, "target_lambda_yj"=target_lambda_yj)
      
      x <- lapply(
        outlier_fraction,
        function(k, x, parameters){
          # Set parameters.
          parameters$k <- k
          
          # Compute interquartile range.
          interquartile_range <- stats::IQR(x)
          
          # Compute upper and lower quartiles.
          q_lower <- stats::quantile(x, probs=0.25, names=FALSE)
          q_upper <- stats::quantile(x, probs=0.75, names=FALSE)
          
          # Set data where the outliers will be copied into.
          x_outlier <- x
          
          if(k != 0.0){
            n_draw <- ceiling(k * length(x))
            
            # Generate outlier values that are smaller than Q1 - 1.5 IQR or larger
            # than Q3 + 1.5 IQR.
            x_random <- stats::runif(n_draw, min=0.0, max=2.0)
            outlier <- q_upper + 1.5 * interquartile_range + x_random * interquartile_range
            
            # Randomly insert outlier values.
            x_outlier[sample(seq_along(x), size=n_draw, replace=FALSE)] <- outlier
          }
          
          return(list(
            "x" = x_outlier,
            "parameters" = parameters))
        },
        x = x,
        parameters)
      
      return(x)
    },
    x = x,
    n = n,
    alpha = alpha,
    beta = beta,
    target_lambda_bc = target_lambda_bc,
    target_lambda_yj = target_lambda_yj,
    MoreArgs=list("outlier_fraction"=outlier_fraction),
    SIMPLIFY=FALSE,
    USE.NAMES=FALSE)
  
  x <- unlist(x, recursive = FALSE)
  
  # Optimise k1, k2 so that sum of absolute differences with target-lambda is
  # minimised for each combination of source and weighting method.
  experiment_args <- coro::generator(
    function(){
      
      for(method in c("box_cox", "yeo_johnson")){
        # Non-robust transformation.
        fun_args <- list(
          "method" = method,
          "robust" = FALSE,
          "shift" = TRUE)
        
        opt_args <- list()
        
        yield(list(
          "name" = "non-robust",
          "method" = method,
          "fun_args" = fun_args,
          "opt_args" = opt_args
        ))
      }
      
      for(robustness_source in c("original", "transformed", "residual")){
        
        def_opt_limits <- list("k1"=c(0.0, 10.0), "k2"=c(0.0, 10.0))
        
        if(robustness_source == "original"){
          def_opt_limits$k1[2] <- 1.0
          def_opt_limits$k2[2] <- 1.0
          
          def_opt_init <- list("k1" = 0.80, "k2" = 0.95)
          
        } else if(robustness_source == "transformed"){
          def_opt_init <- list("k1" = 1.28, "k2" = 1.96)
          
        } else if(robustness_source == "residual"){
          def_opt_init <- list("k1" = 0.50, "k2" = 1.0)
        }
        
        for(method in c("box_cox", "yeo_johnson")){
          for(robustness_weighting_function in c("step", "triangle", "cosine")){
            
            fun_args <- list(
              "method" = method,
              "robust" = TRUE,
              "shift" = TRUE,
              "weight_method" = paste0(robustness_source, "_", robustness_weighting_function),
              "backup_use_default" = FALSE)
            
            opt_limits <- def_opt_limits
            opt_init <- def_opt_init
            
            # Step-weighting only has k1 as a parameter.
            if(robustness_weighting_function == "step"){
              opt_limits$k2 <- NULL
              opt_init$k2 <- NULL
            }
            
            yield(list(
              "name" = paste0(robustness_source, "-", robustness_weighting_function),
              "method" = method,
              "fun_args" = fun_args,
              "opt_args" = list("initial" = opt_init, "limits" = opt_limits)
            ))
          }
        }
      }
    })
  
  # Function for computing transformer parameters under optimisation constraints.
  compute_robust_lambda <- function(
    x,
    fun_args,
    opt_args,
    cl_internal=NULL){
    
    # Custom parser.
    ..outlier_parser <- function(
    x,
    fun_args,
    opt_args
    ){
      # Create transformer.
      transformer <- suppressWarnings(do.call(
        power.transform::find_transformation_parameters,
        args=c(
          list("x" = x$x),
          fun_args,
          opt_args)))
      
      parameter_data <- c(
        x$parameters,
        opt_args,
        list(
          "method" = fun_args$method,
          "weight_method" = fun_args$weight_method,
          "lambda" = transformer@lambda,
          "shift" = transformer@shift)
      )
      
      return(parameter_data)
    }
    
    if(is.null(cl_internal)){
      parameter_data <- lapply(
        x,
        ..outlier_parser,
        fun_args = fun_args,
        opt_args = opt_args)
      
    } else {
      parameter_data <- parallel::parLapply(
        cl = cl_internal,
        X = x,
        fun = ..outlier_parser,
        fun_args = fun_args,
        opt_args = opt_args)
    }
    
    return(parameter_data)
  }
  
  
  # Inner optimisation function that sets the loss.
  ..optimisation_function <- function(
    opt_param,
    x,
    fun_args,
    cl_internal = NULL){
    
    # Parse options.
    opt_args <- list()
    if(length(opt_param) >= 1) opt_args <- c(opt_args, list("k1"=opt_param[1]))
    if(length(opt_param) >= 2) opt_args <- c(opt_args, list("k2"=opt_param[2]))
    
    parameter_data <- compute_robust_lambda(
      x = x,
      fun_args = fun_args,
      opt_args = opt_args,
      cl_internal = cl_internal)
    
    if(fun_args$method == "box_cox"){
      lambda_error <- sapply(
        parameter_data,
        function(x) (abs(x$lambda - x$target_lambda_bc)))
      
    } else {
      # Yeo-Johnson.
      lambda_error <- sapply(
        parameter_data,
        function(x) (abs(x$lambda - x$target_lambda_yj)))
    }
    
    return(sum(lambda_error))
  }
  
  
  # Outer optimisation function
  .optimisation_function <- function(
    experiment,
    data,
    cl_internal = NULL){
    
    set.seed(9)
    
    # Run optimiser.
    if(length(experiment$opt_args) > 0){
      
      x0 <- unlist(experiment$opt_args$initial)
      
      lower <- experiment$opt_args$limits$k1[1]
      upper <- experiment$opt_args$limits$k1[2]
      
      if(length(experiment$opt_args$limits) > 1){
        lower <- c(lower, experiment$opt_args$limits$k2[1])
        upper <- c(upper, experiment$opt_args$limits$k2[2])
      }
      
      h <- nloptr::sbplx(
        x0 = x0,
        fn = .optimisation_inner,
        lower = lower,
        upper = upper,
        control=list("xtol_rel"=1e-3, "ftol_rel"=1e-4),
        x = data,
        fun_args = experiment$fun_args,
        cl_internal = cl_internal)
      
      if(length(h$par) == 1){
        return(list("name"=experiment$name, "method"=experiment$method, "k1" = h$par, "value" = h$value))
        
      } else {
        return(list("name"=experiment$name, "method"=experiment$method, "k1" = h$par[1], "k2" = h$par[2], "value" = h$value))
      }
      
    } else {
      value <- ..optimisation_function(
        opt_param=list(),
        x = data,
        fun_args=experiment$fun_args,
        cl = cl_internal)
      
      return(list("name"=experiment$name, "method"=experiment$method, "value" = value))
    }
  }
  
  
  # Start cluster
  cl <- parallel::makeCluster(18L)
  
  parallel::clusterExport(
    cl=cl,
    varlist = c("compute_robust_lambda", "..optimisation_function")
  )
  
  experiment_results <- parallel::parLapplyLB(
    cl = cl,
    X = coro::collect(experiment_args()),
    fun = .optimisation_function,
    data = x,
    chunk.size = 1L)
  
  # Stop cluster.
  parallel::stopCluster(cl)
  
  saveRDS(experiment_results, file = file.path(manuscript_dir, "robustness_comparison_rightside_outlier.RDS"))
}
```

# Appendix D: Generalisability of empirical goodness of fit test

The empirical goodness of fit test was characterised using the empirical distribution of type I errors related to the mean residual error in the central portion ($\kappa = 0.80$) of the transformed feature distribution, in the presence of outliers. The corresponding power transformations were both robust and shift sensitive, and used maximum likelihood estimation (MLE) for optimisation of transformation parameters. An important concern is how well the test performs for conventional power transformations (in absence of outliers), and for optimisation criteria that are objectively better than MLE, i.e. the Cramér-von Mises criterion.

For Figure *insert* determined type 1 error rate as before, but for several variants. In addition to robust, shift sensitive transformations using MLE, conventional transformations and robust, shift-sensitive using the Crámer-von Mises criterion were assessed. Conventional transformations only used randomly drawn asymmetric generalised normal distributions without added outliers. This figure suggests two things: If conventional transformations were used instead of robust, shift sensitive variants, the test characteristics would be highly similar. However, the robust, shift sensitive variant shows that the test characteristics are even stable in the presence of outliers. Secondly, the optimisation criterion does directly matter, and compared to MLE, the Crámer-von Mises criterion results in transformed distributions that better approximate central normality. In this case, one could argue whether characteristics of the empirical goodness of fit test should be based on the Crámer-von Mises criterion instead of MLE. We would argue that MLE-based test characteristics form a good foundation, for two reasons. One, MLE is the most prevalent optimisation criterion for power transformations, and two, the resulting transformations still approach central normality.

```{r gof_test_type_1_error_rate_appendix, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "Type I error rates for power transformation variants. ",
  "The robust, shift sensitive power transformation using maximum likelihood estimation (MLE) is shown as comparison. ",
  "Optimisation of transformation parameters using the Crámer-von Mises (C-vM) criterion leads to lower residual errors compared to MLE, ",
  "particularly for Box-Cox transformations, i.e. they lead to transformations that are closer to central normality. ",
  "Conventional transformations (in the absence of outliers) yield curves that are comparable to their robust, shift sensitive variants."
)

.plot_type_1_error_rate_appendix(
  plot_theme = plot_theme,
  manuscript_dir = manuscript_dir)

```


# References
