---
title: "Appendix"
author: "Alex Zwanenburg, Steffen Löck"
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: lualatex
    keep_tex: true
papersize: a4
bibliography: refs.bib
header-includes:
- \usepackage{amsmath}
- \usepackage{amsthm}
- \usepackage{booktabs}
- \usepackage{multirow}
- \usepackage{placeins}
- \newtheorem{definition}{Definition}
- \DeclareMathOperator*{\argmax}{argmax}
- \DeclareMathOperator*{\argmin}{argmin}
- \DeclareMathOperator{\sgn}{sgn}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  fig.align = "center",
  dev = "cairo_pdf"
)

# Allow for defining figure captions within a chunk.
knitr::opts_knit$set(
  eval.after = "fig.cap")

require(power.transform)
require(ggplot2)
require(patchwork)
require(data.table)
require(latex2exp)
require(coro)
require(paletteer)
require(robustHD)

manuscript_dir <- file.path("C:/Users/alexz/Documents/GitHub/power.transform/manuscript_rmarkdown")
source(file.path(manuscript_dir, "simulations.R"))
source(file.path(manuscript_dir, "plots.R"))

# Base font size.
base_size <- 9

# Set general theme.
plot_theme <- ggplot2::theme_light(base_size=base_size)
plot_theme$plot.title$size <- ggplot2::rel(1.0)
plot_theme$plot.title$hjust <- 0.5
plot_theme$strip.background <- ggplot2::element_blank()
plot_theme$strip.text$colour <- NULL
plot_theme$plot.margin <- grid::unit(c(2, 2, 2, 2), "points")
plot_theme$legend.key.size <- grid::unit(0.3, 'lines')
plot_theme$plot.tag <- ggplot2::element_text(
  size = ggplot2::rel(1.0),
  hjust = 0.0,
  vjust = 0.0,
  inherit.blank = FALSE
)
plot_theme$plot.tag.position <- c(0.2, 1)

parse_mean_sd_latex <- function(mu, sigma, digits=2L){
  return(paste0(
    "$", round(mu, digits=digits), " \\pm ", round(sigma, digits=digits), "$"
  ))
}

```

# Appendix A: Log-likelihood functions for location- and scale-invariant power transformation

Location- and scale-invariant Box-Cox and Yeo-Johnson transformations are parametrised using location $x_0$ and scale $s$ parameters, in addition to transformation parameter $\lambda$.
This leads to the following transformations. The location- and scale-invariant Box-Cox transformation is:

```{=latex}
\begin{equation}
\phi_{\text{BC}}^{\lambda, x_0, s} (x_i) = 
\begin{cases}
\left( \left(\frac{x_i - x_0}{s} \right)^\lambda - 1 \right) / \lambda & \text{if } \lambda \neq 0\\
\log\left[\frac{x_i - x_0}{s}\right] & \text{if } \lambda = 0
\end{cases}
\end{equation}
```
where $x_i - x_0 > 0$.
The location- and scale-invariant Yeo-Johnson transformation is:

```{=latex}
\begin{equation}
\phi_{\text{YJ}}^{\lambda, x_0, s} (x_i) = 
\begin{cases}
\left( \left( 1 + \frac{x_i - x_0}{s}\right)^\lambda - 1\right) / \lambda & \text{if } \lambda \neq 0 \text{ and } x_i - x_0 \geq 0\\
\log\left[1 + \frac{x_i - x_0}{s}\right] & \text{if } \lambda = 0 \text{ and } x_i - x_0 \geq 0\\
-\left( \left( 1 - \frac{x_i - x_0}{s}\right)^{2 - \lambda} - 1 \right) / \left(2 - \lambda \right) & \text{if } \lambda \neq 2 \text{ and } x_i - x_0 < 0\\
-\log\left[1 - \frac{x_i - x_0}{s}\right] & \text{if } \lambda = 2 \text{ and } x_i - x_0 < 0
\end{cases}
\end{equation}
```

The parameters of these power transformations can be optimised by maximising the
log-likelihood function, under the assumption that the transformed feature $\phi^{\lambda, x_0, s} (\mathbf{X})$ follows a normal distribution.
The log-likelihood functions for conventional Box-Cox and Yeo-Johnson transformations are well-known.
However, the introduction of scaling parameter $s$ prevents their direct use.
Here, we first derive the general form of the log-likelihood functions, and then derive their power-transformation specific definitions.

Let $f(x_1, \ldots, x_n)$ be the probability density function of feature $\mathbf{X} = \{ x_1, \ldots, x_n\}$, 
and $f^{\lambda, x_0, s} (\phi^{\lambda, x_0, s}(x_1), \ldots, \phi^{\lambda, x_0, s}(x_n))$ be the probability density function of 
the transformed feature $\phi^{\lambda, x_0, s} (\mathbf{X})$, that is assumed to follow a normal distribution.

The two probability density functions are related as follows:

```{=latex}
\begin{equation}
f^{\lambda, x_0, s}(x_1, \ldots, x_n) = f^{\lambda, x_0, s} (\phi^{\lambda, x_0, s}(x_1), \ldots, \phi^{\lambda, x_0, s}(x_n)) \left|\mathbf{J}\right|
\end{equation}
```
Where, $\left|\mathbf{J}\right|$ is the determinant of Jacobian $\mathbf{J}$.
The Jacobian takes the following form, with off-diagonal elements $0$:
```{=latex}
\begin{equation}
\mathbf{J} =
\begin{bmatrix}
    \frac{\partial}{\partial x_1} \phi^{\lambda, x_0, s}(x_1) & 0 & \dots & 0 \\
    0 & \frac{\partial}{\partial x_2} \phi^{\lambda, x_0, s}(x_2) & \dots & 0 \\
    \vdots & \vdots  & \ddots &  \vdots \\
    0  & 0 & 0 & \frac{\partial}{\partial x_n} \phi^{\lambda, x_0, s}(x_n)
\end{bmatrix}
\end{equation}
```

Thus, $\left| \mathbf{J} \right| = \prod_{i=1}^n \frac{\partial}{\partial x_i} \phi^{\lambda, x_0, s}(x_i)$.

Since in our situation $\{x_1, \ldots, x_n\}$ in $f^{\lambda, x_0, s}(x_1, \ldots, x_n)$ are considered fixed (i.e., known), $f^{\lambda, x_0, s}(x_1, \ldots, x_n)$ may be considered a likelihood function.
The log-likelihood function $\mathcal{L}^{\lambda, x_0, s}$ is then:
```{=latex}
\begin{equation}
\begin{split}
\mathcal{L}^{\lambda, x_0, s} & = \log f^{\lambda, x_0, s}(x_1, \ldots, x_n) \\
 & = \log \left[ f^{\lambda, x_0, s} (\phi^{\lambda, x_0, s}(x_1), \ldots, \phi^{\lambda, x_0, s}(x_n)) \right] + \log \left|\mathbf{J}\right| \\
 & = \log \left[ f^{\lambda, x_0, s} (\phi^{\lambda, x_0, s}(x_1), \ldots, \phi^{\lambda, x_0, s}(x_n)) \right] + \log \prod_{i=1}^n \frac{\partial}{\partial x_i} \phi^{\lambda, x_0, s}(x_i) \\
 & = -\frac{n}{2} \log \left[2 \pi \sigma^2 \right] -\frac{1}{2 \sigma^2} \sum_{i=1}^n \left( \phi^{\lambda, x_0, s}(x_i) - \mu \right)^2 + \sum_{i=1}^n \log \left[ \frac{\partial}{\partial x_i} \phi^{\lambda, x_0, s}(x_i)\right]
\end{split}
\end{equation}
```
With $\mu$ the average of $\phi^{\lambda, x_0, s}(\mathbf{X})$ and $\sigma^2$ its variance.
The first two terms derive directly from the log-likelihood function of a normal distribution, and are not specific to the type of power transformation used.
However, the final term differs between Box-Cox and Yeo-Johnson transformations.

## Location- and scale-invariant Box-Cox transformation

For the location- and scale-invariant Box-Cox transformation the partial derivative is:
```{=latex}
\begin{equation}
\begin{split}
\frac{\partial}{\partial x_i} \phi_{\text{BC}}^{\lambda, x_0, s}(x_i) & = \frac{1}{s} \left(\frac{x_i - x_0}{s} \right)^{\lambda-1} \\
 & = \frac{1} {s^\lambda} \left(x_i - x_0 \right)^{\lambda - 1}
\end{split}
\end{equation}
```

Thus the final term in $\mathcal{L}_{\text{BC}}^{\lambda, x_0, s}$ is:
```{=latex}
\begin{equation}
\begin{split}
\sum_{i=1}^n \log \frac{\partial}{\partial x_i} \phi_{\text{BC}}^{\lambda, x_0, s}(x_i) & = \sum_{i=1}^n \log \left[ s^{-\lambda} (x_i - x_0)^{\lambda - 1} \right] \\
& = \sum_{i=1}^n \log \left[s^{-\lambda} \right] + \log \left[ (x_i - x_0)^{\lambda - 1} \right]\\
& = -n \lambda \log s + \left( \lambda - 1 \right) \sum_{i=1}^n \log \left[ x_i - x_0 \right]
\end{split}
\end{equation}
```

This leads to the following log-likelihood:
```{=latex}
\begin{equation}
\begin{split}
\mathcal{L}_{\text{BC}}^{\lambda, x_0, s} = & -\frac{n}{2} \log \left[2 \pi \sigma^2 \right] -\frac{1}{2 \sigma^2} \sum_{i=1}^n \left( \phi^{\lambda, x_0, s}(x_i) - \mu \right)^2 \\
& -n \lambda \log s + \left( \lambda - 1 \right) \sum_{i=1}^n \log \left[ x_i - x_0 \right]
\end{split}
\end{equation}
```

Similarly to @Raymaekers2024-zf, sample weights $w_i$ are introduced to facilitate robust power transformations.
The weighted log-likelihood of the location- and scale-invariant Box-Cox transformation is:
```{=latex}
\begin{equation}
\begin{split}
\mathcal{L}_{\text{rBC}}^{\lambda, x_0, s} = & -\frac{1}{2} \left(\sum_{i=1}^n w_i \right) \log \left[ 2 \pi \sigma_w^2 \right] -\frac{1}{2 \sigma_w^2} \sum_{i=1}^n w_i \left( \phi^{\lambda, x_0, s}(x_i) - \mu_w \right)^2 \\
& - \lambda \left( \sum_{i=1}^n w_i \right) \log s + \left( \lambda - 1 \right) \sum_{i=1}^n w_i \log \left[ x_i - x_0 \right]
\end{split}
\end{equation}
```
where $\mu_w$ and $\sigma^2_w$ are the weighted mean and weighted variance of the Box-Cox transformed feature $\phi_{\text{BC}}^{\lambda, x_0, s} (\mathbf{X})$, respectively:

```{=latex}
\begin{equation}
\sigma_w^2 = \frac{\sum_{i=1}^n w_i \left(\phi_{\text{BC}}^{\lambda, x_0, s} (x_i) - \mu_w \right)^2}{\sum_{i=1}^n w_i} \quad \text{with } \mu_w = \frac{\sum_{i=1}^n \phi_{\text{BC}}^{\lambda, x_0, s} (x_i)} {\sum_{i=1}^n w_i}
\end{equation}
```

## Location- and scale-invariant Yeo-Johnson transformation

For the location- and scale-invariant Yeo-Johnson transformation, the partial derivative is:
```{=latex}
\begin{equation}
\frac{\partial}{\partial x_i} \phi_{\text{YJ}}^{\lambda, x_0, s}(x_i) =
\begin{cases}
\frac{1}{s} \left(1 + \frac{x_i - x_0}{s}\right)^{\lambda - 1} & \text{if } x_i - x_0 \geq 0\\
\frac{1}{s} \left(1 - \frac{x_i - x_0}{s}\right)^{1 - \lambda} & \text{if } x_i - x_0 < 0
\end{cases}
\end{equation}
```

Thus the final term in $\mathcal{L}_{\text{YJ}}^{\lambda, x_0, s}$ is:
```{=latex}
\begin{equation}
\begin{split}
\sum_{i=1}^n \log \frac{\partial}{\partial x_i} \phi_{\text{YJ}}^{\lambda, x_0, s}(x_i) & = - n \log s + (\lambda - 1) \sum_{i=1}^n \sgn(x_i - x_0) \log \left[1 + \frac{|x_i - x_0|}{s} \right]
\end{split}
\end{equation}
```

This leads to the following log-likelihood:
```{=latex}
\begin{equation}
\begin{split}
\mathcal{L}_{\text{YJ}}^{\lambda, x_0, s} = & -\frac{n}{2} \log\left[2 \pi \sigma^2\right] -\frac{1}{2 \sigma^2} \sum_{i=1}^n \left( \phi^{\lambda, x_0, s}(x_i) - \mu \right)^2 \\
& - n \log s + (\lambda - 1) \sum_{i=1}^n \sgn(x_i - x_0) \log \left[1 + \frac{|x_i - x_0|}{s} \right]
\end{split}
\end{equation}
```

The weighted log-likelihood for location- and scale-invariant Yeo-Johnson transformation is:
```{=latex}
\begin{equation}
\begin{split}
\mathcal{L}_{\text{rYJ}}^{\lambda, x_0, s} = & -\frac{1}{2} \left(\sum_{i=1}^n w_i \right) \log \left[ 2 \pi \sigma_w^2 \right] -\frac{1}{2 \sigma_w^2} \sum_{i=1}^n w_i \left( \phi^{\lambda, x_0, s}(x_i) - \mu_w \right)^2 \\
& - \left( \sum_{i=1}^n w_i \right) \log s + (\lambda - 1) \sum_{i=1}^n w_i \sgn(x_i - x_0) \log \left[1 + \frac{|x_i - x_0|}{s} \right]
\end{split}
\end{equation}
```
where $\mu_w$ and $\sigma^2_w$ are the weighted mean and weighted variance of the Yeo-Johnson transformed feature $\phi_{\text{YJ}}^{\lambda, x_0, s} (\mathbf{X})$:

```{=latex}
\begin{equation}
\sigma_w^2 = \frac{\sum_{i=1}^n w_i \left(\phi_{\text{YJ}}^{\lambda, x_0, s} (x_i) - \mu_w \right)^2}{\sum_{i=1}^n w_i} \quad \text{with } \mu_w = \frac{\sum_{i=1}^n \phi_{\text{YJ}}^{\lambda, x_0, s} (x_i)} {\sum_{i=1}^n w_i}
\end{equation}
```


# Appendix B: Optimisation of transformation parameters

Maximum likelihood estimation (MLE) is commonly used to optimise parameters for power transformation.
Generally, optimisation requires minimisation or maximisation of a criterion.
In MLE, the maximised criterion is the log-likelihood function of the normal distribution.
Here, we investigate power transformation using optimisation criteria that are closely related to test statistics for normality tests.

Let $\mathbf{X}$ be a feature with ordered feature values,
and $\mathbf{Y}^\lambda =\phi^{\lambda} \left(\mathbf{X} \right)$ and
$\mathbf{Y}^{\lambda, x_0, s} =\phi^{\lambda, x_0, s} \left(\mathbf{X} \right)$ 
its transformed values using conventional and location- and scale-invariant power transformations, respectively.
Since power transformations are monotonic, $\mathbf{Y}$ will likewise be ordered.

Below we will focus on criteria based on the empirical density function and those based on skewness and kurtosis of the transformed feature.
Other potential criteria, such as the Shapiro-Wilk test statistic [@Shapiro1965-zd] are not investigated here.
In the case of the Shapiro-Wilk test statistic this is because of lack of scalability to features with many ($> 5000$) instances, and because adapting the test statistic to include weights is not straightforward.

## Empirical density function-based criteria

The first class of criteria is based on the empirical distribution function (EDF).
Transformation parameters are then fit through minimisation of the distance between the empirical distribution function $F_{\epsilon}$ and the cumulative density function (CDF) of the normal distribution $F_{\mathcal{N}}$.
Let $F_{\epsilon}\left(x_i \right) = \frac{i - 1/3}{n + 1/3}$ be the empirical probability of instance $i$.
The normal distribution is parametrised by location parameter $\mu$ and scale parameter $\sigma$, both of which have to be estimated from the data. 
For non-robust power transformations, $\mu$ and $\sigma$ are sample mean and sample standard deviation, respectively.
For robust power transformations, we estimate $\mu$ and $\sigma$ as Huber M-estimates of location and scale of the transformed feature $\phi^{\lambda, x_0, s} (\mathbf{X})$ [@Huber1981-su].

### Anderson-Darling criterion

The Anderson-Darling criterion is based on the empirical distribution function of $\mathbf{X}$.
We define this criterion as follows:

```{=latex}
\begin{equation}
U_{\text{AD}} \left(\mathbf{X}, \lambda, x_0 \right) = \frac{1}{\sum_{i=1}^n w_i} \sum_{i=1}^n w_i \frac{\left( F_{\epsilon}\left(x_i \right) - F_{\mathcal{N}} \left(\phi^{\lambda, x_0, s} \left(x_i \right); \mu, \sigma \right) \right)^2} {F_{\mathcal{N}} \left(\phi^{\lambda, x_0, s} \left(x_i \right); \mu, \sigma \right) \left(1 - F_{\mathcal{N}} \left(\phi^{\lambda, x_0, s} \left(x_i \right); \mu, \sigma \right) \right) }
\end{equation}
```

Here $w_i$ are weights, and $\mu$ and $\sigma$ are location and scale parameters. 
For non-robust power transformations, all $w_i = 1$.
Note that this criterion is not the same as the Anderson-Darling test statistic [@Anderson1952-gz],
which involves solving (or approximating) an integral function,
contains an extra scalar multiplication term, and does not include weights.
The Anderson-Darling criterion seeks to minimise the squared Euclidean distance 
between the EDF and the normal CDF, with differences at the upper and lower end
of the normal CDF receiving more weight than those at the centre of the CDF.

### Cramér-von Mises criterion

The Cramér-von Mises criterion is also based on the empirical distribution function of $\mathbf{X}$.
We define the Cramér-von Mises criterion as follows:
```{=latex}
\begin{equation}
U_{\text{CvM}} \left(\mathbf{X}, \lambda, x_0 \right) = \frac{1}{\sum_{i=1}^n w_i} \sum_{i=1}^n w_i \left( F_{\epsilon}\left(x_i \right) - F_{\mathcal{N}} \left(\phi^{\lambda, x_0, s} \left(x_i \right); \mu, \sigma \right) \right)^2
\end{equation}
```

Here $w_i$ are weights, and $\mu$ and $\sigma$ are location and scale parameters.
For non-robust power transformations, all $w_i = 1$.
The criterion is similar to the Cramér-von Mises test statistic [@Cramer1928-rc, @Von_Mises1928-ef],
aside from an additive scalar value and the introduction of weights.
This criterion, like the Anderson-Darling criterion, seeks to minimise the squared Euclidean distance between the EDF and the normal CDF.
Unlike the Anderson-Darling criterion, this criterion weights all instances equally.

For conventional power transformations with a fixed shift parameter,
the transformation $\phi^{\lambda, x_0, s} (\mathbf{X})$ may be substituted 
by $\phi^{\lambda} (\mathbf{X})$ in the definition of the Cramér-von Mises criterion.

## Skewness-kurtosis-based criteria

The second class of criteria seeks to reduce skewness and (excess) kurtosis of the transformed feature $\mathbf{Y}$.
We will first define the location $\mu$ and scale $\sigma$ of the transformed feature
as these are required for computing skewness and kurtosis.
Here, $\mu$ is defined as:

```{=latex}
\begin{equation}
\mu = \frac{\sum_{i=1}^n \phi^{\lambda, x_0, s} \left(x_i \right)} {\sum_{i=1}^n w_i}
\end{equation}
```

The location, or mean, is weighted by $w_i$, which equals $1$ for non-robust transformations.

Then, $\sigma^2$ is defined as:
```{=latex}
\begin{equation}
\sigma^2 = \frac{\sum_{i=1}^n w_i \left(\phi^{\lambda, x_0, s} \left( x_i \right) - \mu \right)^2}{\sum_{i=1}^n w_i}
\end{equation}
```

Skewness is defined as:
```{=latex}
\begin{equation}
s = \frac{\sum_{i=1}^n w_i \left(\phi^{\lambda, x_0, s} \left( x_i \right) - \mu \right)^3}{\sigma^3 \sum_{i=1}^n w_i}
\end{equation}
```

Kurtosis is defined as:
```{=latex}
\begin{equation}
k = \frac{\sum_{i=1}^n w_i \left(\phi^{\lambda, x_0, s} \left( x_i \right) - \mu \right)^4}{\sigma^4 \sum_{i=1}^n w_i}
\end{equation}
```

### D'Agostino criterion

The D'Agostino criterion defined here follows the D'Agostino $K^2$ test statistic [@DAgostino1990-kp].
This test statistic is composed of two separate test statistics, one of which is related to skewness, and the other to kurtosis.
Both test statistics are computed in several steps.
Let us first define $\nu=\sum_{i=1}^n w_i$.
Thus for non-robust power transformations, $\nu = n$.

For the skewness test statistic we first compute [@DAgostino1990-kp]:

```{=latex}
\begin{equation}
\beta_1 = s \sqrt{ \frac{\left(\nu + 1\right) \left(\nu + 3\right)} {6 \left(\nu - 2\right)} }
\end{equation}

\begin{equation}
\beta_2 = 3 \frac{\left(\nu^2 + 27\nu - 70\right) \left(\nu + 1\right) \left(\nu + 3\right)} {\left(\nu - 2\right) \left(\nu + 5\right) \left(\nu + 7\right) \left(\nu + 9\right)}
\end{equation}

\begin{equation}
\alpha = \sqrt{\frac{2} {\sqrt{2 \beta_2 - 2} - 2}}
\end{equation}

\begin{equation}
\delta = \frac{1}{\sqrt{\log \left[\sqrt{-1 + \sqrt{2 * \beta_2 - 2}} \right]}}
\end{equation}
```

The skewness test statistic is then:

```{=latex}
\begin{equation}
Z_s = \delta \log\left[\frac{\beta_1}{\alpha} + \sqrt{\frac{\beta_1^2}{\alpha^2} + 1} \right]
\end{equation}
```

For the kurtosis test statistic we first compute [@DAgostino1990-kp; @Anscombe1983-nz]:

```{=latex}
\begin{equation}
\beta_1 = 3 \frac{\nu - 1}{\nu + 1}
\end{equation}

\begin{equation}
\beta_2 = 24 \nu \frac{\left(\nu - 2\right)\left(\nu - 3\right)}{\left(\nu + 1\right)^2 \left(\nu + 3\right) \left(\nu + 5\right)}
\end{equation}

\begin{equation}
\beta_3 = 6 \frac{\nu^2 - 5 \nu + 2}{\left(\nu + 7\right) \left(\nu + 9\right)} \sqrt{6 \frac{\left(\nu + 3\right) \left(\nu + 5\right)}{\nu \left(\nu - 2\right) \left(\nu - 3 \right)}}
\end{equation}

\begin{equation}
\alpha_1 = 6 + \frac{8}{\beta_3} \left[\frac{2}{\beta_3} + \sqrt{1 + \frac{4}{\beta_3^2}} \right]
\end{equation}

\begin{equation}
\alpha_2 = \frac{k - \beta_1}{\sqrt{\beta_2}}
\end{equation}
```

The kurtosis test statistic is then:

```{=latex}
\begin{equation}
Z_k = \sqrt{\frac{9 \alpha_1}{2}} \left[ 1 - \frac{2}{9 \alpha_1} - \left(\frac{1 - 2 / \alpha_1}{1 + \alpha_2 \sqrt{2 / \left(\alpha_1 - 4 \right)}} \right)^{1 / 3}  \right]
\end{equation}
```

The D'Agostino $K^2$ test statistic and our criterion are the same, and are defined as:
```{=latex}
\begin{equation}
U_{\text{DA}} \left(\mathbf{X}, \lambda, x_0 \right) = Z_s^2 + Z_k^2
\end{equation}
```

The main difference between the test statistic as originally formulated, and the criterion proposed here is the presence of weights for robust power transformation.

### Jarque-Bera criterion

The second criterion based on skewness and kurtosis is the Jarque-Bera criterion.
It is relatively simple to compute compared to the D'Agostino criterion:
```{=latex}
\begin{equation}
U_{\text{JB}} \left(\mathbf{X}, \lambda, x_0 \right) = s^2 + \left(k - 3\right)^2 / 4
\end{equation}
```

The main difference between the above criterion and the Jarque-Bera test statistic [@Jarque1980-hw] is that a scalar multiplication is absent.

## Optimisation using non-MLE criteria

Each of the above criteria can be used for optimisation, i.e.:

```{=latex}
\begin{equation}
\left\{ \hat{\lambda}, \hat{x}_0, \hat{s}_0 \right\} = \argmin_{\lambda, x_0, s} U\left(\mathbf{X}, \lambda, x_0, s \right)
\end{equation}
```

For conventional power transformations with fixed location and scale parameters, 
the transformation $\phi^{\lambda, x_0, s} (\mathbf{X})$ may be substituted by $\phi^{\lambda} (\mathbf{X})$, 
or equivalently, $x_0$ and $s$ may be fixed:
```{=latex}
\begin{equation}
\left\{ \hat{\lambda}\right\} = \argmin_{\lambda} U\left(\mathbf{X}, \lambda; x_0, s \right)
\end{equation}
```


## Simulations with other optimisation criteria

Invariance of location- and scale-invariant power transformations was assessed 
using the optimisation criteria in [Appendix B](Appendix B: Optimisation of transformation parameters).
This follows the simulation in the main manuscript, where MLE was used for optimization.
In short, we first randomly drew $10000$ values from a normal distribution:
$\mathbf{X}_{\text{normal}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{N}\left(0, 1\right)$,
or equivalently $\mathbf{X}_{\text{normal}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{AGN}\left(0, 1/\sqrt{2}, 0.5, 2\right)$.
The second distribution was a right-skewed normal distribution 
$\mathbf{X}_{\text{right}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{AGN}\left(0, 1/\sqrt{2}, 0.2, 2\right)$.
The third distribution was a left-skewed normal distribution 
$\mathbf{X}_{\text{left}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{AGN}\left(0, 1/\sqrt{2}, 0.8, 2\right)$.

We then computed transformation parameter $\lambda$ using the original definitions
(equations \ref{eqn:box-cox-original} and \ref{eqn:yeo-johnson-original}) 
and the location- and scale-invariant definitions (equations \ref{eqn:box-cox-invariant} 
and \ref{eqn:yeo-johnson-invariant}) for each distribution using different optimisation criteria.
To assess location invariance, a positive value $d_{\text{shift}}$ was added to each distribution with $d_{\text{shift}} \in [1, 10^6]$.
Similarly, to assess scale invariance, each distribution was multiplied by a positive value $d_{\text{scale}}$, where $d_{\text{scale}} \in [1, 10^6]$.

The results are shown in Figure \ref{fig:shifted-distributions-appendix}.

```{r shifted-distributions-appendix, warning=FALSE, message=FALSE, fig.cap=cap}
cap <- paste0(
  "Invariant power transformation produces transformation parameters that are invariant to location and scale. ",
  "Samples were drawn from normal, right-skewed and left-skewed distributions, respectively, which then underwent a shift $d_{\\text{shift}}$ or multiplication by $d_{\\text{scale}}$. ",
  "Estimates of the transformation parameter $\\lambda$ for the conventional power transformations show strong dependency on the overall location and scale of the distribution and the optimisation criterion, ",
  "whereas estimates obtained for the location- and scale-invariant power transformations are constant. ",
  "For location- and scale-invariant power transformations, the Anderson-Darling criterion leads to unstable estimates of $\\lambda$ for skewed distributions, possibly due to large weights being assigned to samples at the upper and lower ends of the distribution." 
)

.plot_shifted_distributions_appendix(manuscript_dir = manuscript_dir, plot_theme = plot_theme)
```

# Appendix C: Central normality test and empirical central normality test critical values

Critical values for the empirical central normality test at $\kappa = 0.80$ are
found in Table \ref{tab:empirical-central-normality-critical-statistic}.


```{r include = FALSE, eval = FALSE}
# This generates the table shown below.
.get_test_statistic_lookup_table_manuscript(
  manuscript_dir = manuscript_dir,
  kappa_lookup = 0.80
)

```

\begin{table}
\begin{center}
\caption{Critical values of test statistic $\tau_{\alpha, n, \kappa = 0.80}$ for
central normality at $\kappa = 0.80$, as a function of significance 
level $\alpha$ and number of instances $n$. The values in this table should be divided by $100$ before use.}
\label{tab:empirical-central-normality-critical-statistic}
\begin{tabular}{l | r r r r r r r r r}

\toprule
$n$ \textbackslash $\alpha$ & 0.001 & 0.01 & 0.025 & 0.05 & 0.1 & 0.2 & 0.5 & 0.8 & 0.9 \\

\midrule
    5 & 200.88 & 90.25 & 64.46 & 49.04 & 36.07 & 27.70 & 19.28 & 13.41 & 10.90 \\
   10 &  45.90 & 31.78 & 27.12 & 24.14 & 21.23 & 18.30 & 13.68 & 10.20 &  8.71 \\
   20 &  28.88 & 22.53 & 19.96 & 18.01 & 15.99 & 13.93 & 10.67 &  8.26 &  7.27 \\
   50 &  16.86 & 14.10 & 12.66 & 11.55 & 10.28 &  8.95 &  7.00 &  5.53 &  4.91 \\
  100 &  12.41 & 10.00 &  9.02 &  8.21 &  7.36 &  6.45 &  5.02 &  4.00 &  3.58 \\
  200 &   8.65 &  7.05 &  6.37 &  5.80 &  5.22 &  4.57 &  3.57 &  2.84 &  2.54 \\
  500 &   5.54 &  4.47 &  4.03 &  3.69 &  3.30 &  2.90 &  2.27 &  1.81 &  1.63 \\
 1000 &   3.88 &  3.17 &  2.86 &  2.60 &  2.34 &  2.05 &  1.61 &  1.28 &  1.15 \\
 2000 &   2.79 &  2.24 &  2.03 &  1.84 &  1.66 &  1.45 &  1.14 &  0.91 &  0.82 \\ 
 5000 &   1.75 &  1.41 &  1.27 &  1.17 &  1.05 &  0.92 &  0.72 &  0.58 &  0.52 \\
10000 &   1.21 &  1.00 &  0.90 &  0.82 &  0.74 &  0.65 &  0.51 &  0.41 &  0.36 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\FloatBarrier






# Appendix D: Assessing transformations using simulated data

## Ranking Box-Cox transformations

Box-Cox transformations were assessed in the same manner as Yeo-Johnson transformations.
The results are shown in Table \ref{tab:comparison_methods_simulations_box_cox}.

```{r include = FALSE, eval = FALSE}

# Box-Cox on clean and dirty data.
clean_data <- .assess_simulated_data(manuscript_dir = manuscript_dir, data_type = "clean", method = "box_cox")
dirty_data <- .assess_simulated_data(manuscript_dir = manuscript_dir, data_type = "dirty", method = "box_cox")
shifted_data <- .assess_simulated_data(manuscript_dir = manuscript_dir, data_type = "dirty_shifted", method = "box_cox")

data <- rbind(clean_data, dirty_data, shifted_data, fill = TRUE)
data$dataset <- factor(data$dataset, levels = c("clean", "dirty", "dirty_shifted"))

# Add rank based on residual error for central normality.
data[outlier == FALSE, "method_rank" := rank(residuals), by = c("experiment_id", "outlier", "dataset")]
data[outlier == TRUE, "method_rank" := rank(residuals_central), by = c("experiment_id", "outlier", "dataset")]

table_data <- data[
  ,
  list(
    "avg." = round(mean(method_rank), digits = 2L),
    "10%" = quantile(method_rank, 0.10),
    "50%" = quantile(method_rank, 0.50),
    "90%" = quantile(method_rank, 0.90)
  ),
  by = c("transformation", "outlier", "dataset")
]

data.table::dcast(transformation ~ dataset + outlier, data = table_data, value.var = "avg.")

```
\begin{table}
\begin{center}
\caption{
Comparison of average rank between Box-Cox transformation methods based on either residual error (without outliers) or residual error of the central portion
(with outliers; $\kappa = 0.80$) over 3 datasets with 100000 sequences each. The clean dataset consists of sequences derived through inverse Box-Cox transformation
of data sampled from a standard normal distribution. The dirty dataset contains sequences sampled from asymmetric generalised normal distributions.
The shifted dataset also contains sequences sampled from asymmetric generalised normal distributions, but centred at 100, and scaled by 0.001.
If necessary, each sequence was shifted so that every element had a strictly positive value. 
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling.
A rank of 1 is the best and a rank of 9 the worst. For each dataset, the best ranking transformation is marked in bold.
}
\label{tab:comparison_methods_simulations_box_cox}
\begin{tabular}{l | l r r r r r r}

\toprule
& dataset: & \multicolumn{2}{c}{clean} & \multicolumn{2}{c}{dirty} & \multicolumn{2}{c}{shifted} \\
transformation & outliers: & no & yes & no & yes & no & yes \\

\midrule

none                                  & &         7.63  &         6.95  &         7.29  &         6.59  &         7.47  &         6.78 \\
conventional                          & &         3.11  &         5.98  &         5.41  &         6.18  &         6.51  &         6.10 \\
conventional (z-score norm.)          & &         4.15  &         6.27  &         4.83  &         6.32  &         4.34  &         5.84 \\
conventional (robust scaling)         & &         4.20  &         6.29  &         4.84  &         6.31  &         4.35  &         5.84 \\
Raymaekers-Rousseeuw                  & &         4.50  &         3.56  &         5.39  &         3.69  &         6.30  &         5.80 \\ 
Raymaekers-Rousseeuw (z-score norm.)  & &         6.23  &         3.80  &         4.93  &         3.85  &         4.48  &         3.50 \\ 
Raymaekers-Rousseeuw (robust scaling) & &         6.12  &         3.80  &         4.99  &         3.87  &         4.53  &         3.51 \\
invariant                             & & \textbf{2.96} &         5.59  & \textbf{3.62} &         6.01  & \textbf{3.51} &         5.58 \\
robust invariant                      & &         6.10  & \textbf{2.76} &         3.68  & \textbf{2.19} & \textbf{3.51} & \textbf{2.00} \\

\bottomrule
\end{tabular}
\end{center}
\end{table}


\FloatBarrier

## Ranking Yeo-Johnson transformations: residual errors

In the main manuscript, transformation methods are compared by ranking within
each experiment, defined by the combination of sequence and absence/presence of
outliers. This allows for direct comparison of methods between different
experiments.

Table \ref{tab:comparison_methods_simulations_yeo_johnson_avg_residual} shows
the average normalised residual error for these transformation methods. This
metric is computed from the residual error and the residual error of the central
portion ($\kappa = 0.70$) for sequences without and with outliers, respectively.
In each experiment, the respective residual error was normalised by dividing with
the sequence length, and can be interpreted as the average residual error of a
single sample. This reduces the effect of sequence length on the residual error.

Table \ref{tab:comparison_methods_simulations_yeo_johnson_avg_residual_diff} shows
the average normalised difference in residual error between the best method for each
experiment and the individual methods. As above, normalisation was done by
division by the length of the sequence corresponding to each experiment.

Both tables show that the differences between best-performing methods on each
dataset are relatively small.

```{r include = FALSE, eval = FALSE}

# Yeo-Johnson on clean and dirty data.
clean_data <- .assess_simulated_data(manuscript_dir = manuscript_dir, data_type = "clean")
dirty_data <- .assess_simulated_data(manuscript_dir = manuscript_dir, data_type = "dirty")
shifted_data <- .assess_simulated_data(manuscript_dir = manuscript_dir, data_type = "dirty_shifted")

data <- rbind(clean_data, dirty_data, shifted_data, fill = TRUE)
data$dataset <- factor(data$dataset, levels = c("clean", "dirty", "dirty_shifted"))

# Add rank based on residual error for central normality.
data[outlier == FALSE, "method_rank" := rank(residuals), by = c("experiment_id", "outlier", "dataset")]
data[outlier == TRUE, "method_rank" := rank(residuals_central), by = c("experiment_id", "outlier", "dataset")]
data[outlier == FALSE, "norm_residuals" := residuals / n_samples]
data[outlier == TRUE, "norm_residuals" := residuals_central / (0.8 * n_samples)]

data[outlier == FALSE, "min_residual" := min(residuals), by = c("experiment_id", "outlier", "dataset")]
data[outlier == TRUE, "min_residual" := min(residuals_central), by = c("experiment_id", "outlier", "dataset")]
data[outlier == FALSE, "residual_delta" := (residuals - min_residual) / n_samples]
data[outlier == TRUE, "residual_delta" := (residuals_central - min_residual) / (0.8 * n_samples)]

table_data <- data[
  ,
  list(
    "avg." = round(mean(method_rank), digits = 2L),
    "10%" = quantile(method_rank, 0.10),
    "50%" = quantile(method_rank, 0.50),
    "90%" = quantile(method_rank, 0.90),
    "avg. norm. res." = round(mean(norm_residuals), digits = 3),
    "avg. res. diff." = round(mean(residual_delta), digits = 3)
  ),
  by = c("transformation", "outlier", "dataset")
]

# Table with average rank --> this was already presented in the main manuscript.
data.table::dcast(transformation ~ dataset + outlier, data = table_data, value.var = "avg.")

# Table with average normalised residuals.
data.table::dcast(transformation ~ dataset + outlier, data = table_data, value.var = "avg. norm. res.")

# Table with difference with best-performing method in an experiment.
data.table::dcast(transformation ~ dataset + outlier, data = table_data, value.var = "avg. res. diff.")
```

\begin{table}
\begin{center}
\caption{
Comparison of average normalised residual error between Yeo-Johnson transformation methods based on either the residual error (without outliers) or the residual error of the central portion
(with outliers; $\kappa = 0.80$) over 3 datasets with 100000 sequences each. Residual errors were normalised by sequence length to reduce the effect of sequence length on the residual error.
The clean dataset consists of sequences derived through inverse Yeo-Johnson transformation
of data sampled from a standard normal distribution. The dirty dataset contains sequences sampled from asymmetric generalised normal distributions, centred at 0.
The shifted dataset also contains sequences sampled from asymmetric generalised normal distributions, but centred at 100, and scaled by 0.001.
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling.
A lower normalised residual error is better. Discrepancies with the rank-based method may occur, as even with normalisation residual errors 
are not fully comparable between experiments, whereas the ranks achieved within each experiment are.
}
\label{tab:comparison_methods_simulations_yeo_johnson_avg_residual}
\begin{tabular}{l | l r r r r r r}

\toprule
& dataset: & \multicolumn{2}{c}{clean} & \multicolumn{2}{c}{dirty} & \multicolumn{2}{c}{shifted} \\
transformation & outliers: & no & yes & no & yes & no & yes \\

\midrule

none                                  & & 0.202 & 0.112 & 0.128 & 0.080 & 0.129 & 0.080 \\
conventional                          & & 0.058 & 0.094 & 0.085 & 0.078 & 0.129 & 0.080 \\
conventional (z-score norm.)          & & 0.061 & 0.094 & 0.091 & 0.073 & 0.091 & 0.073 \\
conventional (robust scaling)         & & 0.058 & 0.094 & 0.090 & 0.073 & 0.090 & 0.073 \\
Raymaekers-Rousseeuw                  & & 0.071 & 0.055 & 0.092 & 0.279 & 0.129 & 0.080 \\
Raymaekers-Rousseeuw (z-score norm.)  & & 0.087 & 0.057 & 0.097 & 0.060 & 0.097 & 0.060 \\
Raymaekers-Rousseeuw (robust scaling) & & 0.071 & 0.056 & 0.095 & 0.059 & 0.095 & 0.059 \\
invariant                             & & 0.057 & 0.099 & 0.083 & 0.075 & 0.083 & 0.075 \\
robust invariant                      & & 0.069 & 0.066 & 0.084 & 0.058 & 0.084 & 0.058 \\

\bottomrule
\end{tabular}
\end{center}
\end{table}



\begin{table}
\begin{center}
\caption{
Comparison of average difference in residual error between Yeo-Johnson
transformation methods and the best transformation method within each
experiment, normalised by the corresponding sequence length. This is based on
either the residual error (without outliers) or the residual error of the
central portion (with outliers; $\kappa = 0.80$) over 3 datasets with 100000
sequences each. Residual errors were normalised by sequence length to reduce the
effect of sequence length on the residual error. The clean dataset consists of
sequences derived through inverse Yeo-Johnson transformation of data sampled
from a standard normal distribution. The dirty dataset contains sequences
sampled from asymmetric generalised normal distributions, centred at 0. The
shifted dataset also contains sequences sampled from asymmetric generalised
normal distributions, but centred at 100, and scaled by 0.001. Several
transformation methods include normalisation before transformation, indicated by
z-score normalisation (norm.) or robust scaling. A lower difference is better,
with a minimum value of 0.0. Discrepancies with the rank-based method may occur,
as normalised differences in residual error are not fully comparable between
experiments, whereas the ranks achieved within each experiment are.
}
\label{tab:comparison_methods_simulations_yeo_johnson_avg_residual_diff}
\begin{tabular}{l | l r r r r r r}

\toprule
& dataset: & \multicolumn{2}{c}{clean} & \multicolumn{2}{c}{dirty} & \multicolumn{2}{c}{shifted} \\
transformation & outliers: & no & yes & no & yes & no & yes \\

\midrule

none                                  & & 0.150 & 0.068 & 0.050 & 0.033 & 0.049 & 0.033 \\
conventional                          & & 0.005 & 0.049 & 0.006 & 0.032 & 0.049 & 0.033 \\
conventional (z-score norm.)          & & 0.008 & 0.049 & 0.013 & 0.026 & 0.011 & 0.025 \\
conventional (robust scaling)         & & 0.005 & 0.050 & 0.012 & 0.026 & 0.010 & 0.026 \\
Raymaekers-Rousseeuw                  & & 0.018 & 0.011 & 0.014 & 0.233 & 0.049 & 0.033 \\
Raymaekers-Rousseeuw (z-score norm.)  & & 0.034 & 0.012 & 0.018 & 0.014 & 0.017 & 0.013 \\
Raymaekers-Rousseeuw (robust scaling) & & 0.018 & 0.011 & 0.017 & 0.012 & 0.015 & 0.011 \\
invariant                             & & 0.004 & 0.054 & 0.005 & 0.029 & 0.003 & 0.028 \\
robust invariant                      & & 0.016 & 0.021 & 0.006 & 0.011 & 0.004 & 0.010 \\

\bottomrule
\end{tabular}
\end{center}
\end{table}

\FloatBarrier

## Ranking Yeo-Johnson transformations: effect of sequence length

The main manuscript shows the overall ranking of different Yeo-Johnson-based power transformation methods across three
sets of problems.
To assess the effect of limited sequence lengths, we perform ranking on subsets of the experiment.
The results for ranking the subset of sequences with 30 samples or fewer ($n = 16001$) 
is shown in Table \ref{tab:comparison_methods_simulations_yeo_johnson_10_30}.
In the clean dataset without outliers, where all sequences were sampled from a normal distribution and 
then underwent an inverse Yeo-Johnson transformation,
conventional and location- and scale-invariant are ranked similarly, with no clear advantage
to any method. For the remaining datasets, the robust location- and scale-invariant
method ranked best. It slightly outperformed (robustly scaled) conventional Yeo-Johnson transformations
and (z-score normalised) robust transformations [@Raymaekers2024-zf], in datasets without and with outliers, respectively.

In the subset for larger sequence lengths, with at least 1000 samples per sequence ($n = 33037$),
the cost of making the location- and scale-invariant transformation robust against outliers
becomes more apparent (Table \ref{tab:comparison_methods_simulations_yeo_johnson_1000_10000}).
In the clean dataset without outliers, down-weighting of the tails of the distribution by
the robust location- and scale-invariant method leads to larger residuals compared 
to other methods (Table \ref{tab:comparison_methods_simulations_yeo_johnson_1000_10000_avg_residual}.


```{r include = FALSE, eval = FALSE}

# Yeo-Johnson on clean and dirty data.
clean_data <- .assess_simulated_data(manuscript_dir = manuscript_dir, data_type = "clean")
dirty_data <- .assess_simulated_data(manuscript_dir = manuscript_dir, data_type = "dirty")
shifted_data <- .assess_simulated_data(manuscript_dir = manuscript_dir, data_type = "dirty_shifted")

data <- rbind(clean_data, dirty_data, shifted_data, fill = TRUE)
data$dataset <- factor(data$dataset, levels = c("clean", "dirty", "dirty_shifted"))

# 30 samples or fewer
data <- data[n_samples <= 30L, ]

# Add rank based on residual error for central normality.
data[outlier == FALSE, "method_rank" := rank(residuals), by = c("experiment_id", "outlier", "dataset")]
data[outlier == TRUE, "method_rank" := rank(residuals_central), by = c("experiment_id", "outlier", "dataset")]
data[outlier == FALSE, "norm_residuals" := residuals / n_samples]
data[outlier == TRUE, "norm_residuals" := residuals_central / (0.8 * n_samples)]

table_data <- data[
  ,
  list(
    "avg." = round(mean(method_rank), digits = 2L),
    "10%" = quantile(method_rank, 0.10),
    "50%" = quantile(method_rank, 0.50),
    "90%" = quantile(method_rank, 0.90),
    "avg. norm. res." = round(mean(norm_residuals), digits = 3)
  ),
  by = c("transformation", "outlier", "dataset")
]

# Table with average rank.
data.table::dcast(transformation ~ dataset + outlier, data = table_data, value.var = "avg.")

# Table with average normalised residuals.
data.table::dcast(transformation ~ dataset + outlier, data = table_data, value.var = "avg. norm. res.")

```


\begin{table}
\begin{center}
\caption{
Comparison of average rank between Yeo-Johnson transformation methods based on either residual error (without outliers) or residual error of the central portion
(with outliers; $\kappa = 0.80$) over 3 datasets with 16001 sequences each. Each sequence contained 30 samples or fewer. 
The clean dataset consists of sequences derived through inverse Yeo-Johnson transformation
of data sampled from a standard normal distribution. The dirty dataset contains sequences sampled from asymmetric generalised normal distributions, centred at 0.
The shifted dataset also contains sequences sampled from asymmetric generalised normal distributions, but centred at 100, and scaled by 0.001.
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling.
A rank of 1 is the best and a rank of 9 the worst. For each dataset, the best ranking transformation is marked in bold.
}
\label{tab:comparison_methods_simulations_yeo_johnson_10_30}
\begin{tabular}{l | l r r r r r r}

\toprule
& dataset: & \multicolumn{2}{c}{clean} & \multicolumn{2}{c}{dirty} & \multicolumn{2}{c}{shifted} \\
transformation & outliers: & no & yes & no & yes & no & yes \\

\midrule

none                                  & &         7.75  &         6.81  &         7.27  &         6.40  &         7.45  &         6.60 \\
conventional                          & &         4.19  &         5.47  &         4.48  &         5.78  &         6.55  &         5.94 \\
conventional (z-score norm.)          & &         4.36  &         5.12  &         5.01  &         4.99  &         4.35  &         4.70 \\
conventional (robust scaling)         & & \textbf{4.05} &         5.63  &         4.38  &         5.49  &         3.76  &         5.20 \\
Raymaekers-Rousseeuw                  & &         5.36  &         4.19  &         5.14  &         4.62  &         6.32  &         5.83 \\
Raymaekers-Rousseeuw (z-score norm.)  & &         5.56  &         4.08  &         5.67  &         4.05  &         5.03  &         3.81 \\
Raymaekers-Rousseeuw (robust scaling) & &         5.33  &         4.14  &         5.15  &         4.15  &         4.55  &         3.94 \\
invariant                             & &         4.11  &         5.92  &         4.12  &         5.67  &         3.62  &         5.37 \\
robust invariant                      & &         4.28  & \textbf{3.65} & \textbf{3.79} & \textbf{3.85} & \textbf{3.37} & \textbf{3.61} \\

\bottomrule
\end{tabular}
\end{center}
\end{table}



\begin{table}
\begin{center}
\caption{
Comparison of average normalised residual error between Yeo-Johnson transformation methods based on either the residual error (without outliers) or the residual error of the central portion
(with outliers; $\kappa = 0.80$) over 3 datasets with 16001 sequences each. Residual errors were normalised by sequence length to reduce the effect of sequence length on the residual error.
Each sequence contained 30 samples or fewer. The clean dataset consists of sequences derived through inverse Yeo-Johnson transformation
of data sampled from a standard normal distribution. The dirty dataset contains sequences sampled from asymmetric generalised normal distributions, centred at 0.
The shifted dataset also contains sequences sampled from asymmetric generalised normal distributions, but centred at 100, and scaled by 0.001.
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling.
A lower normalised residual error is better. Discrepancies with the rank-based method may occur, as even with normalisation residual errors 
are not fully comparable between experiments, whereas the ranks achieved within each experiment are.
}
\label{tab:comparison_methods_simulations_yeo_johnson_10_30_avg_residual}
\begin{tabular}{l | l r r r r r r}

\toprule
& dataset: & \multicolumn{2}{c}{clean} & \multicolumn{2}{c}{dirty} & \multicolumn{2}{c}{shifted} \\
transformation & outliers: & no & yes & no & yes & no & yes \\

\midrule

none                                  & & 0.246 & 0.173 & 0.186 & 0.156 & 0.187 & 0.157 \\
conventional                          & & 0.139 & 0.142 & 0.142 & 0.132 & 0.187 & 0.157 \\
conventional (z-score norm.)          & & 0.139 & 0.142 & 0.142 & 0.130 & 0.142 & 0.130 \\
conventional (robust scaling)         & & 0.139 & 0.142 & 0.141 & 0.130 & 0.141 & 0.130 \\
Raymaekers-Rousseeuw                  & & 0.211 & 0.151 & 0.183 & 1.515 & 0.187 & 0.157 \\
Raymaekers-Rousseeuw (z-score norm.)  & & 0.295 & 0.161 & 0.175 & 0.156 & 0.175 & 0.156 \\
Raymaekers-Rousseeuw (robust scaling) & & 0.209 & 0.155 & 0.172 & 0.144 & 0.172 & 0.144 \\
invariant                             & & 0.141 & 0.144 & 0.140 & 0.132 & 0.140 & 0.132 \\
robust invariant                      & & 0.141 & 0.125 & 0.140 & 0.119 & 0.140 & 0.119 \\

\bottomrule
\end{tabular}
\end{center}
\end{table}


```{r include = FALSE, eval = FALSE}

# Yeo-Johnson on clean and dirty data.
clean_data <- .assess_simulated_data(manuscript_dir = manuscript_dir, data_type = "clean")
dirty_data <- .assess_simulated_data(manuscript_dir = manuscript_dir, data_type = "dirty")
shifted_data <- .assess_simulated_data(manuscript_dir = manuscript_dir, data_type = "dirty_shifted")

data <- rbind(clean_data, dirty_data, shifted_data, fill = TRUE)
data$dataset <- factor(data$dataset, levels = c("clean", "dirty", "dirty_shifted"))

# 30 samples or fewer
data <- data[n_samples >= 1000L, ]

# Add rank based on residual error for central normality.
data[outlier == FALSE, "method_rank" := rank(residuals), by = c("experiment_id", "outlier", "dataset")]
data[outlier == TRUE, "method_rank" := rank(residuals_central), by = c("experiment_id", "outlier", "dataset")]
data[outlier == FALSE, "norm_residuals" := residuals / n_samples]
data[outlier == TRUE, "norm_residuals" := residuals_central / (0.8 * n_samples)]

table_data <- data[
  ,
  list(
    "avg." = round(mean(method_rank), digits = 2L),
    "10%" = quantile(method_rank, 0.10),
    "50%" = quantile(method_rank, 0.50),
    "90%" = quantile(method_rank, 0.90),
    "avg. norm. res." = round(mean(norm_residuals), digits = 3)
  ),
  by = c("transformation", "outlier", "dataset")
]

# Table with average rank.
data.table::dcast(transformation ~ dataset + outlier, data = table_data, value.var = "avg.")

# Table with average normalised residuals.
data.table::dcast(transformation ~ dataset + outlier, data = table_data, value.var = "avg. norm. res.")


```

\begin{table}
\begin{center}
\caption{
Comparison of average rank between Yeo-Johnson transformation methods based on either residual error (without outliers) or residual error of the central portion
(with outliers; $\kappa = 0.80$) over 3 datasets with 33037 sequences each. Each sequence contained between 1000 and 10000 samples.
The clean dataset consists of sequences derived through inverse Yeo-Johnson transformation
of data sampled from a standard normal distribution. The dirty dataset contains sequences sampled from asymmetric generalised normal distributions, centred at 0.
The shifted dataset also contains sequences sampled from asymmetric generalised normal distributions, but centred at 100, and scaled by 0.001.
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling.
A rank of 1 is the best and a rank of 9 the worst. For each dataset, the best ranking transformation is marked in bold.
}
\label{tab:comparison_methods_simulations_yeo_johnson_1000_10000}
\begin{tabular}{l | l r r r r r r}

\toprule
& dataset: & \multicolumn{2}{c}{clean} & \multicolumn{2}{c}{dirty} & \multicolumn{2}{c}{shifted} \\
transformation & outliers: & no & yes & no & yes & no & yes \\

\midrule

none                                  & &         8.85  &         8.14  &         7.35  &         6.29  &         7.41  &         6.87 \\
conventional                          & &         3.50  &         5.65  &         3.52  &         7.78  &         6.42  &         6.05 \\
conventional (z-score norm.)          & &         5.24  &         5.94  &         7.34  &         4.98  &         6.26  &         4.79 \\
conventional (robust scaling)         & &         3.53  &         6.98  &         5.97  &         5.79  &         4.87  &         5.65 \\
Raymaekers-Rousseeuw                  & &         4.15  &         2.81  &         3.37  &         3.72  &         6.19  &         5.79 \\
Raymaekers-Rousseeuw (z-score norm.)  & &         5.34  & \textbf{1.86} &         6.57  &         3.39  &         5.49  &         3.30 \\
Raymaekers-Rousseeuw (robust scaling) & &         4.20  &         2.33  &         5.21  &         3.36  &         4.12  &         3.23 \\
invariant                             & & \textbf{2.36} &         7.59  & \textbf{2.66} &         6.82  & \textbf{1.98} &         6.81 \\
robust invariant                      & &         7.83  &         3.70  &         3.01  & \textbf{2.88} &         2.26  & \textbf{2.51} \\

\bottomrule
\end{tabular}
\end{center}
\end{table}


\begin{table}
\begin{center}
\caption{
Comparison of average normalised residual error between Yeo-Johnson transformation methods based on either the residual error (without outliers) or the residual error of the central portion
(with outliers; $\kappa = 0.80$) over 3 datasets with 16001 sequences each. Residual errors were normalised by sequence length to reduce the effect of sequence length on the residual error.
Each sequence contained between 1000 and 10000 samples. The clean dataset consists of sequences derived through inverse Yeo-Johnson transformation
of data sampled from a standard normal distribution. The dirty dataset contains sequences sampled from asymmetric generalised normal distributions, centred at 0.
The shifted dataset also contains sequences sampled from asymmetric generalised normal distributions, but centred at 100, and scaled by 0.001.
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling.
A lower normalised residual error is better. Discrepancies with the rank-based method may occur, as even with normalisation residual errors 
are not fully comparable between experiments, whereas the ranks achieved within each experiment are.
}
\label{tab:comparison_methods_simulations_yeo_johnson_1000_10000_avg_residual}
\begin{tabular}{l | l r r r r r r}

\toprule
& dataset: & \multicolumn{2}{c}{clean} & \multicolumn{2}{c}{dirty} & \multicolumn{2}{c}{shifted} \\
transformation & outliers: & no & yes & no & yes & no & yes \\

\midrule

none                                  & & 0.186 & 0.091 & 0.106 & 0.053 & 0.106 & 0.053 \\
conventional                          & & 0.014 & 0.076 & 0.060 & 0.057 & 0.106 & 0.053 \\
conventional (z-score norm.)          & & 0.021 & 0.076 & 0.070 & 0.050 & 0.070 & 0.050 \\
conventional (robust scaling)         & & 0.014 & 0.076 & 0.069 & 0.050 & 0.069 & 0.050 \\
Raymaekers-Rousseeuw                  & & 0.015 & 0.021 & 0.060 & 0.030 & 0.106 & 0.053 \\
Raymaekers-Rousseeuw (z-score norm.)  & & 0.021 & 0.020 & 0.070 & 0.030 & 0.070 & 0.030 \\
Raymaekers-Rousseeuw (robust scaling) & & 0.015 & 0.020 & 0.068 & 0.030 & 0.068 & 0.030 \\
invariant                             & & 0.013 & 0.084 & 0.059 & 0.055 & 0.059 & 0.055 \\
robust invariant                      & & 0.034 & 0.042 & 0.061 & 0.032 & 0.061 & 0.032 \\

\bottomrule
\end{tabular}
\end{center}
\end{table}

\FloatBarrier

## Examples using clean data

Robust transformations are hypothesised to have a cost in efficiency for data
without outliers, i.e. clean data. Here we draw nine sequences with elements randomly drawn from a standard normal
distribution $\mathcal{N}(0,1)$. Subsequently, we perform an inverse transformation
$\left(\phi^{\lambda, 0, 1}\right)^{-1}$, with $\lambda \in \left(0, 2\right)$.

The sequences drawn resulted from the permutations of the number of elements of each
sequence ($n \in \{30, 100, 500\}$) and transformation parameter for the
inverse transformation $\lambda \in \{0.1, 1.0, 1.9\}$. Each sequence then underwent
power transformation using Box-Cox and Yeo-Johnson transformations. For Box-Cox
transformations, each sequence was shifted prior to inverse transformation
ensuring all elements were strictly positive after inverse transformation.

Results for Box-Cox transformations are shown in Table
\ref{tab:clean-transformation-appendix-residuals-bc},
\ref{tab:clean-transformation-appendix-lambda-bc},
and \ref{tab:clean-transformation-appendix-p-value-bc}.
Additionally, results for Yeo-Johnson transformations are shown in Table
\ref{tab:clean-transformation-appendix-residuals-yj},
\ref{tab:clean-transformation-appendix-lambda-yj},
and \ref{tab:clean-transformation-appendix-p-value-yj}.
As may be observed, in these examples robust location- and shift-invariant 
transformations have higher residual errors because of low weights being
assigned to tails of a distribution. This is also shown in 
Tables \ref{tab:comparison_methods_simulations_yeo_johnson}
and \ref{tab:comparison_methods_simulations_box_cox} for clean data without outliers.

```{r include = FALSE, eval = FALSE}

data <- .assess_standardisation_before_transformation_simulation(lambda_limit = c(-4.0, 6.0))
data <- data[, mget(c("residuals", "residuals_central", "lambda", "p_value", "transformation", "dataset"))]
data[, "residuals" := round(residuals, digits = 1L)]
data[, "residuals_central" := round(residuals_central, digits = 1L)]
data[, "lambda" := round(lambda, digits = 1L)]
data[, "p_value" := round(p_value, digits = 3L)]
data.table::dcast(data, transformation ~ dataset, value.var = "residuals")
data.table::dcast(data, transformation ~ dataset, value.var = "residuals_central")
data.table::dcast(data, transformation ~ dataset, value.var = "lambda")
data.table::dcast(data, transformation ~ dataset, value.var = "p_value")
```


\begin{table}
\begin{center}
\caption{Residual errors for features from simulated clean data without outliers after Yeo-Johnson transformation to normality.
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling.}
\label{tab:clean-transformation-appendix-residuals-yj}
\small{
\begin{tabular}{l | l r r r r r r r r r}

\toprule
& n: & \multicolumn{3}{c}{30} & \multicolumn{3}{c}{100} & \multicolumn{3}{c}{500} \\
transformation & $\lambda$: & 0.1 & 1.0 & 1.9 & 0.1 & 1.0 & 1.9 & 0.1 & 1.0 & 1.9 \\

\midrule

none                                  & & 4.0 & 3.4 & 4.6 & 40.1 &  8.9 & 20.8 & 184.6 & 10.1 & 191.8 \\
conventional                          & & 2.7 & 3.3 & 3.6 &  6.5 &  8.1 &  5.6 &  14.2 & 10.1 &  16.1 \\
conventional (z-score norm.)          & & 2.5 & 3.3 & 3.7 &  9.8 &  8.1 &  5.6 &  24.1 & 10.1 &  15.9 \\
conventional (robust scaling)         & & 2.6 & 3.3 & 3.7 &  7.4 &  8.2 &  5.6 &  15.5 & 10.1 &  14.0 \\
Raymaekers-Rousseeuw                  & & 3.8 & 3.3 & 3.6 &  6.5 &  8.5 &  5.6 &  14.4 & 11.0 &  17.6 \\
Raymaekers-Rousseeuw (z-score norm.)  & & 2.6 & 3.3 & 3.7 &  9.8 &  8.5 &  5.6 &  24.1 & 11.1 &  15.9 \\
Raymaekers-Rousseeuw (robust scaling) & & 2.6 & 3.3 & 3.7 &  7.4 &  8.6 &  5.6 &  15.4 & 11.1 &  15.1 \\
invariant                             & & 3.0 & 3.5 & 3.6 &  5.0 &  8.1 &  5.6 &  13.1 & 10.9 &  17.8 \\
robust invariant                      & & 3.0 & 3.4 & 3.6 &  5.5 & 10.1 &  6.2 &  18.7 & 13.2 &  32.8 \\

\bottomrule
\end{tabular}
}
\end{center}
\end{table}



\begin{table}
\begin{center}
\caption{Transformation parameter $\lambda$ for features from simulated clean data without outliers after Yeo-Johnson transformation to normality.
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling.}
\label{tab:clean-transformation-appendix-lambda-yj}
\small{
\begin{tabular}{l | l r r r r r r r r r}

\toprule
& n: & \multicolumn{3}{c}{30} & \multicolumn{3}{c}{100} & \multicolumn{3}{c}{500} \\
transformation & $\lambda$: & 0.1 & 1.0 & 1.9 & 0.1 & 1.0 & 1.9 & 0.1 & 1.0 & 1.9 \\

\midrule

conventional                          & & 0.6 & 1.2 & 1.5 &  0.0 & 0.9 & 1.6 &  0.1 & 1.0 & 1.9 \\
conventional (z-score norm.)          & & 0.5 & 1.2 & 1.5 & -0.2 & 0.9 & 1.7 & -0.2 & 1.0 & 2.2 \\
conventional (robust scaling)         & & 0.5 & 1.3 & 1.6 & -0.3 & 0.9 & 1.7 & -0.2 & 1.0 & 2.1 \\
Raymaekers-Rousseeuw                  & & 1.0 & 1.2 & 1.5 &  0.0 & 0.7 & 1.6 &  0.1 & 1.0 & 1.9 \\
Raymaekers-Rousseeuw (z-score norm.)  & & 0.7 & 1.2 & 1.5 & -0.2 & 0.7 & 1.7 & -0.2 & 1.0 & 2.2 \\
Raymaekers-Rousseeuw (robust scaling) & & 0.6 & 1.3 & 1.6 & -0.3 & 0.6 & 1.7 & -0.2 & 1.0 & 2.1 \\
invariant                             & & 0.4 & 1.4 & 1.7 &  0.1 & 0.9 & 1.8 &  0.1 & 1.1 & 3.0 \\
robust invariant                      & & 0.5 & 1.1 & 1.5 & -0.1 & 0.7 & 1.7 &  0.1 & 1.1 & 1.9 \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table}



\begin{table}
\begin{center}
\caption{
p-values of empirical central normality tests for features from simulated clean data without outliers after Yeo-Johnson transformation to normality.
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling.}
\label{tab:clean-transformation-appendix-p-value-yj}
\small{
\begin{tabular}{p{3.5cm} | l r r r r r r r r r}

\toprule
& n: & \multicolumn{3}{c}{30} & \multicolumn{3}{c}{100} & \multicolumn{3}{c}{500} \\
transformation & $\lambda$: & 0.1 & 1.0 & 1.9 & 0.1 & 1.0 & 1.9 & 0.1 & 1.0 & 1.9 \\

\midrule

none                                  & & 0.981 & 0.816 & 0.576 & $<0.001$ & 0.099 & 0.011 & $<0.001$ & 0.993 & $<0.001$ \\
conventional                          & & 0.979 & 0.614 & 0.617 &   0.722  & 0.221 & 0.782 &   0.946  & 0.997 &   0.637 \\
conventional (z-score norm.)          & & 0.970 & 0.594 & 0.645 &   0.251  & 0.224 & 0.895 &   0.529  & 0.998 &   0.828 \\
conventional (robust scaling)         & & 0.974 & 0.593 & 0.640 &   0.538  & 0.207 & 0.837 &   0.933  & 0.998 &   0.810 \\
Raymaekers-Rousseeuw                  & & 0.983 & 0.614 & 0.617 &   0.722  & 0.622 & 0.782 &   0.927  & 0.985 &   0.831 \\
Raymaekers-Rousseeuw (z-score norm.)  & & 0.985 & 0.594 & 0.645 &   0.251  & 0.623 & 0.895 &   0.564  & 0.984 &   0.807 \\
Raymaekers-Rousseeuw (robust scaling) & & 0.987 & 0.593 & 0.640 &   0.538  & 0.599 & 0.837 &   0.935  & 0.984 &   0.875 \\
invariant                             & & 0.987 & 0.487 & 0.573 &   0.778  & 0.241 & 0.683 &   0.963  & 0.997 &   0.865 \\
robust invariant                      & & 0.992 & 0.753 & 0.667 &   0.909  & 0.739 & 0.628 &   0.968  & 0.994 &   0.680 \\

\bottomrule
\end{tabular}
}
\end{center}
\end{table}




```{r include = FALSE, eval = FALSE}

data <- .assess_standardisation_before_transformation_simulation(lambda_limit = c(-4.0, 6.0), method = "box_cox")
data <- data[, mget(c("residuals", "residuals_central", "lambda", "p_value", "transformation", "dataset"))]
data[, "residuals" := round(residuals, digits = 1L)]
data[, "residuals_central" := round(residuals, digits = 1L)]
data[, "lambda" := round(lambda, digits = 1L)]
data[, "p_value" := round(p_value, digits = 3L)]
data.table::dcast(data, transformation ~ dataset, value.var = "residuals")
data.table::dcast(data, transformation ~ dataset, value.var = "residuals_central")
data.table::dcast(data, transformation ~ dataset, value.var = "lambda")
data.table::dcast(data, transformation ~ dataset, value.var = "p_value")
```

\begin{table}
\begin{center}
\caption{Residual errors for features from simulated clean data without outliers after Box-Cox transformation to normality.
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling.}
\label{tab:clean-transformation-appendix-residuals-bc}
\small{
\begin{tabular}{l | l r r r r r r r r r}

\toprule
& n: & \multicolumn{3}{c}{30} & \multicolumn{3}{c}{100} & \multicolumn{3}{c}{500} \\
transformation & $\lambda$: & 0.1 & 1.0 & 1.9 & 0.1 & 1.0 & 1.9 & 0.1 & 1.0 & 1.9 \\

\midrule

none                                  & & 5.3 & 3.4 & 3.8 & 45.7 &  8.9 & 6.0 & 221.8 & 10.1 & 29.7 \\
conventional                          & & 2.4 & 3.2 & 3.5 &  5.9 &  8.4 & 5.5 &  14.3 &  9.8 & 16.1 \\
conventional (z-score norm.)          & & 2.4 & 3.2 & 3.7 &  5.4 &  8.7 & 5.5 &  14.0 & 10.4 & 16.1 \\
conventional (robust scaling)         & & 2.4 & 3.2 & 3.7 &  5.4 &  8.7 & 5.5 &  14.0 & 10.4 & 16.1 \\
Raymaekers-Rousseeuw                  & & 2.4 & 3.2 & 3.5 &  5.9 & 13.3 & 5.7 &  16.1 & 13.6 & 16.9 \\
Raymaekers-Rousseeuw (z-score norm.)  & & 2.4 & 3.2 & 4.9 &  6.3 &  8.9 & 6.2 &  19.1 & 19.0 & 19.9 \\
Raymaekers-Rousseeuw (robust scaling) & & 2.4 & 3.2 & 4.9 &  6.3 &  8.9 & 6.2 &  18.4 & 19.0 & 19.9 \\
invariant                             & & 2.4 & 3.2 & 3.7 &  5.4 &  8.7 & 5.5 &  13.7 &  9.9 & 16.0 \\
robust invariant                      & & 2.4 & 4.3 & 4.7 &  8.9 & 20.4 & 6.7 &  14.1 & 15.8 & 37.6 \\

\bottomrule
\end{tabular}
}
\end{center}
\end{table}



\begin{table}
\begin{center}
\caption{Transformation parameter $\lambda$ for features from simulated clean data without outliers after Box-Cox transformation to normality.
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling.}
\label{tab:clean-transformation-appendix-lambda-bc}
\small{
\begin{tabular}{l | l r r r r r r r r r}

\toprule
& n: & \multicolumn{3}{c}{30} & \multicolumn{3}{c}{100} & \multicolumn{3}{c}{500} \\
transformation & $\lambda$: & 0.1 & 1.0 & 1.9 & 0.1 & 1.0 & 1.9 & 0.1 & 1.0 & 1.9 \\

\midrule

conventional                          & & 0.4 & 1.3 & 0.5 & 0.0 &  0.9 & 0.7 & 0.1 & 1.1 & 2.0 \\
conventional (z-score norm.)          & & 0.4 & 1.1 & 0.8 & 0.2 &  0.9 & 0.8 & 0.1 & 1.0 & 1.4 \\
conventional (robust scaling)         & & 0.4 & 1.1 & 0.8 & 0.2 &  0.9 & 0.8 & 0.1 & 1.0 & 1.4 \\
Raymaekers-Rousseeuw                  & & 0.4 & 1.3 & 0.5 & 0.0 & -0.1 & 0.5 & 0.1 & 0.9 & 1.8 \\
Raymaekers-Rousseeuw (z-score norm.)  & & 0.4 & 1.1 & 0.4 & 0.1 &  0.8 & 0.7 & 0.1 & 0.9 & 1.2 \\
Raymaekers-Rousseeuw (robust scaling) & & 0.4 & 1.1 & 0.4 & 0.1 &  0.8 & 0.7 & 0.1 & 0.9 & 1.2 \\
invariant                             & & 0.4 & 1.6 & 0.8 & 0.2 &  0.9 & 0.8 & 0.1 & 1.1 & 1.9 \\
robust invariant                      & & 0.5 & 0.8 & 0.4 & 0.0 &  0.2 & 0.7 & 0.1 & 0.9 & 0.9 \\

\bottomrule
\end{tabular}
}
\end{center}
\end{table}



\begin{table}
\begin{center}
\caption{
p-values of empirical central normality tests for features from simulated clean data without outliers after Box-Cox transformation to normality.
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling.}
\label{tab:clean-transformation-appendix-p-value-bc}
\small{
\begin{tabular}{p{3.5cm} | l r r r r r r r r r}

\toprule
& n: & \multicolumn{3}{c}{30} & \multicolumn{3}{c}{100} & \multicolumn{3}{c}{500} \\
transformation & $\lambda$: & 0.1 & 1.0 & 1.9 & 0.1 & 1.0 & 1.9 & 0.1 & 1.0 & 1.9 \\

\midrule

none                                  & & 0.940 & 0.816 & 0.498 & $<0.001$ & 0.098 & 0.714 & $<0.001$ & 0.993 & 0.928 \\
conventional                          & & 0.948 & 0.695 & 0.603 &   0.798  & 0.163 & 0.791 &   0.931  & 0.997 & 0.658 \\
conventional (z-score norm.)          & & 0.947 & 0.777 & 0.624 &   0.611  & 0.182 & 0.788 &   0.969  & 0.991 & 0.822 \\
conventional (robust scaling)         & & 0.947 & 0.777 & 0.624 &   0.611  & 0.182 & 0.788 &   0.969  & 0.991 & 0.822 \\
Raymaekers-Rousseeuw                  & & 0.948 & 0.695 & 0.603 &   0.798  & 0.768 & 0.806 &   0.816  & 0.962 & 0.782 \\
Raymaekers-Rousseeuw (z-score norm.)  & & 0.947 & 0.777 & 0.753 &   0.913  & 0.255 & 0.784 &   0.775  & 0.866 & 0.921 \\
Raymaekers-Rousseeuw (robust scaling) & & 0.947 & 0.777 & 0.753 &   0.913  & 0.255 & 0.784 &   0.807  & 0.866 & 0.921 \\
invariant                             & & 0.947 & 0.661 & 0.624 &   0.611  & 0.182 & 0.789 &   0.956  & 0.997 & 0.664 \\
robust invariant                      & & 0.969 & 0.824 & 0.750 &   0.972  & 0.779 & 0.775 &   0.956  & 0.934 & 0.840 \\

\bottomrule
\end{tabular}
}
\end{center}
\end{table}

\FloatBarrier


# Appendix E: Experimental results

The effect of using location- and scale-invariant transformations was investigated
using real-world datasets, as described in the main manuscript.

## Yeo-Johnson transformation

Additional results for Yeo-Johnson transformations are shown in Tables
\ref{tab:experimental-results-appendix-residuals}, 
\ref{tab:experimental-results-appendix-lambda},
\ref{tab:experimental-results-appendix-p-value}.


```{r include = FALSE, eval = FALSE}

data <- .assess_standardisation_before_transformation()
data <- data[, mget(c("residuals", "residuals_central", "lambda", "p_value", "transformation", "dataset"))]
data[, "residuals" := round(residuals, digits = 1L)]
data[, "residuals_central" := round(residuals_central, digits = 1L)]
data[, "lambda" := round(lambda, digits = 1L)]
data[, "p_value" := round(p_value, digits = 3L)]

data.table::dcast(data, transformation ~ dataset, value.var = "residuals")
data.table::dcast(data, transformation ~ dataset, value.var = "residuals_central")
data.table::dcast(data, transformation ~ dataset, value.var = "lambda")
data.table::dcast(data, transformation ~ dataset, value.var = "p_value")
```

\begin{table}
\begin{center}
\caption{Residual errors for features from real-world datasets after Yeo-Johnson transformation to normality. 
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling. 
AWT: arterial wall thickness; FE: fuel efficiency; PBM: penguin body mass}
\label{tab:experimental-results-appendix-residuals}
\begin{tabular}{l | r r r r r}

\toprule
feature & age & AWT & FE & latitude & PBM \\

\midrule
none                                  & 16.5 & 110.1 & 54.5 & 328.4 & 48.0 \\
conventional                          & 11.5 &  30.0 & 55.3 & 319.0 & 32.2 \\
conventional (z-score norm.)          & 11.5 &  19.3 & 49.0 & 326.2 & 33.3 \\
conventional (robust scaling)         & 11.3 &  33.3 & 55.9 & 326.4 & 31.5 \\
Raymaekers-Rousseeuw                  & 11.5 & 136.7 & 47.7 & 315.1 & 32.2 \\
Raymaekers-Rousseeuw (z-score norm.)  & 13.2 & 214.5 & 57.4 & 326.2 & 33.3 \\
Raymaekers-Rousseeuw (robust scaling) & 13.4 & 203.0 & 56.9 & 326.4 & 31.5 \\
invariant                             &  8.8 &  12.2 & 44.0 & 326.4 & 26.8 \\
robust invariant                      &  9.3 &  30.2 & 55.8 & 308.1 & 22.0 \\

\bottomrule
\end{tabular}
\end{center}
\end{table}



\begin{table}
\begin{center}
\caption{Transformation parameter $\lambda$ for features from real-world datasets after Yeo-Johnson transformation to normality. 
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling. 
AWT: arterial wall thickness; FE: fuel efficiency; PBM: penguin body mass}
\label{tab:experimental-results-appendix-lambda}
\begin{tabular}{l | r r r r r}

\toprule
feature & age & AWT & FE & latitude & PBM \\

\midrule
conventional                          & 2.0 & -0.7 & -0.1 & 62.1 & -0.5 \\
conventional (z-score norm.)          & 1.2 & -1.7 & -0.1 &  1.3 &  0.6 \\
conventional (robust scaling)         & 1.2 &  0.0 &  0.3 &  1.3 &  0.6 \\
Raymaekers-Rousseeuw                  & 2.0 &  1.1 &  0.8 & 95.4 & -0.5 \\
Raymaekers-Rousseeuw (z-score norm.)  & 1.3 &  1.4 &  1.0 &  1.3 &  0.6 \\
Raymaekers-Rousseeuw (robust scaling) & 1.3 &  1.2 &  1.0 &  1.3 &  0.6 \\
invariant                             & 1.3 &  0.2 & -1.3 &  1.5 &  0.5 \\
robust invariant                      & 1.3 & -0.3 &  1.0 &  1.1 &  0.3 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}



\begin{table}
\begin{center}
\caption{
p-values of empirical central normality tests for features from real-world datasets after Yeo-Johnson transformation to normality.
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling. 
AWT: arterial wall thickness; FE: fuel efficiency; PBM: penguin body mass}
\label{tab:experimental-results-appendix-p-value}
\begin{tabular}{l | r r r r r}

\toprule
feature & age & AWT & FE & latitude & PBM \\

\midrule

none                                  & 0.694 & 0.832     & 0.727     & $< 0.001$ & $< 0.001$ \\
conventional                          & 0.961 & 0.003     & $< 0.001$ & $< 0.001$ & 0.101 \\
conventional (z-score norm.)          & 0.919 & 0.021     & $< 0.001$ & $< 0.001$ & 0.099 \\
conventional (robust scaling)         & 0.933 & $< 0.001$ & $< 0.001$ & $< 0.001$ & 0.129 \\
Raymaekers-Rousseeuw                  & 0.961 & 0.881     & 0.373     & $< 0.001$ & 0.101 \\
Raymaekers-Rousseeuw (z-score norm.)  & 0.694 & 0.899     & 0.809     & $< 0.001$ & 0.099 \\
Raymaekers-Rousseeuw (robust scaling) & 0.900 & 0.880     & 0.811     & $< 0.001$ & 0.144 \\
invariant                             & 0.976 & 0.147     & $< 0.001$ & $< 0.001$ & 0.276 \\
robust invariant                      & 0.933 & 0.178     & 0.759     & $< 0.001$ & 0.688 \\

\bottomrule
\end{tabular}
\end{center}
\end{table}

\FloatBarrier



## Box-Cox transformation

Results for Box-Cox transformations are shown in Tables \ref{tab:experimental-results-appendix-residuals-bc},
\ref{tab:experimental-results-appendix-lambda-bc} and \ref{tab:experimental-results-appendix-p-value-bc}.


```{r include = FALSE, eval = FALSE}

data <- .assess_standardisation_before_transformation(method = "box_cox")
data <- data[, mget(c("residuals", "residuals_central", "lambda", "p_value", "transformation", "dataset"))]
data[, "residuals" := round(residuals, digits = 1L)]
data[, "residuals_central" := round(residuals_central, digits = 1L)]
data[, "lambda" := round(lambda, digits = 1L)]
data[, "p_value" := round(p_value, digits = 3L)]

data.table::dcast(data, transformation ~ dataset, value.var = "residuals")
data.table::dcast(data, transformation ~ dataset, value.var = "residuals_central")
data.table::dcast(data, transformation ~ dataset, value.var = "lambda")
data.table::dcast(data, transformation ~ dataset, value.var = "p_value")
```

\begin{table}
\begin{center}
\caption{Residual errors for features from real-world datasets after Box-Cox transformation to normality. 
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling. 
AWT: arterial wall thickness; FE: fuel efficiency; PBM: penguin body mass}
\label{tab:experimental-results-appendix-residuals-bc}
\begin{tabular}{l | r r r r r}

\toprule
feature & age & AWT & FE & latitude & PBM \\

\midrule
none                                  & 16.5 & 110.1 &  54.5 & 328.4 & 48.0 \\
conventional                          & 11.5 &  33.5 &  55.7 & 318.9 & 32.2 \\
conventional (z-score norm.)          & 12.9 &  44.5 &  58.8 & 305.3 & 27.3 \\
conventional (robust scaling)         & 12.9 &  44.7 &  59.0 & 305.3 & 27.3 \\
Raymaekers-Rousseeuw                  & 11.5 & 127.1 &  47.6 & 314.8 & 32.2 \\ 
Raymaekers-Rousseeuw (z-score norm.)  & 13.1 &  54.8 & 127.5 & 510.2 & 23.6 \\
Raymaekers-Rousseeuw (robust scaling) & 13.1 & 100.2 &  44.9 & 423.2 & 23.6 \\
invariant                             & 11.6 &  28.0 &  48.4 & 311.8 & 27.3 \\
robust invariant                      & 12.8 & 150.1 &  60.5 & 646.0 & 27.4 \\

\bottomrule
\end{tabular}
\end{center}
\end{table}



\begin{table}
\begin{center}
\caption{Transformation parameter $\lambda$ for features from real-world datasets after Box-Cox transformation to normality. 
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling. 
AWT: arterial wall thickness; FE: fuel efficiency; PBM: penguin body mass}
\label{tab:experimental-results-appendix-lambda-bc}
\begin{tabular}{l | r r r r r}

\toprule
feature & age & AWT & FE & latitude & PBM \\

\midrule

conventional                          & 1.9 & -0.5 & -0.1 & 62.1 & -0.5 \\
conventional (z-score norm.)          & 1.2 &  0.1 &  0.2 &  1.2 &  0.5 \\
conventional (robust scaling)         & 1.2 &  0.1 &  0.1 &  1.2 &  0.5 \\
Raymaekers-Rousseeuw                  & 1.9 &  1.1 &  0.8 & 95.9 & -0.5 \\
Raymaekers-Rousseeuw (z-score norm.)  & 1.2 &  0.6 & -0.5 &  0.6 &  0.3 \\
Raymaekers-Rousseeuw (robust scaling) & 1.2 &  1.0 &  0.8 &  0.7 &  0.3 \\
invariant                             & 1.7 & -1.0 & -0.7 &  1.9 &  0.5 \\
robust invariant                      & 1.3 &  1.2 &  1.1 &  0.4 &  0.1 \\

\bottomrule
\end{tabular}
\end{center}
\end{table}



\begin{table}
\begin{center}
\caption{
p-values of empirical central normality tests for features from real-world datasets after Box-Cox transformation to normality.
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling. 
AWT: arterial wall thickness; FE: fuel efficiency; PBM: penguin body mass}
\label{tab:experimental-results-appendix-p-value-bc}
\begin{tabular}{l | r r r r r}

\toprule
feature & age & AWT & FE & latitude & PBM \\

\midrule

none                                  & 0.694 & 0.832 & 0.727     & $< 0.001$ & $< 0.001$ \\
conventional                          & 0.961 & 0.002 & $< 0.001$ & $< 0.001$ & 0.101 \\
conventional (z-score norm.)          & 0.901 & 0.005 & $< 0.001$ & $< 0.001$ & 0.154 \\
conventional (robust scaling)         & 0.901 & 0.005 & $< 0.001$ & $< 0.001$ & 0.154 \\
Raymaekers-Rousseeuw                  & 0.961 & 0.871 & 0.365     & $< 0.001$ & 0.101 \\
Raymaekers-Rousseeuw (z-score norm.)  & 0.891 & 0.268 & $< 0.001$ & $< 0.001$ &	0.495 \\
Raymaekers-Rousseeuw (robust scaling) & 0.562 & 0.789 & $< 0.001$ & $< 0.001$ &	0.492 \\
invariant                             & 0.962 & 0.004 & $< 0.001$ & $< 0.001$ &	0.154 \\
robust invariant                      & 0.899 & 0.890 & 0.866     & $< 0.001$ & 0.622 \\

\bottomrule
\end{tabular}
\end{center}
\end{table}

\FloatBarrier

# References
