% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  a4paper,
]{article}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage[]{natbib}
\bibliographystyle{plainnat}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\newtheorem*{definition}{Definition}
\newtheorem*{corollary}[Corollary]
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\sgn}{sgn}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Location and Scale-Invariant Power Transformations for Transforming Data to Normality},
  pdfauthor={Alex Zwanenburg, Steffen Löck},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Location and Scale-Invariant Power Transformations for
Transforming Data to Normality}
\author{Alex Zwanenburg, Steffen Löck}
\date{2025-10-28}

\begin{document}
\maketitle

\section{Abstract}\label{abstract}

Power transformations are used to stabilize variance and achieve
normality of features, especially in methods assuming normal
distributions such as ANOVA and linear discriminant analysis. However,
the commonly used Box-Cox and Yeo-Johnson power transformation methods
are sensitive to the location, scale, and presence of outliers in the
data. Here we present location- and scale-invariant Box-Cox and
Yeo-Johnson transformations to mitigate these issues. We derive maximum
likelihood estimation criteria for optimizing transformation parameters
and propose robust adaptations that reduce the influence of outliers. We
also introduce an empirical test for assessing central normality of
transformed features. In simulations and real-world datasets, robust
location- and scale-invariant transformations outperform conventional
variants, resulting in better transformations to central normality. In a
machine learning experiment with 231 datasets with numerical features,
integrating robust location- and scale-invariant power transformations
into an automated data processing and machine learning pipeline did not
result in a meaningful improvement or detriment to model performance
compared to conventional variants. In conclusion, robust location- and
scale-invariant power transformations can replace conventional variants.

\section{Introduction}\label{introduction}

Many statistical and some machine learning methods assume normality of
the underlying data, e.g.~analysis of variance and linear discriminant
analysis. However, numerical features in datasets may strongly deviate
from normal distributions, e.g.~by being skewed. Power transformations
aim to stabilise variance and improve normality of such features
\citep{Bartlett1947-rx, Tukey1957-rt}. The two most commonly used
transformations are that of \citet{Box1964-mz} and \citet{Yeo2000-vw}.

The Box-Cox transformation is defined as:

\begin{definition}[Box-Cox power transformation]
Let $\mathbf{X} = \left\{x_1, x_2, \ldots, x_n | x_i > 0 \right\}$ be a finite sequence of length $n > 0$.
Let $\lambda \in \mathcal{R}$ be a transformation parameter.
Then the Box-Cox power transformation of element $x_i$ is defined as:
\begin{equation}
\label{eqn:box-cox-original}
\phi_{\text{BC}}^\lambda (x_i) = 
\begin{cases}
\left(x_i^\lambda - 1 \right) / \lambda & \text{if } \lambda \neq 0\\
\log(x_i) & \text{if } \lambda = 0
\end{cases}
\end{equation}
\end{definition}

One limitation of the Box-Cox transformation is that it is only defined
for \(x_i > 0\). In contrast, the Yeo-Johnson transformation under the
transformation parameter \(\lambda\) is defined for any
\(x_i \in \mathbb{R}\), as follows:

\begin{definition}[Yeo-Johnson power transformation]
Let $\mathbf{X} = \left\{x_1, x_2, \ldots, x_n | x_i \in \mathbb{R} \right\}$ be a finite sequence of length $n > 0$.
Let $\lambda \in \mathcal{R}$ be a transformation parameter.
\begin{equation}
\label{eqn:yeo-johnson-original}
\phi_{\text{YJ}}^\lambda (x_i) = 
\begin{cases}
\left( \left( 1 + x_i \right)^\lambda - 1\right) / \lambda & \text{if } \lambda \neq 0 \text{ and } x_i \geq 0\\
\log(1 + x_i) & \text{if } \lambda = 0 \text{ and } x_i \geq 0\\
-\left( \left( 1 - x_i\right)^{2 - \lambda} - 1 \right) / \left(2 - \lambda \right) & \text{if } \lambda \neq 2 \text{ and } x_i < 0\\
-\log(1 - x_i) & \text{if } \lambda = 2 \text{ and } x_i < 0
\end{cases}
\end{equation}
\end{definition}

The \(\lambda\)-parameter is typically optimised using maximum
likelihood estimation under the assumption that the transformed feature
is normally distributed. As noted by Raymaekers and Rousseeuw, this
approach is sensitive to outliers, and robust versions of Box-Cox and
Yeo-Johnson transformations were devised \citep{Raymaekers2024-zf}.

Applying a power transformation does not guarantee that transformed
features are normally distributed. Depending on location and scale of a
feature and the presence of outliers, power transformations may decrease
normality, as shown in Figure \ref{fig:decreased-normality}. If
normality of the transformed feature is not checked, e.g.~in automated
power transformation in machine learning workflows, several issues may
arise. For example, statistical tests such as ANOVA may produce
incorrect results due to violation of the normality assumption.
Likewise, machine learning methods that assume normality of input
features may suffer a decrease in performance. Moreover, large negative
or positive \(\lambda\)-parameters may lead to numeric issues in any
subsequent computations.

Statistical tests for normality, such as the Shapiro-Wilk test
\citep{Shapiro1965-zd}, could be automatically applied to transformed
features. However, given sufficiently large sample sizes such tests can
detect trivial deviations from normality, and may lead to rejection of
sufficiently good power transformations.

\begin{figure}

{\centering \includegraphics{manuscript_files/figure-latex/decreased-normality-1} 

}

\caption{Effect of location, scale and outliers on estimation of the Box-Cox and Yeo-Johnson transformation parameter $\lambda$. $10000$ samples were drawn from a normal distribution: $\mathcal{N}(\mu, 1)$ for the \textit{shift} dataset, $\mathcal{N}(10, \sigma)$ for the \textit{scale} dataset and $\mathcal{N}(0, 1)$ for the \textit{outlier} dataset. Additionally, an outlier with value $d$ was added to the outlier dataset. Since samples are drawn from a normal distribution, a transformation parameter of $\lambda = 1$ is expected. However, a large shift in location, a scale that is small compared to the location, or presence of large outliers lead to incorrectly estimated transformation parameter values.}\label{fig:decreased-normality}
\end{figure}

To address these issues, we make the following contributions:

\begin{itemize}
\item
  We devise location- and scale-invariant versions of the Box-Cox and
  Yeo-Johnson transformation, including versions robust to outliers.
\item
  We derive the maximum likelihood criterion for location- and
  scale-invariant Box-Cox and Yeo-Johnson transformations to allow for
  optimising transformation parameters.
\item
  We define an empirical central normality test for detecting cases
  where power transformations fail to yield an approximately normally
  distributed transformed feature.
\item
  We assess the effect of power transformations on the performance of
  machine learning models.
\end{itemize}

\section{Theory}\label{theory}

In this section, we will first introduce location- and scale-invariant
versions of the Box-Cox and Yeo-Johnson transformations. Subsequently,
we define weighted location- and scale-invariant transformations and
weighting methods for robust transformations. We then define the
quantile function for asymmetric generalised normal distributions to
enable random sampling. Finally, we define the overall framework for the
empirical central normality test.

First, we define a (numeric) feature. Throughout this work, whenever
reference is made to a feature, a numeric feature is meant unless noted
otherwise.

\begin{definition}[Numeric feature]
A numeric feature is a finite sequence $\mathbf{X} = \left\{x_1, x_2, \ldots, x_n | x_i \in  \right\}$ of length $n$.
\end{definition}

\subsection{Location- and scale-invariant power
transformation}\label{location--and-scale-invariant-power-transformation}

\citet{Box1964-mz} originally included a shift parameter in their
transformation, but for their formal analysis set it to \(0\), whereas
\citet{Yeo2000-vw} did not include a shift or scale parameter. Here,
Box-Cox and Yeo-Johnson transformations are modified by introducing
shift parameter \(x_0\) and scale parameter \(s\) into equations
\ref{eqn:box-cox-original} and \ref{eqn:yeo-johnson-original}. The
location- and scale-invariant Box-Cox transformation of a feature value
\(x_i\) of feature \(\mathbf{X}\) under transformation parameter
\(\lambda\), shift parameter \(x_0\) and scale parameter \(s\) is then
defined as:

\begin{definition}[Location- and scale- invariant Box-Cox power transformation]
Let $\lambda \in \mathcal{R}$ be a transformation parameter.
Let $x_0 \in \mathcal{R}$ and $s > 0$ be location and scale parameters.
Let $\mathbf{X} = \left\{x_1, x_2, \ldots, x_n | x_i - x_0 > 0 \right\}$ be a finite sequence of length $n > 0$.

Then the location- and scale-invariant Box-Cox power transformation of element $x_i$ is defined as:

\begin{equation}
\label{eqn:box-cox-invariant}
\phi_{\text{BC}}^{\lambda, x_0, s} (x_i) = 
\begin{cases}
\left( \left(\frac{x_i - x_0}{s} \right)^\lambda - 1 \right) / \lambda & \text{if } \lambda \neq 0\\
\log\left[\frac{x_i - x_0}{s}\right] & \text{if } \lambda = 0
\end{cases}
\end{equation}
\end{definition}

Likewise, the location- and scale-invariant Yeo-Johnson transformation
of a feature value \(x_i\) under transformation parameter \(\lambda\),
shift parameter \(x_0\) and scale parameter \(s\) is defined as:

\begin{definition}[Location- and scale- invariant Yeo-Johnson power transformation]
Let $\lambda \in \mathcal{R}$ be a transformation parameter.
Let $x_0 \in \mathcal{R}$ and $s > 0$ be location and scale parameters.
Let $\mathbf{X} = \left\{x_1, x_2, \ldots, x_n | x_i \in \mathcal{R} \right\}$ be a finite sequence of length $n > 0$.

Then the location- and scale-invariant Yeo-Johnson power transformation of element $x_i$ is defined as:

\begin{equation}
\label{eqn:yeo-johnson-invariant}
\phi_{\text{YJ}}^{\lambda, x_0, s} (x_i) = 
\begin{cases}
\left( \left( 1 + \frac{x_i - x_0}{s}\right)^\lambda - 1\right) / \lambda & \text{if } \lambda \neq 0 \text{ and } x_i - x_0 \geq 0\\
\log\left[1 + \frac{x_i - x_0}{s}\right] & \text{if } \lambda = 0 \text{ and } x_i - x_0 \geq 0\\
-\left( \left( 1 - \frac{x_i - x_0}{s}\right)^{2 - \lambda} - 1 \right) / \left(2 - \lambda \right) & \text{if } \lambda \neq 2 \text{ and } x_i - x_0 < 0\\
-\log\left[1 - \frac{x_i - x_0}{s}\right] & \text{if } \lambda = 2 \text{ and } x_i - x_0 < 0
\end{cases}
\end{equation}
\end{definition}

For both invariant transformations, \(\lambda\), \(x_0\) and \(s\)
parameters can be obtained by maximising the log-likelihood function,
i.e.~using maximum likelihood estimation (MLE). A full derivation of the
log-likelihood function for both transformations is shown in Appendix A.
The location- and scale-invariant Box-Cox log-likelihood function is:

\begin{definition} [Log-likelihood function of the location- and scale- invariant Box-Cox power transformation]
Let $\mathbf{X}$, $\lambda$, $x_0$, and $s$ be defined as earlier (Eqn. \ref{eqn:box-cox-invariant}).
Let $\mu$ and $\sigma^2$ be the mean and variance of the transformed sequence $\phi_{\text{BC}}^{\lambda, x_0, s} (\mathbf{X})$, respectively.

Then the log-likelihood function of the location- and scale- invariant Box-Cox power transformation is defined as:
\begin{equation}
\label{eqn:box-cox-invariant-log-likelihood}
\begin{split}
\mathcal{L}_{\text{BC}}^{\lambda, x_0, s} = & -\frac{n}{2} \log \left[2 \pi \sigma^2 \right] -\frac{1}{2 \sigma^2} \sum_{i=1}^n \left( \phi_{BC}^{\lambda, x_0, s}(x_i) - \mu \right)^2 \\
& -n \lambda \log s + \left( \lambda - 1 \right) \sum_{i=1}^n \log \left[ x_i - x_0 \right]
\end{split}
\end{equation}
\end{definition}

Similarly, the location- and scale-invariant Yeo-Johnson log-likelihood
function is:

\begin{definition}[Log-likelihood function of the location- and scale- invariant Yeo-Johnson power transformation]
Let $\mathbf{X}$, $\lambda$, $x_0$, and $s$ be defined as earlier (Eqn. \ref{eqn:yeo-johnson-invariant}).
Let $\mu$ and $\sigma^2$ be the mean and variance of the transformed sequence $\phi_{\text{YJ}}^{\lambda, x_0, s} (\mathbf{X})$, respectively.

Then the log-likelihood function of the location- and scale- invariant Yeo-Johnson power transformation is defined as:
\begin{equation}
\label{eqn:yeo-johnson-invariant-log-likelihood}
\begin{split}
\mathcal{L}_{\text{YJ}}^{\lambda, x_0, s} = & -\frac{n}{2} \log\left[2 \pi \sigma^2\right] -\frac{1}{2 \sigma^2} \sum_{i=1}^n \left( \phi_{YJ}^{\lambda, x_0, s}(x_i) - \mu \right)^2 \\
& - n \log s + (\lambda - 1) \sum_{i=1}^n \mathop{\mathrm{sgn}}(x_i - x_0) \log \left[1 + \frac{|x_i - x_0|}{s} \right]
\end{split}
\end{equation}
\end{definition}

\subsection{Robust location- and scale-invariant power
transformations}\label{robust-location--and-scale-invariant-power-transformations}

Real-world data may contain outliers, to which maximum likelihood
estimation can be sensitive. Their presence may lead to poor
transformations to normality, as shown in Figure
\ref{fig:decreased-normality}. As indicated by
\citet{Raymaekers2024-zf}, the general aim of power transformations
should be to transform non-outlier data to normality, i.e.~achieve
\emph{central normality}. To achieve this, they devised an iterative
procedure to find a robust estimate of the transformation parameter
\(\lambda\). Briefly, this process requires identifying outliers in the
data and weighting such instances during the optimisation process.
\citet{Raymaekers2024-zf} achieve this through weighted maximum
likelihood estimation. However, because this procedure iteratively
estimates and updates \(\lambda\), it can not be used here to
simultaneously estimate \(\lambda\), \(x_0\) and \(s\) for location- and
scale-invariant power transformations. Nonetheless, as a procedure,
weighted MLE can be used for estimating the transformation, shift and
scale parameters.

Here, weighted maximum likelihood estimation is based on equations
\ref{eqn:box-cox-invariant-log-likelihood} and
\ref{eqn:yeo-johnson-invariant-log-likelihood}. Compared to
\citet{Raymaekers2024-zf}, these log-likelihood functions include
additional terms to accommodate estimation of \(x_0\) and \(s\). The
weighted location- and scale-invariant Box-Cox log-likelihood function
is:

\begin{definition}[Weighted log-likelihood function of the location- and scale- invariant Box-Cox power transformation]
Let $\mathbf{X}$, $\lambda$, $x_0$, and $s$ be defined as earlier (Eqn. \ref{eqn:box-cox-invariant}).
Let $w_i \geq 0$ be the weight corresponding to each element of $\mathbf{X}$.
Let $\mu_w$ be the weighted mean of the Box-Cox transformed sequence:
\begin{equation*}
\mu_w = \frac{\sum_{i=1}^n w_i \phi_{\text{BC}}^{\lambda, x_0, s} (x_i)} {\sum_{i=1}^n w_i}
\end{equation*}

Let $\sigma^2_w$ be the weighted variance of the Box-Cox transformed sequence:
\begin{equation*}
\sigma_w^2 = \frac{\sum_{i=1}^n w_i \left(\phi_{\text{BC}}^{\lambda, x_0, s} (x_i) - \mu_w \right)^2}{\sum_{i=1}^n w_i}
\end{equation*}

Then, the weighted log-likelihood function of the location- and scale- invariant Box-Cox power transformation is:

\begin{equation}
\label{eqn:box-cox-weighted-invariant-log-likelihood}
\begin{split}
\mathcal{L}_{\text{rBC}}^{\lambda, x_0, s} = & -\frac{1}{2} \left(\sum_{i=1}^n w_i \right) \log \left[ 2 \pi \sigma_w^2 \right] -\frac{1}{2 \sigma_w^2} \sum_{i=1}^n w_i \left( \phi_{\text{BC}}^{\lambda, x_0, s}(x_i) - \mu_w \right)^2 \\
& - \lambda \left( \sum_{i=1}^n w_i \right) \log s + \left( \lambda - 1 \right) \sum_{i=1}^n w_i \log \left[ x_i - x_0 \right]
\end{split}
\end{equation}
\end{definition}

Analogously, the weighted location- and scale-invariant Yeo-Johnson
log-likelihood function is:

\begin{definition}[Weighted log-likelihood function of the location- and scale- invariant Yeo-Johnson power transformation]

Let $\mathbf{X}$, $\lambda$, $x_0$, and $s$ be defined as earlier (Eqn. \ref{eqn:yeo-johnson-invariant}).
Let $w_i \geq 0$ be the weight corresponding to each element of $\mathbf{X}$.
Let $\mu_w$ be the weighted mean of the Yeo-Johnson transformed sequence:
\begin{equation*}
\mu_w = \frac{\sum_{i=1}^n w_i \phi_{\text{YJ}}^{\lambda, x_0, s} (x_i)} {\sum_{i=1}^n w_i}
\end{equation*}

Let $\sigma^2_w$ be the weighted variance of the Yeo-Johnson transformed sequence:
\begin{equation*}
\sigma_w^2 = \frac{\sum_{i=1}^n w_i \left(\phi_{\text{YJ}}^{\lambda, x_0, s} (x_i) - \mu_w \right)^2}{\sum_{i=1}^n w_i}
\end{equation*}

Then, the weighted log-likelihood function of the location- and scale- invariant Yeo-Johnson power transformation is:

\begin{equation}
\label{eqn:yeo-johnson-weighted-invariant-log-likelihood}
\begin{split}
\mathcal{L}_{\text{rYJ}}^{\lambda, x_0, s} = & -\frac{1}{2} \left(\sum_{i=1}^n w_i \right) \log \left[ 2 \pi \sigma_w^2 \right] -\frac{1}{2 \sigma_w^2} \sum_{i=1}^n w_i \left( \phi_{\text{YJ}}^{\lambda, x_0, s}(x_i) - \mu_w \right)^2 \\
& - \left( \sum_{i=1}^n w_i \right) \log s + (\lambda - 1) \sum_{i=1}^n w_i \mathop{\mathrm{sgn}}(x_i - x_0) \log \left[1 + \frac{|x_i - x_0|}{s} \right]
\end{split}
\end{equation}
\end{definition}

The weights \(w_i\) in equations
\ref{eqn:box-cox-weighted-invariant-log-likelihood} and
\ref{eqn:yeo-johnson-weighted-invariant-log-likelihood} can be set using
several weighting functions. Using \(\dot{x}_i\) as an argument that
will be defined later, we investigate three weighting functions:

\begin{itemize}
\item
  A step function, with \(\delta_1 \geq 0\) as threshold parameter:
  \begin{equation*}
  w_i =
  \begin{cases}
  1 & \text{if } \left| \dot{x}_i \right| \leq \delta_1\\
  0 & \text{if } \left| \dot{x}_i \right| > \delta_1
  \end{cases}
  \end{equation*}
\item
  A triangle function (or generalised Huber weight), with
  \(\delta_1 \geq 0\) and \(\delta_2 \geq \delta_1\) as threshold
  parameters: \begin{equation*}
  w_i =
  \begin{cases}
  1 & \text{if } \left| \dot{x}_i \right| < \delta_1\\
  1 - \frac{\left| \dot{x}_i \right| - \delta_1}{\delta_2 - \delta_1} & \text{if } \delta_1 \leq \left| \dot{x}_i \right| \leq \delta_2 \\
  0 & \text{if } \left| \dot{x}_i \right| > \delta_2
  \end{cases}
  \end{equation*}
\item
  A tapered cosine function \citep{Tukey1967-eb}, with
  \(\delta_1 \geq 0\) and \(\delta_2 \geq \delta_1\) as threshold
  parameters: \begin{equation*}
  w_i =
  \begin{cases}
  1 & \text{if } \left| \dot{x}_i \right| < \delta_1\\
  0.5 + 0.5 \cos\left(\pi \frac{\left| \dot{x}_i \right| - \delta_1}{\delta_2 - \delta_1} \right) & \text{if } \delta_1 \leq \left| \dot{x}_i \right| \leq \delta_2 \\
  0 & \text{if } \left| \dot{x}_i \right| > \delta_2
  \end{cases}
  \end{equation*}
\end{itemize}

All weighting functions share the characteristic that for
\(\left| \dot{x}_i \right|< \delta_1\), instances are fully weighted,
i.e.~when \(\delta_1 > 0\) the weighting functions are symmetric window
functions with a flat top. The triangle and tapered cosine functions
then gradually down-weight instances with
\(\delta_1 \leq \left| \dot{x}_i \right| \leq \delta_2\), and assign no
weight to instances \(\left| x_i \right| > \delta_2\). Examples of these
weighting function are shown in Figure \ref{fig:weighting-functions}.

\begin{figure}

{\centering \includegraphics{manuscript_files/figure-latex/weighting-functions-1} 

}

\caption{Weighting functions investigated in this study to make power transformations more robust against outliers. In this example, the step function was parameterised with $\delta_1 = 0.60$. The triangle and tapered cosine functions were both parameterised with $\delta_1 = 0.30$ and $\delta_2 = 0.90$.}\label{fig:weighting-functions}
\end{figure}

Each weighting function has an argument \(\dot{x}\) that is related to
the (transformed) feature in one of several ways:

\begin{itemize}
\item
  The weighting function uses empirical probabilities of the
  distribution of the original feature \(\mathbf{X}\). After sorting
  \(\mathbf{X}\) in ascending order, probabilities are determined as
  \(p_i = \frac{i - 1/3}{n + 1/3}\), with \(i = 1, 2, \ldots n\), with
  \(n\) the number of instances of feature \(\mathbf{X}\). Then
  \(\dot{x}_i = p^{*}_i=2 \left( p_i - 0.5\right)\), so that argument is
  zero-centered.
\item
  The weighting function uses the z-score of the transformed feature
  \(\phi^{\lambda, x_0, s} (\mathbf{X})\). After
  \citet{Raymaekers2024-zf},
  \(z_i = \frac{\phi^{\lambda, x_0, s}(x_i) - \mu_M}{\sigma_M}\). Here,
  \(\mu_M\) and \(\sigma_M\) are robust Huber M-estimates of location
  and scale of the transformed feature
  \(\phi^{\lambda, x_0, s} (\mathbf{X})\) \citep{Huber1981-su}. Then
  \(\dot{x}_i = z_i\).
\item
  After sorting \(\mathbf{X}\) in ascending order, the weighting
  function uses the residual error between the z-score of the
  transformed feature \(\phi^{\lambda, x_0, s} (\mathbf{X})\) and the
  theoretical z-score from a standard normal distribution:
  \(r_i =\left| \left( \phi^{\lambda, x_0, s}(x_i) - \mu_M)\right) / \sigma_M - F^{-1}_{\mathcal{N}}(p_i) \right|\),
  with \(\mu_M\), \(\sigma_M\) and \(p_i\) as defined above. Then
  \(\dot{x}_i = r_i\).
\end{itemize}

\subsection{Asymmetric generalised normal
distributions}\label{asymmetric-generalised-normal-distributions}

Modifications intended to make power transformations invariant to
location and scale of a feature and methods to improve their robustness
against outliers need to be assessed using data drawn from a range of
different distributions. Since the power transformations are intended
for use with unimodal distributions, the generalised normal distribution
\citep{Subbotin1923-qk, Nadarajah2005-xe} is a suitable option for
simulating realistic feature distributions. This distribution has the
following probability density function \(f_{\beta}\) for a value
\(x \in \mathbb{R}\):

\begin{definition}[Standard generalised normal distribution probability density function]
Let $x \in \mathcal{R}$.
Let $\beta > 0$ be a shape parameter.
Let $\Gamma$ be the gamma function.

Then the standard generalised normal distribution probability density function, without scale and location parameters, is:
\begin{equation}
f_{\beta}(x) = \frac{\beta}{2\Gamma\left(1 / \beta \right)} e^{-\left| x \right|^\beta}
\end{equation}
\end{definition}

For \(\beta = 1\), the probability density function describes a Laplace
distribution. A normal distribution is found for \(\beta=2\), and for
large \(\beta\), the distribution approaches a uniform distribution.

Realistic feature distributions may be skewed. Gijbels et al.~describe a
recipe for introducing skewness into the otherwise symmetric generalised
normal distribution \citep{Gijbels2019-te}, leading to the following
probability density function:

\begin{definition}[Asymmetric generalised normal distribution probability density function]
Let $\alpha \in (0,1)$ be a skewness parameter.
Let $\mu \in \mathcal{R}$ be a location parameter.
Let $\sigma > 0$ be a shape parameter.

Then the probability density function of the asymmetric generalised normal distribution is:
\begin{equation}
f_{\alpha}(x; \mu, \sigma, \beta) = \frac{2 \alpha \left(1 - \alpha\right)}{\sigma}
\begin{cases}
f_{\beta}\left( \left(1 - \alpha \right) \frac{\left| x - \mu \right|}{\sigma} \right) & \text{, } x \leq \mu \\
f_{\beta}\left( \alpha \frac{\left| x - \mu \right|}{\sigma} \right) & \text{, } x > \mu
\end{cases}
\end{equation}
\end{definition}

\(\alpha > 0.5\) creates a distribution with a negative skew, i.e.~a
left-skewed distribution. A right-skewed distribution is created for
\(\alpha < 0.5\). \(f_{\alpha}\) thus describes the probability density
function of an asymmetric generalised normal distribution, which we will
refer to here and parametrise as
\(\mathcal{AGN}\left(\mu, \sigma, \alpha, \beta \right)\).

We require a quantile function (or an approximation thereof) to draw
random values from an asymmetric generalised normal distribution using
inverse transform sampling. Gijbels et al.~derived the quantile function
\(F_{\alpha}^{-1}(p)\), which incorporates the quantile function of the
symmetric generalised normal distribution derived by Griffin
\citep{Griffin2018-bf}:

\begin{definition}[Asymmetric generalised normal distribution quantile function]
Let $p \in \left[0, 1\right]$ be a probability.
Let  $F_{\Gamma}^{-1}$ is the quantile function of the gamma distribution with shape $1 / \beta$, which can be numerically approximated. 
Let $F_{\beta}^{-1}$ be the quantile function of the generalised normal distribution:
\begin{equation}
F_{\beta}^{-1}(p) = \mathop{\mathrm{sgn}}\left(p - 0.5 \right) F_{\Gamma}^{-1}\left(2 \left|p - 0.5 \right|; 1 / \beta \right)
\end{equation}

Then the quantile function of the asymmetric generalised normal distribution is:
\begin{equation}
F_{\alpha}^{-1}(p; \mu, \sigma, \beta) =
\begin{cases}
\mu + \frac{\sigma}{1 - \alpha} F_{\beta}^{-1} \left( \frac{p}{2 \alpha}\right) & \text{, } p \leq \alpha \\
\mu + \frac{\sigma}{\alpha} F_{\beta}^{-1} \left( \frac{1 + p - 2 \alpha}{2 \left(1 - \alpha \right)} \right) & \text{, } p > \alpha
\end{cases}
\end{equation}
\end{definition}

\subsection{Empirical central normality
test}\label{empirical-central-normality-test}

Power transformations aim to transform features to a normal
distribution. However, this may not always be successful or possible.
Deviations from normality can be detected by normality tests, such as
the Shapiro-Wilk test \citep{Shapiro1965-zd}. In practice, normality
tests may be too stringent with large sample sizes, outliers, or both.
Here we develop an empirical test for central normality.

\begin{definition}[Central portion of a sequence]
Let $\mathbf{X} = \left\{x_1, x_2, \ldots, x_n | x_i \in \mathbb{R}\right\}$ be a finite sequence of length $n > 0$,
ordered so that $x_1 \leq x_2 \leq \ldots \leq x_n$.
Let $p_i = \frac{i - 1/3}{n + 1/3}$, with $i = 1, 2, \ldots n$ be the percentile value
corresponding to each element in $\mathbf{X}$. For elements with tied values in $\mathbf{X}$,
percentile values are replaced by the average in their group, i.e.
if $x_j = x_{j+1} = \ldots = x_{j+m}$, then 
$p_j' = p_{j+1}' = \ldots = p_{j+m}' = 1/(m+1)\sum_j^{j+m}p_j$.
Furthermore, let central portion $\kappa \in (0,1)$.

Then the central portion of $\mathbf{X}$ is 
$\mathbf{X}_{\text{c}} = \left\{x_i \in \mathbf{X} \, | \,  \frac{1-\kappa}{2} \leq  p_i \leq \frac{1 + \kappa}{2}\right\}$.

\end{definition}

\begin{definition}[Residual errors of the central portion of a sequence]

Let the residual error for each element of $\mathbf{X}$ be $r_i =\left| \frac{x_i - \mu_M}{\sigma_M} - F^{-1}_{\mathcal{N}}(p_i) \right|$, 
with $\mu_M$ and $\sigma_M$ robust Huber M-estimates of location and scale of $\mathbf{X}$ [@Huber1981-su], 
and $F^{-1}_{\mathcal{N}}$ the quantile function of the normal distribution $\mathcal{N}(0, 1)$.

Then, the set of residual errors of the central portion of $\mathbf{X}$ (i.e. $\mathbf{X}_{\text{c}}$) is
$\mathbf{R}_{\text{c}} = \left\{ r_i \in \left\{ r_1, r_2, \ldots, r_n\right\} \, | \,  \frac{1-\kappa}{2} \leq  p_i \leq \frac{1 + \kappa}{2}\right\}$.

\end{definition}

\begin{definition}[Central normal distribution]

A central normal distribution is any distribution whose quantile function is 
defined by $F^{-1}_{\mathcal{N}}$ for $\frac{1-\kappa}{2} \leq  p \leq \frac{1 + \kappa}{2}$,
with $\mathcal{N}$ any normal distribution.

\end{definition}

\begin{corollary}

For $\kappa = 1.0$, the central normal distribution is a normal distribution.

\end{corollary}

\begin{corollary}

By the Glivenko-Cantelli theorem, if a sequence $\mathbf{X}$ was sampled from a
central normal distribution, then almost surely
$\lim_{n \rightarrow \infty} \sum_{r_j \in \mathbf{R}_{\text{c}}} r_j = 0$,

\end{corollary}

\begin{definition}[Central normality test]
The null-hypothesis $\mathcal{H}_0$ is that the sequence $\mathbf{X}$ was sampled
from a population that is distributed according to a central normal distribution.

The alternative hypothesis is that the sequence $\mathbf{X}$ was sampled from a
population that is not distributed according to a central normal distribution.
\end{definition}

To test this hypothesis, a test statistic is computed and compared
against a critical value.

\begin{definition}[Central normality test statistic]
Let $m$ be the number of elements of $\mathbf{R}_{\text{c}}$.

The test statistic is then $\tau_{n, \kappa} = \frac{1}{m}\sum_{r_j \in \mathbf{R}_{\text{c}}}r_j$.

\end{definition}

The null-hypothesis should be rejected at significance level \(\alpha\)
if \(\tau_{n, \kappa} \geq \tau_{\alpha, n, \kappa, \text{critical}}\).
Critical test statistic values are determined in the
\hyperref[simulation-ecn-test]{Simulation} section.

\section{Simulation}\label{simulation}

We used simulated data to assess invariance to location and scale of the
proposed power transformations, to develop the empirical central
normality test, and to determine weighting for robust transformations.
The \(\lambda\) parameter for conventional power transformations (Eqn.
\ref{eqn:box-cox-original} and \ref{eqn:yeo-johnson-original}), as well
as \(\lambda\), \(x_0\) and \(s\) parameters for location- and
scale-invariant power transformations (Eqn. \ref{eqn:box-cox-invariant}
and \ref{eqn:yeo-johnson-invariant}) were estimated using the BOBYQA
algorithm for derivative-free bound constraint optimisation
\citep{Powell2009-zb} through maximum likelihood estimation. The
required algorithms were implemented in the \texttt{power.transform} R
software package \citep{Zwanenburg2024-kq} (version 1.0.1). Of note, the
\texttt{power.transform} package shifts feature values into the positive
domain if negative or zero values are present for Box-Cox power
transformations.

\subsection{Invariance to location and
scale}\label{invariance-to-location-and-scale}

To assess whether the proposed power transformations lead to values of
\(\lambda\) that are invariant to location and scale of the
distribution, we simulated three different sequences. We first randomly
drew \(10000\) values from a normal distribution:
\(\mathbf{X}_{\text{normal}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{N}\left(0, 1\right)\),
or equivalently
\(\mathbf{X}_{\text{normal}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{AGN}\left(0, 1/\sqrt{2}, 0.5, 2\right)\).
The second distribution was a right-skewed generalised normal
distribution
\(\mathbf{X}_{\text{right}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{AGN}\left(0, 1/\sqrt{2}, 0.2, 2\right)\).
The third distribution was a left-skewed generalised normal distribution
\(\mathbf{X}_{\text{left}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{AGN}\left(0, 1/\sqrt{2}, 0.8, 2\right)\).
We then computed transformation parameter \(\lambda\) using the original
definitions (Eqn. \ref{eqn:box-cox-original} and
\ref{eqn:yeo-johnson-original}) and the location- and scale-invariant
definitions (Eqn. \ref{eqn:box-cox-invariant} and
\ref{eqn:yeo-johnson-invariant}) for each distribution. To assess
location invariance, a positive value \(d_{\text{shift}}\) was added to
each distribution with \(d_{\text{shift}} \in [1, 10^6]\). Similarly, to
assess scale invariance, each distribution was multiplied by a positive
value \(d_{\text{scale}}\), where \(d_{\text{scale}} \in [1, 10^6]\).

\begin{figure}

{\centering \includegraphics{manuscript_files/figure-latex/shifted-distributions-1} 

}

\caption{Invariant power transformation produces transformation parameters that are invariant to location and scale. Samples were drawn from normal, right-skewed and left-skewed distributions, respectively, which then underwent a shift $d_{\text{shift}}$ or multiplication by $d_{\text{scale}}$. Estimates of the transformation parameter $\lambda$ for the conventional power transformations show strong dependency on the overall location and scale of the distribution, whereas estimates obtained for the location- and scale-invariant power transformations are constant.}\label{fig:shifted-distributions}
\end{figure}

The result is shown in Figure \ref{fig:shifted-distributions}. For each
distribution, transformation parameter \(\lambda\) varied with
\(d_{\text{shift}}\) and \(d_{\text{scale}}\) when estimated for
conventional transformations. In contrast, estimation of \(\lambda\) for
invariant power transformations was invariant to both
\(d_{\text{shift}}\) and \(d_{\text{scale}}\).

\subsection{Empirical central normality test}\label{simulation-ecn-test}

The critical test statistic for the empirical central normality test
depends on the significance level \(\alpha\), sequence length \(n\) and
central portion \(\kappa\). Critical test statistic values were set
through simulation, as described below.

For each number of samples
\(n \in \left\{\lfloor 10^\nu \rfloor | \nu \in \left\{0.7500, 0.8125, \ldots, 4.0000 \right \} \right\}\)
we randomly sampled \(m_d = 30000\) ordered sequences \(\mathbf{X}\)
from \(\mathcal{N}(0,1)\). For each sequence we then computed
\(\tau_{\kappa}\) for
\(\kappa \in \left\{0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 1.00\right\}\).

Figure \ref{fig:empirical-central-normality-combined} shows the critical
test statistic \(\tau_{\alpha, n, \kappa}\) as a function of \(n\) for
different values of \(\kappa\) and \(\alpha\). For larger central
portion \(\kappa\), critical test statistic values increase. Similarly,
critical test statistic values increase for lower significance levels.

\begin{figure}

{\centering \includegraphics{manuscript_files/figure-latex/empirical-central-normality-combined-1} 

}

\caption{Critical test statistic $\tau_{\alpha = 0.05, n, \kappa}$ of the empirical central normality test as function of $n$ for several values of central portion $\kappa$ (left) and several values of significance level $\alpha$ (right).}\label{fig:empirical-central-normality-combined}
\end{figure}

To assess type I error rates for the empirical central normality test
and compare these to the Shapiro-Wilk test for normality, we randomly
drew another 10000 sequences from a central normal distribution for each
\(n \in \left\{\lfloor 10^\nu \rfloor | \nu \in \left\{0.7500, 0.8125, \ldots, 3.0000 \right \} \right\}\).

Each sequence was created as follows: first \(n\) values are sampled
from \(\mathcal{N}(0, 1)\). Then, the elements outside the central
portion of the sequence
(i.e.~\(\mathbf{X} \setminus \mathbf{X}_{\text{c}}\)) are replaced.
Elements with \(pi < \frac{1-\kappa}{2}\) are replaced with values
sampled from \(\mathcal{U}(-10,\min(\mathbf{X}_{\text{c}}))\), and
elements with \(p_i > \frac{1 + \kappa}{2}\) are replaced with values
sampled from \(\mathcal{U}(\max(\mathbf{X}_{\text{c}}), 10)\).

For each sequence, the p-value for the respective test was used to
reject the null hypothesis that sequences were derived from a population
with a (central) normal distribution. The null hypothesis was rejected
if \(p \leq 0.05\). The type I error rate was determined by computing
the fraction of sequences rejected this way. Figure
\ref{fig:empirical-central-normality-error_rate} shows that when the
central portion \(\kappa\) of the central normal distribution is equal
or greater than the \(\kappa\) parameter for the empirical central
normality test, the tests are accurate. If \(\kappa\) of the empirical
central normality test exceeds that the central portion \(\kappa\) for
the underlying distribution, tests are conservative. The Shapiro-Wilk
test behaves similar to the empirical central normality test with
\(\kappa = 1.0\), which is expected. For \(n < 10\), the empirical
central normality test can be optimistic.

\textbackslash begin\{figure\}

\{\centering \includegraphics{manuscript_files/figure-latex/empirical-central-normality-error_rate-1}

\}

\textbackslash caption\{Observed type I error rates for central
normality tests in the presence of data sampled from central normal
distributions with \$\kappa \in \left{0.6, 0.8, 1.0\right}\}. Three
empirical central normality (ECN) tests were performed, parameterised
with same value for the central portion \(\kappa\).Additional, the
Shapiro-Wilk test was assessed.ECN: empirical central
normality\}\label{fig:empirical-central-normality-error_rate}
\textbackslash end\{figure\}

In Figure \ref{fig:empirical-central-normality-examples} we apply the
empirical central normality test (with \(\kappa = 0.80\)) to assess
central normality of features that are composed of a mixture of samples
drawn from two normal distributions (\(n = 100\) each). With increased
separation of the underlying normal distributions, the test's p-value
decreases, as expected.

\begin{figure}

{\centering \includegraphics{manuscript_files/figure-latex/empirical-central-normality-examples-1} 

}

\caption{Bi-modal distributions and empirical central normality test results. The feature (black) is a mixture of two identical sample sets (blue and orange, $n = 100$) drawn from normal distributions that are offset by a distance $d$. The empirical central normality ($\kappa = 0.80$) is used to test the hypothesis that the mixture is sampled from a central normal distribution. As may be observed, with increasing offset $d$ the test's p-value decreases. Quantile-quantile plots are drawn below each distribution.}\label{fig:empirical-central-normality-examples}
\end{figure}

We use \(\kappa = 0.80\) for the empirical central normality tests in
the remainder of the manuscript, unless noted otherwise. Critical
statistic values of the empirical central normality test for
\(\kappa = 0.80\) are shown in Table
\ref{tab:empirical-central-normality-critical-statistic}.

\subsection{Robust transformations}\label{robust-transformations}

Outliers may be present in data and affect estimation of transformation
parameters. The log-likelihood function can be weighted to assign less
weight to outlier instances, see equations
\ref{eqn:box-cox-weighted-invariant-log-likelihood} and
\ref{eqn:yeo-johnson-weighted-invariant-log-likelihood}. We propose
three weighting functions: step, triangle and tapered cosine, that have
one, two and two parameters, respectively. Each weighting function then
uses one of the following inputs: probabilities of the empirical
distribution of the original feature, the z-score of the transformed
feature values, or the residual error between the z-score of the
transformed feature values and their expected z-score based on the
normal distribution.

To determine the weighting function parameters for each of the nine
combinations, \(m_d=500\) sequences \(\mathbf{X}_i\)
(\(i \in \{1, 2, \ldots, m_d\}\)) were randomly drawn from randomly
parametrised asymmetric generalised normal distributions. Each
distribution was parametrised with a random skewness parameter
\(\alpha \sim U\left(0.01, 0.99\right)\) and shape parameter
\(\beta \sim U\left(1.00, 5.00 \right)\). Location and scale parameters
were set as \(\mu = 0\) and \(\sigma = 1\), respectively. To form each
sequence \(\mathbf{X}_i\), \(n = \lceil 10^\gamma \rceil\) instances
were then randomly drawn, with
\(\gamma \sim U\left(\log_{10}50, 3\right)\), resulting in a set of
sequences with between \(50\) and \(1000\) elements each. Outlier values
were then drawn to randomly replace 10 percent of the elements of
\(\mathbf{X}_i\). These values were set according to
\citet{Tukey1977-xm}, as follows. Let
\(x^{*} \sim U\left(-2, 2\right)\). Then the corresponding outlier value
was:

\begin{equation}
\label{eqn:outlier_rate}
x_{out} =
\begin{cases}
Q_1 - \left(1.5 - x^{*} \right) \text{IQR} & \text{if } x^{*} < 0 \\
Q_3 + \left(1.5 + x^{*} \right) \text{IQR} & \text{if } x^{*} \geq 0
\end{cases}
\end{equation}

Here, \(Q_1\), \(Q_3\) and \(\text{IQR}\) are the first quartile, third
quartile and interquartile range of \(\mathbf{X}_i\), respectively.

To find the optimal values for the weighting function parameters
\(\delta_1\) and \(\delta_2\) (if applicable), we minimised a composite
loss
\(L = \sum_{i=1}^{m_d} L_{\text{cn},i} + 0.1 \sum_{i=1}^{m_d} L_{\lambda,i}\).
The composite loss consisted of two components: a loss term
\(L_{\text{cn},i} = \tau_{n_i, \kappa = 0.80,i}\), i.e.~the mean of
residual errors of the central 80\% of elements of each sequence
\(\mathbf{X}_i\), which aimed at optimising central normality; and a
loss term
\(L_{\lambda,i} = \max\left(0.0,|\lambda_{0,i} - \lambda_{i}| - \xi \right)\),
with \(\lambda_{0,i}\) and \(\lambda_{i}\) transformation parameters
found for sequence \(\mathbf{X}_i\) prior to and after adding outliers,
respectively, and tolerance parameter \(\xi = 0.5\) for Box-Cox and
\(\xi = 0.3\) for Yeo-Johnson power transformations. This second term
aimed to prevent solutions that provide small improvements in central
normality at the cost of a poor fit of the tails of sequences.

Minimisation was conducted using the BOBYQA algorithm for
derivative-free bound constraint optimisation \citep{Powell2009-zb}. The
resulting weighting function parameters for weighted MLE are shown in
Tables \ref{tab:optimal-weighting-parameters-box-cox} and
\ref{tab:optimal-weighting-parameters-yeo-johnson} for robust location-
and scale-invariant Box-Cox and Yeo-Johnson transformations,
respectively.

\begin{table}
\begin{center}
\caption{Optimal weighting parameters and corresponding loss for location- and scale-invariant Box-Cox power transformations.
$p^{*}$ indicates use of the empirical distribution of feature values, $z$ the z-score of the transformed feature values,
and $r$ the residual error between the z-score of transformed feature values and the expected z-score according to the normal distribution.
The \textit{initial} column shows the starting parameter value for the optimisation process, with the corresponding boundary values in the \textit{limits} column. 
The \textit{optimal} column shows the optimal parameter values.
The \textit{loss} column shows the composite loss achieved by each method under optimised parameters.
This loss is based on residual errors of transformed features and deviations in $\lambda$ parameters
found compared to the $\lambda$ parameters found prior to inserting outliers.
The loss metric can be compared between different weighting methods.
}
\label{tab:optimal-weighting-parameters-box-cox}
\begin{tabular}{l r r r r r r r}

\toprule
method & \multicolumn{3}{c}{$\delta_1$} & \multicolumn{3}{c}{$\delta_2$} & loss \\
& initial & limits & optimal & initial & limits & optimal & \\

\midrule
non-robust               & ---  & ---       & ---  & ---  & ---       & ---  & 49.6 \\
$p^{*}$ (step)           & 0.80 & $(0, 1]$  & 0.80 & ---  & ---       & ---  & 38.5 \\
$p^{*}$ (triangle)       & 0.80 & $(0, 1]$  & 0.01 & 1.00 & $(0, 1]$  & 0.86 & 43.7 \\
$p^{*}$ (tapered cosine) & 0.80 & $(0, 1]$  & 0.00 & 1.00 & $(0, 1]$  & 0.90 & 42.7 \\
$z$ (step)               & 1.28 & $(0, 10]$ & 2.39 & ---  & ---       & ---  & 52.2 \\
$z$ (triangle)           & 1.28 & $(0, 10]$ & 2.39 & 2.40 & $(0, 10]$ & 4.92 & 52.7 \\
$z$ (tapered cosine)     & 1.28 & $(0, 10]$ & 1.07 & 3.63 & $(0, 10]$ & 3.43 & 57.4 \\
$r$ (step)               & 0.50 & $(0, 10]$ & 0.83 & ---  & ---       & ---  & 53.1 \\
$r$ (triangle)           & 0.50 & $(0, 10]$ & 0.80 & 0.81 & $(0, 10]$ & 0.84 & 54.4 \\
$r$ (tapered cosine)     & 0.50 & $(0, 10]$ & 0.76 & 0.77 & $(0, 10]$ & 0.76 & 53.2 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}
\begin{center}
\caption{Optimal weighting parameters and corresponding loss for location- and scale-invariant Yeo-Johnson power transformations.
$p^{*}$ indicates use of the empirical distribution of feature values, $z$ the z-score of the transformed feature values,
and $r$ the residual error between the z-score of transformed feature values and the expected z-score according to the normal distribution. 
The \textit{initial} column shows the starting parameter value for the optimisation process, with the corresponding boundary values in the \textit{limits} column.
The \textit{loss} column shows the composite loss achieved by each method under optimised parameters.
This loss is based on residual errors of transformed features and deviations in $\lambda$ parameters
found compared to the $\lambda$ parameters found prior to inserting outliers.
The loss metric can be compared between different weighting methods.
}
\label{tab:optimal-weighting-parameters-yeo-johnson}
\begin{tabular}{l r r r r r r r}

\toprule
method & \multicolumn{3}{c}{$\delta_1$} & \multicolumn{3}{c}{$\delta_2$} & loss \\
& initial & limits & optimal & initial & limits & optimal & \\

\midrule
non-robust               & ---  & ---       & ---  & ---  & ---       & ---  & 42.1 \\
$p^{*}$ (step)           & 0.80 & $(0, 1]$  & 0.78 & ---  & ---       & ---  & 35.1 \\
$p^{*}$ (triangle)       & 0.80 & $(0, 1]$  & 0.20 & 0.95 & $(0, 1]$  & 1.00 & 34.0 \\
$p^{*}$ (tapered cosine) & 0.80 & $(0, 1]$  & 0.54 & 0.95 & $(0, 1]$  & 1.00 & 32.6 \\
$z$ (step)               & 1.28 & $(0, 10]$ & 2.32 & ---  & ---       & ---  & 43.0 \\
$z$ (triangle)           & 1.28 & $(0, 10]$ & 1.28 & 1.96 & $(0, 10]$ & 3.43 & 49.7 \\
$z$ (tapered cosine)     & 1.28 & $(0, 10]$ & 0.41 & 1.96 & $(0, 10]$ & 3.75 & 51.3 \\
$r$ (step)               & 0.50 & $(0, 10]$ & 0.92 & ---  & ---       & ---  & 65.4 \\
$r$ (triangle)           & 0.50 & $(0, 10]$ & 0.87 & 1.00 & $(0, 10]$ & 0.87 & 66.0 \\
$r$ (tapered cosine)     & 0.50 & $(0, 10]$ & 1.06 & 1.00 & $(0, 10]$ & 1.07 & 67.1 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Assessing transformations using simulated
data}\label{assessing-transformations-using-simulated-data}

Three datasets with 100000 sequences each were created to assess
transformation to normality. For each sequence \(\mathbf{X}_i\),
\(n = \lceil 10^\gamma \rceil\) elements were randomly drawn, with
\(\gamma \sim U\left(1, 4\right)\), resulting in sequences with between
\(10\) and \(10000\) elements.

\begin{itemize}
\item
  A \textit{clean} dataset with each sequence drawn from
  \(\mathcal{N}(0, 1)\) and transformed using an inverse power
  transformation with randomly drawn transformation parameter
  \(\lambda \sim U\left(0.00, 2.00 \right)\).
  \(\left(\phi_{\text{BC}}^\lambda\right)^{-1}\) and
  \(\left(\phi_{\text{YJ}}^\lambda\right)^{-1}\) were used as inverse
  transformations for assessing Box-Cox and Yeo-Johnson transformations,
  respectively. Prior to inverse transformation, sequences for Box-Cox
  transformations were shifted into the positive domain, with minimum
  value \(1\).
\item
  A \textit{dirty} dataset with each sequence drawn from
  \(\mathcal{AGN}\left(\mu = 0, \sigma = 1/\sqrt{2}, \alpha, \beta \right)\),
  with randomly drawn skewness parameter
  \(\alpha \sim U\left(0.01, 0.99\right)\) and shape parameter
  \(\beta \sim U\left(1.00, 5.00 \right)\).
\item
  A \textit{shifted} dataset with each sequence drawn from
  \(\mathcal{AGN}\left(\mu = 100, \sigma = 10^{-3} \cdot 1/\sqrt{2} , \alpha, \beta \right)\),
  with randomly drawn skewness parameter
  \(\alpha \sim U\left(0.01, 0.99\right)\) and shape parameter
  \(\beta \sim U\left(1.00, 5.00 \right)\).
\end{itemize}

A dataset with outliers was created for each of the above datasets by
replacing 10 percent of elements in each sequence, as described earlier.

In addition to no power transformation and location- and scale-invariant
power transformations, conventional and Raymaekers and Rousseeuw's
robust adaptation \citep{Raymaekers2024-zf} were assessed. For the
latter two methods, normalisation before standardisation using was
additionally assessed using the following two methods:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  z-standardisation:
  \(x^{\prime}_{i} = \left(x_i - \mu \right) / \sigma\), with \(\mu\)
  and \(\sigma\) the mean and standard deviation of sequence
  \(\mathbf{X}\).
\item
  robust scaling:
  \(x^{\prime}_{i} = \left(x_i - \text{median}\left(\mathbf{X}\right) \right) / Q_n\left(\mathbf{X}\right)\),
  with \(Q_n\) representing the robust \(Q_n\) scale estimator of
  \(\mathbf{X}\) \citep{Croux1992-zz, Rousseeuw1993-it}.
\end{enumerate}

This results in nine types of power transformation. Transformation
parameters were optimised for each sequence. Subsequently the sum of
residual errors and sum of residual errors of the central portion
(\(\kappa = 0.80\)) were computed for each sequence after
transformation. Then, each method was ranked according to the sum of
residual errors (data without outliers) or the sum of residual errors of
the central portion (data with outliers). The average rank of each
transformation method was computed over all 100000 sequences in each
dataset. Average ranks for Yeo-Johnson transformations are shown in
Table \ref{tab:comparison_methods_simulation_yeo_johnson}. Location and
shift-invariant Yeo-Johnson transformation ranked best for all datasets
without outliers. The robust variant ranked best in dirty and shifted
datasets with outliers, but not in the clean dataset. Results for
Box-Cox transformations are shown in Table
\ref{tab:comparison_methods_simulations_box_cox}.

\begin{table}
\begin{center}
\caption{
Comparison of average rank between Yeo-Johnson transformation methods based on either residual error (without outliers) or residual error of the central portion
(with outliers; $\kappa = 0.80$) over 3 datasets with 100000 sequences each. The clean dataset consists of sequences derived through inverse Yeo-Johnson transformation
of data sampled from a standard normal distribution. The dirty dataset contains sequences sampled from asymmetric generalised normal distributions, centred at 0.
The shifted dataset also contains sequences sampled from asymmetric generalised normal distributions, but centred at 100, and scaled by 0.001.
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling.
A rank of 1 is the best and a rank of 9 the worst. For each dataset, the best ranking transformation is marked in bold.
}
\label{tab:comparison_methods_simulations_yeo_johnson}
\begin{tabular}{l | l r r r r r r}

\toprule
& dataset: & \multicolumn{2}{c}{clean} & \multicolumn{2}{c}{dirty} & \multicolumn{2}{c}{shifted} \\
transformation & outliers: & no & yes & no & yes & no & yes \\

\midrule

none                                  & &         8.52  &         7.62  &         7.35  &         6.17  &         7.46  &         6.62 \\
conventional                          & &         3.82  &         5.78  &         3.86  &         7.08  &         6.51  &         5.95 \\
conventional (z-score norm.)          & &         4.70  &         5.75  &         6.44  &         5.07  &         5.48  &         4.93 \\
conventional (robust scaling)         & &         3.83  &         6.61  &         5.36  &         5.72  &         4.41  &         5.59 \\
Raymaekers-Rousseeuw                  & &         4.64  &         3.22  &         4.01  &         4.11  &         6.28  &         5.67 \\
Raymaekers-Rousseeuw (z-score norm.)  & &         5.24  & \textbf{2.66} &         6.25  &         3.62  &         5.29  &         3.50 \\
Raymaekers-Rousseeuw (robust scaling) & &         4.70  &         2.99  &         5.22  &         3.69  &         4.27  &         3.57 \\
invariant                             & & \textbf{3.11} &         6.74  & \textbf{3.24} &         6.13  & \textbf{2.63} &         6.03 \\
robust invariant                      & &         6.44  &         3.64  &         3.27  & \textbf{3.40} &         2.65  & \textbf{3.14} \\

\bottomrule
\end{tabular}
\end{center}
\end{table}

\section{Experimental Results}\label{experimental-results}

\subsection{Invariance}\label{invariance}

Location- and scale-invariant power transformations are intended to
yield improved transformations to normality in the presence of large
shifts in location, distributions that due to location and scale are not
centered near zero, or both. Earlier, we assessed these transformations
using simulated data. In the following, they are evaluated using
examples from real datasets. We focus on the Yeo-Johnson transformation
because of its ability to handle features with negative values. Results
for Box-Cox transformations are shown in Appendix D.

\begin{figure}

{\centering \includegraphics{manuscript_files/figure-latex/experimental-results-invariance-1} 

}

\caption{Quantile-quantile plots for several datasets: age of patients with lung cancer (top row); penguin body mass (middle row); and latitude coordinates of houses sold in Ames, Iowa (bottom row). Multiple quantile-quantile plots are shown: for the original feature (left column); the feature transformed using the conventional Yeo-Johnson transformation and Raymaekers and Rousseeuw's robust adaptation (middle column); and the feature transformed using the non-robust and robust location- and-scale invariant Yeo-Johnson transformations (right column).}\label{fig:experimental-results-invariance}
\end{figure}

\subsubsection{Age of patients with lung
cancer}\label{age-of-patients-with-lung-cancer}

A common feature in health-related datasets is age. Here we use data on
228 patients with lung cancer that was collected and published by
Loprinzi et al. \citep{Loprinzi1994-cd}. The age in the cohort was
\(62.4 \pm 9.1\) (mean ± standard deviation) years. Applying
conventional and invariant Yeo-Johnson transformations to patient age
yielded the following results, see Figure
\ref{fig:experimental-results-invariance}: no transformation
(\(\sum r_i = 16.5\), \(p = 0.69\)); conventional transformation
(\(\lambda = 2.0\), \(\sum r_i = 11.5\), \(\mu_{YJ} = 1.8 \cdot 10^3\),
\(\sigma_{YJ} = 0.5 \cdot 10^3\), \(p = 0.96\)); Raymaekers and
Rousseeuw's robust adaptation (\(\lambda = 2.0\), \(\sum r_i = 11.5\),
\(\mu_{YJ} = 1.8 \cdot 10^3\), \(\sigma_{YJ} = 0.5 \cdot 10^3\),
\(p = 0.96\)); location- and scale-invariant transformation
(\(\lambda = 1.3\), \(\sum r_i = 8.8\), \(\mu_{YJ} = -1.2\),
\(\sigma_{YJ} = 1.1\), \(p = 0.98\)); and robust location- and
scale-invariant transformation (\(\lambda = 1.3\), \(\sum r_i = 9.3\),
\(\mu_{YJ} = -1.0\), \(\sigma_{YJ} = 1.1\), \(p = 0.93\)).

Location- and scale-invariant transformation led to a lower overall
residual error, indicating a better transformation to normality.
Conventional transformations inflated the mean \(\mu_{YJ}\) and standard
deviation \(\sigma_{YJ}\) of the age feature after transformation. The
empirical central normality test did not detect any statistically
significant deviations from central normality for any transformation
(all \(p \geq 0.93\)).

\subsubsection{Penguin body mass}\label{penguin-body-mass}

Gorman, Williams and Fraser recorded body mass (in grams) of 342
penguins of three different species \citep{Gorman2014-eo}. The body mass
was \((4.2 \pm 0.8) \cdot 10^3\) (mean ± standard deviation) grams, and
not centrally normal (\(p < 0.001\)). Applying conventional and
invariant Yeo-Johnson transformations to body mass yielded the following
results, see Figure \ref{fig:experimental-results-invariance}: no
transformation (residual sum \(\sum r_i = 48.0\), \(p < 0.001\));
conventional transformation (\(\lambda = -0.5\), \(\sum r_i = 32.2\),
\(\mu_{YJ} = 2.1\), \(\sigma_{YJ} = 4 \cdot 10^{-3}\), \(p = 0.10\));
Raymaekers and Rousseeuw's robust adaptation (\(\lambda = -0.5\),
\(\sum r_i = 32.2\), \(\mu_{YJ} = 2.1\),
\(\sigma_{YJ} = 4 \cdot 10^{-3}\), \(p = 0.10\)); location- and
scale-invariant transformation (\(\lambda = 0.5\), \(\sum r_i = 26.8\),
\(\mu_{YJ} = 0.9\), \(\sigma_{YJ} = 0.9\), \(p = 0.28\)); and robust
location- and scale-invariant transformation (\(\lambda = 0.3\),
\(\sum r_i = 22.0\), \(\mu_{YJ} = 0.7\), \(\sigma_{YJ} = 0.9\),
\(p = 0.69\)).

Location- and scale-invariant transformation produced a lower overall
residual errors, indicating a better transformation. Moreover,
conventional transformations led to low standard deviation
\(\sigma_{YJ}\) of the body mass feature after transformation. The
empirical central normality test did not detect any statistically
significant deviations from central normality for any transformation
(all \(p \geq 0.10\)).

\subsubsection{Latitude in the Ames housing
dataset}\label{latitude-in-the-ames-housing-dataset}

Geospatial datasets usually contain coordinates. The Ames housing
dataset contains data on 2930 properties that were sold between 2006 and
2010 \citep{De-Cock2011-jf}, including their geospatial coordinates. The
latitude was \(42.03 \pm 0.02)\) (mean ± standard deviation). Applying
conventional and invariant Yeo-Johnson transformations to latitude
yielded the following results, see Figure
\ref{fig:experimental-results-invariance}: no transformation (residual
sum \(\sum r_i = 328\), \(p < 0.001\)); conventional transformation
(\(\lambda = 62.1\), \(\sum r_i = 319\),
\(\mu_{YJ} = 4.8 \cdot 10^{99}\), \(\sigma_{YJ} = 0.1 \cdot 10^{99}\),
\(p < 0.001\)); Raymaekers and Rousseeuw's robust adaptation
(\(\lambda = 95.4\), \(\sum r_i = 315\),
\(\mu_{YJ} = 6.4 \cdot 10^{153}\), \(\sigma_{YJ} = 0.3 \cdot 10^{153}\),
\(p < 0.001\)); location- and scale-invariant transformation
(\(\lambda = 1.5\), \(\sum r_i = 326\), \(\mu_{YJ} = -1.2\),
\(\sigma_{YJ} = 0.8\), \(p < 0.001\)); and robust location- and
scale-invariant transformation (\(\lambda = 1.1\), \(\sum r_i = 308\),
\(\mu_{YJ} = -1.3\), \(\sigma_{YJ} = 1.2\), \(p < 0.001\)).

Every transformation reduced the residual sum. None of the
transformations yielded a centrally normal distribution. Conventional
transformations had high values for the \(\lambda\) parameter, which
could lead to numerical issues.

\subsection{Robustness against
outliers}\label{robustness-against-outliers}

We previously simulated data to assess invariant power transformations
and their robustness against outliers. Here, we assess invariant power
transformations in real data with outliers.

\begin{figure}

{\centering \includegraphics{manuscript_files/figure-latex/experimental-results-outlier-robustness-1} 

}

\caption{Quantile-quantile plots for two datasets with outliers: vehicle fuel consumption (top row), where outliers are related to highly fuel-efficient vehicles; and maximum arterial wall thickness in patients with ischemic stroke (bottom row). Multiple quantile-quantile plots are shown: for the original feature (left column); the feature transformed using the conventional Yeo-Johnson transformation and Raymaekers and Rousseeuw's robust adaptation (middle column); and the feature transformed using the non-robust and robust location- and-scale invariant Yeo-Johnson transformations (right column). Samples with observed quantiles below $-3.0$ or above $3.0$ are indicated by crosses.}\label{fig:experimental-results-outlier-robustness}
\end{figure}

\subsubsection{Fuel efficiency in the Top Gear
dataset}\label{fuel-efficiency-in-the-top-gear-dataset}

The Top Gear dataset contains data on 297 vehicles that appeared on the
BBC television show \emph{Top Gear} \citep{Alfons2021-kc}. Within this
dataset, the fuel consumption feature contains outliers due to highly
fuel-efficient vehicles. Applying conventional and invariant Yeo-Johnson
transformations to the fuel consumption feature yielded the following
results, see Figure \ref{fig:experimental-results-outlier-robustness}:
no transformation (residual sum \(\sum r_i = 54\), \(p=0.72\));
conventional transformation (\(\lambda = -0.1\), \(\sum r_i = 55\),
\(\mu_{YJ} = 3.0\), \(\sigma_{YJ} = 0.3\), \(p < 0.001\)); Raymaekers
and Rousseeuw's robust adaptation (\(\lambda = 0.8\), \(\sum r_i = 48\),
\(\mu_{YJ} = 29\), \(\sigma_{YJ} = 15\), \(p=0.37\)); location- and
scale-invariant transformation (\(\lambda = -1.3\), \(\sum r_i = 44\),
\(\mu_{YJ} = 0.5\), \(\sigma_{YJ} = 0.1\), \(p < 0.001\)); and robust
location- and scale-invariant transformation (\(\lambda = 1.0\),
\(\sum r_i = 56\), \(\mu_{YJ} = 1.7\), \(\sigma_{YJ} = 1.0\),
\(p=0.76\)).

Outliers cause non-robust transformations to fail to transform the data
to a centrally normal distribution (empirical central normality test
\(p < 0.001\) for conventional and invariant transformations). Robust
transformations produce distributions that are centrally normal
(empirical central normality test \(p > 0.05\)).

\subsubsection{Maximum arterial wall thickness in an ischemic stroke
dataset}\label{maximum-arterial-wall-thickness-in-an-ischemic-stroke-dataset}

The ischemic stroke dataset contains historic data from 126 patients
with risk at ischemic stroke \citep{Kuhn2019-kt}. These patients
underwent Computed Tomography Angiography to characterize the carotid
artery blockages. Angiography imaging was then assessed, and various
characteristics related to the blood vessels and the disease are
measured. The maximum arterial wall thickness feature contains several
instances with outlier values. Applying conventional and invariant
Yeo-Johnson transformations to this feature yielded the following
results, see Figure \ref{fig:experimental-results-outlier-robustness}:
no transformation (residual sum \(\sum r_i = 110\), \(p=0.83\));
conventional transformation (\(\lambda = -0.7\), \(\sum r_i = 30\),
\(\mu_{YJ} = 1.0\), \(\sigma_{YJ} = 0.1\), \(p=0.003\)); Raymaekers and
Rousseeuw's robust adaptation (\(\lambda = 1.1\), \(\sum r_i = 136\),
\(\mu_{YJ} = 7.2\), \(\sigma_{YJ} = 14.3\), \(p=0.88\)); location- and
scale-invariant transformation (\(\lambda = 0.2\), \(\sum r_i = 12\),
\(\mu_{YJ} = -11.8\), \(\sigma_{YJ} = 6.9\), \(p=0.15\)); and robust
location- and scale-invariant transformation (\(\lambda = -0.3\),
\(\sum r_i = 30\), \(\mu_{YJ} = 0.8\), \(\sigma_{YJ} = 0.2\),
\(p=0.18\)).

The conventional non-robust transformation failed to produce a centrally
normal distribution (empirical central normality test \(p=0.003\)).
Robust transformations produce distributions that are centrally normal
(empirical central normality test \(p > 0.05\)).

\subsection{Integration into end-to-end machine
learning}\label{integration-into-end-to-end-machine-learning}

We used 231 datasets containing at least one numeric feature from the
Penn Machine Learning Benchmarks collection \citep{Romano2022-gq}. In
this collection, 114 datasets correspond to regression tasks and 117
datasets to classification tasks. Using the familiar auto-machine
learning library \citep{Zwanenburg2021-so} (version 1.5.0), each dataset
was used to train a model for each of 32 process configurations. Each
process configuration specifies the learner (generalised linear model,
L1-regularised linear models (Lasso), gradient boosted linear model, or
random forest), transformation method (none, conventional Yeo-Johnson,
robust invariant Yeo-Johnson, robust invariant Yeo-Johnson with
empirical central normality test (rejecting transformations with
\(p \leq 0.01\)), and normalisation method (none,
\(z\)-standardisation), yielding 32 distinct configurations. Before each
experiment, each dataset was randomly split into a training (70\%) and
holdout test (30\%) set five times. Thus, a total of 36960 models were
created. Each model was then evaluated using the holdout test set using
one of two metrics, i.e.~the root relative squared error (RRSE) for
regression tasks and the area under the receiver operating
characteristic curve (AUC) for classification tasks.

For the purpose of assessing the effect of the difficulty of the task,
we computed the median performance score over all models for each
dataset and assigned one the following categories:

\begin{itemize}
\tightlist
\item
  very easy: \(\text{AUC} \geq 0.90\) or \(\text{RRSE} \leq 0.10\) (57
  datasets)
\item
  easy: \(0.90 > \text{AUC} \geq 0.80\) or
  \(0.30 \geq \text{RRSE} > 0.10\) (40 datasets)
\item
  intermediate: \(0.80 > \text{AUC} \geq 0.70\) or
  \(0.60 \geq \text{RRSE} > 0.30\) (52 datasets)
\item
  difficult: \(0.70 > \text{AUC} \geq 0.60\) or
  \(0.80 \geq \text{RRSE} > 0.60\) (33 datasets)
\item
  very difficult: \(0.60 > \text{AUC} \geq 0.50\) or
  \(1.00 \geq \text{RRSE} > 0.80\) (48 datasets)
\item
  unsolvable: \(\text{AUC} < 0.50\) or \(\text{RRSE} > 1.00\) (1
  dataset)
\end{itemize}

To remove the effect of the dataset, and allow for comparing metrics, we
ranked all performance scores for each dataset so that a higher rank
corresponds to better performance. Experiments yielding the same score
received the same, average, rank. Subsequently ranks were normalised to
the \([0.0, 1.0]\) range.

Significant differences exist between process configurations (Friedman
test: \(p < 10^{-8})\).

Considering single process parameters, the choice of learner (Friedman
test: \(p < 10^{-8})\)), normalisation method (Wilcoxon signed rank
test: \(p = 4 \cdot 10^{-8}\)), and transformation method (Friedman
test: \(p = 0.007\)), all had a significant impact (at \(p = 0.05\)).

To estimate the marginal effects of process parameters, including
transformation method, we first fit a regression random forest (ranger
package \citep{Wright2017-rf} version 0.16.0): 2000 trees, node size 2,
other hyperparameters default) with process parameters and task
difficulty as predictors and normalised rank as response variable. The
estimated marginal effects are shown in Figure
\ref{fig:marginal-effect-plot}. On the scale of normalised ranks
(\([0.0, 1.0]\)), only the random forest performed better than the
average of all learners (rank difference \(0.173\)). The marginal
improvement in performance from z-standardisation was \(0.011\).
Transformation methods had the following marginal effects: \(-0.001\)
for using conventional Yeo-Johnson transformation instead of no
transformation; \(-0.012\) for using robust invariant Yeo-Johnson
transformation instead of no transformation; \(-0.012\) for using robust
invariant Yeo-Johnson transformation with empirical central normality
test instead of no transformation; \(-0.012\) for using robust invariant
Yeo-Johnson transformation instead of conventional Yeo-Johnson
transformation; and \(-0.012\) for using using robust invariant
Yeo-Johnson transformation with empirical central normality test instead
of conventional Yeo-Johnson transformation.

\begin{figure}

{\centering \includegraphics{manuscript_files/figure-latex/marginal-effect-plot-1} 

}

\caption{Estimated marginal effect of learners, normalisation and transformation methods on ranked model performance scores in 36960 machine learning experiments on 231 datasets. The four top-left panels shows the marginal effect of learners vs. the average, i.e. generalised linear models (GLM), L1-regularised LM (Lasso), gradient-boosted LM and random forest. Random forests outperform the average. The top-right panel shows the marginal effect of feature normalisation methods, i.e. no normalisation and z-standardisation. z-standardisation is generally beneficial, but the estimated effect is marginal. The bottom panel shows the marginal effects of different transformation methods, split by normalisation method. The estimated effects are similar in size to the effect of normalisation and marginal. Note that the ranges of the $x$-axes of the three main panels differ. ECNT: empirical central normality test.}\label{fig:marginal-effect-plot}
\end{figure}

\section{Discussion}\label{discussion}

In their work on power transformation, Box and Cox already mention
transformation with a shift parameter, but preferred the version in Eq.
\ref{eqn:box-cox-original} for the theoretical analysis in their paper
\citep{Box1964-mz}, which subsequently became the convention. Yeo and
Johnson's power transformation lacks a shift parameter altogether
\citep{Yeo2000-vw}. We showed that these power transformations are
sensitive to location and scale of data distributions. To mitigate this
issue, we defined location- and scale-invariant variants of the Box-Cox
and Yeo-Johnson transformations. We furthermore assessed methods for
making these transformations robust to outliers, and devised an
empirical test for central normality.

Robust location- and scale-invariant transformations are a suitable
replacement for their conventional counterparts. They demonstrated
robustness against outliers and prevent inaccurate transformations and
potential numerical issues due to location and scale of the distribution
of a feature. This is particularly relevant for automated data
processing, where such issues may go unnoticed.

In simulation, robust location- and scale-invariant transformations
ranked best in datasets with outliers, and ranked highly in datasets
without outliers, except for features without outliers that were
directly sampled from strictly normal distributions. In real-world
examples, robust location-and scale-invariant transformations achieved
central normality when the non-robust variant could not.

However, in a machine learning experiment of 231 real-world datasets
that contained at least one numeric feature, we did not find a
meaningful benefit -- nor detriment -- to model performance for
location- and scale-invariant power transformations. One reason may be
that numeric features with large location shifts (\(|\mu| > 1000.0\))
were uncommon. Of the 4886 numeric features in the 231 datasets, 266
(5\%) features in 34 datasets had large location shifts, of which 200
appeared in just 2 datasets. For the latter two datasets, the
transformation method did not show significant difference between groups
(Friedman test; \(p > 0.05\)).

Location- and scale-invariant transformations are realised by
simultaneously optimising three parameters, i.e.~transformation
parameter \(\lambda\), shift parameter \(x_0\) and scale parameter
\(s\). We derived the log-likelihood function to facilitate optimisation
using MLE. Alternatively, standardisation of a numeric feature (e.g.,
through subtracting its median value and division by its interquartile
range) prior to conventional power transformations can achieve a similar
effect in reducing sensitivity to the feature's location and scale.
While this alternative helps prevent these issues -- provided that
standardisation does not lead to negative values for Box-Cox
transformations -- location- and scale-invariant transformations are
able to achieve better transformations to normality, as demonstrated by
lower residual errors in simulation experiments.

We assessed several weighting methods to achieve robust power
transformations. Robust power transformations should satisfy two
conflicting aims: they should minimise residual errors after
transformation to central normality, and minimise the overall effect of
outliers on estimation of transformation parameter \(\lambda\). These
aims are reflected in the composite loss used to optimise weighting
parameters. For Yeo-Johnson transformation, empirical probabilities with
tapered cosine weighting resulted in the lowest loss. The optimal
parameters led to weights that gradually decline towards the tails of
empirical probabilities, i.e.~elements with low and high values receive
less or no weight. Similarly, for Box-Cox transformation, empirical
probabilities with step weighting resulted in the lowest loss. In this
case the central 80\% of the elements are used, and the remaining 10\%
in each tail are ignored. Methods that relied on the z-score of the
transformed feature or the residual error yielded worse loss than the
non-robust method or those based on empirical probabilities.
Underperformance of these weighting methods could be explained by their
reliance on transformed feature values for setting weights.
Consequently, their weights change at each iteration in the MLE
optimisation process. This increases local variance in the
log-likelihood function and creates local optima that the optimiser may
not handle well. Methods that relied on the empirical probability did
not suffer from this issue, as weights remained fixed during MLE.

We introduced an empirical test for central normality to assess whether
sequences deviate from normality in a way that might require closer
inspection prior to further processing. The empirical central normality
test differs from other tests for normality, such as the Shapiro-Wilk
test \citep{Shapiro1965-zd}, as it assesses normality of the central
portion of a feature, instead of the entire feature. Compared to the
Shapiro-Wilk test, the empirical central normality test remains
consistent when a feature was not sampled from a normal distribution,
such as a central normal distribution with \(\kappa < 1.0\). This
enables assessing reasonable normality of (transformed) features in the
presence of outliers.

This work has the following limitation: We observed several numerical
stability issues for optimisation criteria other than MLE (Appendix B).
These appear in regions where transformation parameters would lead to
very large or small numbers when using conventional power
transformations. For MLE stability issues were not observed.

\section{Conclusion}\label{conclusion}

Compared to their conventional versions, robust location- and
scale-invariant Box-Cox and Yeo-Johnson transformations reduce
sensitivity to outliers and the location and scale of features. An
empirical central normality test can assess the quality of
transformation of features to normal distributions. The combination of
both facilitate the use of power transformations in automated data
analysis workflows.

\section{Data and code availability}\label{data-and-code-availability}

Location- and scale-invariant power transformations were implemented in
the \texttt{power.transform} package for R, which is available from
\href{https://github.com/oncoray/power.transform}{GitHub} and the
\href{https://cran.r-project.org/package=power.transform}{CRAN
repository}. The manuscript was created using R Markdown and is likewise
available from the \texttt{power.transform} GitHub repository. Data and
results for the machine learning experiment are separately available
from \href{https://doi.org/10.5281/zenodo.14986689}{Zenodo}.

\renewcommand\refname{References}
\bibliography{refs.bib}

\end{document}
