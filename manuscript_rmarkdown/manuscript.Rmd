---
title: "Location and Scale-Invariant Power Transformations for Transforming Data to
  Normality"
author: "Alex Zwanenburg, Steffen LÃ¶ck"
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: lualatex
    keep_tex: true
papersize: a4
bibliography: refs.bib
header-includes:
- \usepackage{amsmath}
- \usepackage{amsthm}
- \usepackage{booktabs}
- \DeclareMathOperator*{\argmax}{argmax}
- \DeclareMathOperator*{\argmin}{argmin}
- \DeclareMathOperator{\sgn}{sgn}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  dev = "cairo_pdf",
  echo = FALSE,
  eval = TRUE,
  fig.align = "center"
)

# Allow for defining figure captions within a chunk.
knitr::opts_knit$set(
  eval.after = "fig.cap")

require(power.transform)
require(ggplot2)
require(patchwork)
require(data.table)
require(latex2exp)
require(coro)
require(paletteer)
require(robustHD)
require(modeldata)

manuscript_dir <- file.path("C:/Users/alexz/Documents/GitHub/power.transform/manuscript_rmarkdown")
source(file.path(manuscript_dir, "simulations.R"))
source(file.path(manuscript_dir, "plots.R"))

# Base font size.
base_size <- 9

# Set general theme.
plot_theme <- ggplot2::theme_light(base_size=base_size)
plot_theme$plot.title$size <- ggplot2::rel(1.0)
plot_theme$plot.title$hjust <- 0.5
plot_theme$strip.background <- ggplot2::element_blank()
plot_theme$strip.text$colour <- NULL
plot_theme$plot.margin <- grid::unit(c(2, 2, 2, 2), "points")
plot_theme$legend.key.size <- grid::unit(0.3, 'lines')
plot_theme$plot.tag <- ggplot2::element_text(
  size = ggplot2::rel(1.0),
  hjust = 0.0,
  vjust = 0.0,
  inherit.blank = FALSE
)
plot_theme$plot.tag.position <- c(0.2, 1)

parse_mean_sd_latex <- function(mu, sigma, digits=2L){
  return(paste0(
    "$", round(mu, digits=digits), " \\pm ", round(sigma, digits=digits), "$"
  ))
}

```

# Abstract

Power transformations are used to stabilize variance and achieve normality of features, especially in methods assuming normal distributions such as ANOVA and linear discriminant analysis.
However, the commonly used Box-Cox and Yeo-Johnson power transformation methods are sensitive to the location, scale, and presence of outliers in the data.
Here we present location- and scale-invariant Box-Cox and Yeo-Johnson transformations to mitigate these issues.
We derive maximum likelihood estimation criteria for optimizing transformation parameters and propose robust adaptations that reduce the influence of outliers.
We also introduce an empirical test for assessing central normality of transformed features.
In simulations and real-world datasets, robust location- and scale-invariant transformations outperform conventional variants, resulting in better transformations to central normality.
In a machine learning experiment with 231 datasets with numerical features, 
integrating robust location- and scale-invariant power transformations into an automated data processing and machine learning pipeline did not result in a meaningful improvement or detriment to model performance compared to conventional variants.
In conclusion, robust location- and scale-invariant power transformations can replace conventional variants.

# Introduction

Many statistical and some machine learning methods assume normality of the underlying data, e.g. analysis of variance and linear discriminant analysis.
However, numerical features in datasets may strongly deviate from normal distributions, e.g. by being skewed.
Power transformations aim to stabilise variance and improve normality of such features [@Bartlett1947-rx; @Tukey1957-rt].
The two most commonly used transformations are that of @Box1964-mz and @Yeo2000-vw.
The Box-Cox transformation of a feature value $x_i$ of feature $\mathbf{X}=\left\{x_1, x_2, \ldots, x_n \right\}$ under the transformation parameter $\lambda$ is defined as:

```{=latex}
\begin{equation}
\label{eqn:box-cox-original}
\phi_{\text{BC}}^\lambda (x_i) = 
\begin{cases}
\left(x_i^\lambda - 1 \right) / \lambda & \text{if } \lambda \neq 0\\
\log(x_i) & \text{if } \lambda = 0
\end{cases}
\end{equation}
```
One limitation of the Box-Cox transformation is that it is only defined for $x_i > 0$. In contrast, the Yeo-Johnson transformation under the transformation parameter $\lambda$ is defined for any $x_i \in \mathbb{R}$:

```{=latex}
\begin{equation}
\label{eqn:yeo-johnson-original}
\phi_{\text{YJ}}^\lambda (x_i) = 
\begin{cases}
\left( \left( 1 + x_i \right)^\lambda - 1\right) / \lambda & \text{if } \lambda \neq 0 \text{ and } x_i \geq 0\\
\log(1 + x_i) & \text{if } \lambda = 0 \text{ and } x_i \geq 0\\
-\left( \left( 1 - x_i\right)^{2 - \lambda} - 1 \right) / \left(2 - \lambda \right) & \text{if } \lambda \neq 2 \text{ and } x_i < 0\\
-\log(1 - x_i) & \text{if } \lambda = 2 \text{ and } x_i < 0
\end{cases}
\end{equation}
```
The $\lambda$-parameter is typically optimised using maximum likelihood estimation under the assumption that the transformed feature is normally distributed.
As noted by Raymaekers and Rousseeuw, this approach is sensitive to outliers, and robust versions of Box-Cox and Yeo-Johnson transformations were devised [@Raymaekers2024-zf].

Applying a power transformation does not guarantee that transformed features are normally distributed.
Depending on location and scale of a feature and the presence of outliers, power transformations may decrease normality, as shown in Figure \ref{fig:decreased-normality}.
If normality of the transformed feature is not checked, e.g. in automated power transformation in machine learning workflows, several issues may arise.
For example, statistical tests such as ANOVA may produce incorrect results due to violation of the normality assumption.
Likewise, machine learning methods that assume normality of input features may suffer a decrease in performance.
Moreover, large negative or positive $\lambda$-parameters may lead to numeric issues in any subsequent computations.

Statistical tests for normality, such as the Shapiro-Wilk test [@Shapiro1965-zd], could be automatically applied to transformed features.
However, given sufficiently large sample sizes such tests can detect trivial deviations from normality, and may lead to rejection of sufficiently good power transformations.

```{r decreased-normality, echo=FALSE, fig.cap=cap, fig.height=2.5, warning=FALSE}
cap <- paste0(
  "Effect of location, scale and outliers on estimation of the Box-Cox and Yeo-Johnson transformation parameter $\\lambda$. ",
  "$10000$ samples were drawn from a normal distribution: ",
  "$\\mathcal{N}(\\mu, 1)$ for the shift dataset, $\\mathcal{N}(10, \\sigma)$ for the scale dataset and $\\mathcal{N}(0, 1)$ for the outlier dataset. ",
  "Additionally, an outlier with value $d$ was added to the outlier dataset. ",
  "Since samples are drawn from a normal distribution, a transformation parameter of $\\lambda = 1$ is expected. ",
  "However, a large shift in location, a scale that is small compared to the location, or presence of large outliers ",
  "lead to incorrectly estimated transformation parameter values."
)

.plot_reduced_normality(plot_theme = plot_theme, manuscript_dir = manuscript_dir)
```

To address these issues, we make the following contributions:

-   We devise location- and scale-invariant versions of the Box-Cox and Yeo-Johnson transformation, including versions robust to outliers.

-   We derive the maximum likelihood criterion for location- and scale-invariant Box-Cox and Yeo-Johnson transformations to allow for optimising transformation parameters.

-   We define an empirical central normality test for detecting cases where power transformations fail to yield an approximately normally distributed transformed feature.

-   We assess the effect of power transformations on the performance of machine learning models.

# Theory

In this section, we will first introduce location- and scale-invariant versions of the Box-Cox and Yeo-Johnson transformations.
Subsequently, we define weighted location- and scale-invariant transformations and weighting methods for robust transformations.
We then define the quantile function for asymmetric generalised normal distributions to enable random sampling.
Finally, we define the overall framework for the empirical central normality test.

## Location- and scale-invariant power transformation

Box-Cox and Yeo-Johnson transformations are modified by introducing shift parameter $x_0$ and scale parameter $s$ into equations \ref{eqn:box-cox-original} and \ref{eqn:yeo-johnson-original}.
The location- and scale-invariant Box-Cox transformation of a feature value $x_i$ of feature $\mathbf{X}$ under transformation parameter $\lambda$, shift parameter $x_0$ and scale parameter $s$ is then:

```{=latex}
\begin{equation}
\label{eqn:box-cox-invariant}
\phi_{\text{BC}}^{\lambda, x_0, s} (x_i) = 
\begin{cases}
\left( \left(\frac{x_i - x_0}{s} \right)^\lambda - 1 \right) / \lambda & \text{if } \lambda \neq 0\\
\log\left[\frac{x_i - x_0}{s}\right] & \text{if } \lambda = 0
\end{cases}
\end{equation}
```
where $x_i - x_0 > 0$. Likewise, the location- and scale-invariant Yeo-Johnson transformation of a feature value $x_i$ under transformation parameter $\lambda$, shift parameter $x_0$ and scale parameter $s$ is:

```{=latex}
\begin{equation}
\label{eqn:yeo-johnson-invariant}
\phi_{\text{YJ}}^{\lambda, x_0, s} (x_i) = 
\begin{cases}
\left( \left( 1 + \frac{x_i - x_0}{s}\right)^\lambda - 1\right) / \lambda & \text{if } \lambda \neq 0 \text{ and } x_i - x_0 \geq 0\\
\log\left[1 + \frac{x_i - x_0}{s}\right] & \text{if } \lambda = 0 \text{ and } x_i - x_0 \geq 0\\
-\left( \left( 1 - \frac{x_i - x_0}{s}\right)^{2 - \lambda} - 1 \right) / \left(2 - \lambda \right) & \text{if } \lambda \neq 2 \text{ and } x_i - x_0 < 0\\
-\log\left[1 - \frac{x_i - x_0}{s}\right] & \text{if } \lambda = 2 \text{ and } x_i - x_0 < 0
\end{cases}
\end{equation}
```

For both invariant transformations, $\lambda$, $x_0$ and $s$ parameters can be obtained by maximising the log-likelihood function, i.e. using maximum likelihood estimation (MLE).
A full derivation of the log-likelihood function for both transformations is shown in Appendix A.
The location- and scale-invariant Box-Cox log-likelihood function is:

```{=latex}
\begin{equation}
\label{eqn:box-cox-invariant-log-likelihood}
\begin{split}
\mathcal{l}_{\text{BC}}^{\lambda, x_0, s} = & -\frac{n}{2} \log \left[2 \pi \sigma^2 \right] -\frac{1}{2 \sigma^2} \sum_{i=1}^n \left( \phi_{BC}^{\lambda, x_0, s}(x_i) - \mu \right)^2 \\
& -n \lambda \log s + \left( \lambda - 1 \right) \sum_{i=1}^n \log \left[ x_i - x_0 \right]
\end{split}
\end{equation}
```
subject to $x_i - x_0 > 0$.
$\mu$ and $\sigma^2$ are the mean and variance of the Box-Cox transformed feature $\phi_{\text{BC}}^{\lambda, x_0, s} (\mathbf{X})$, respectively.
Similarly, the location- and scale-invariant Yeo-Johnson log-likelihood function is:

```{=latex}
\begin{equation}
\label{eqn:yeo-johnson-invariant-log-likelihood}
\begin{split}
\mathcal{l}_{\text{YJ}}^{\lambda, x_0, s} = & -\frac{n}{2} \log\left[2 \pi \sigma^2\right] -\frac{1}{2 \sigma^2} \sum_{i=1}^n \left( \phi_{YJ}^{\lambda, x_0, s}(x_i) - \mu \right)^2 \\
& - n \log s + (\lambda - 1) \sum_{i=1}^n \sgn(x_i - x_0) \log \left[1 + \frac{|x_i - x_0|}{s} \right]
\end{split}
\end{equation}
```
where $\mu$ and $\sigma^2$ are the mean and variance of the Yeo-Johnson transformed feature $\phi_{\text{YJ}}^{\lambda, x_0, s} (\mathbf{X})$, respectively.


## Robust location- and scale-invariant power transformations

Real-world data may contain outliers, to which maximum likelihood estimation can be sensitive.
Their presence may lead to poor transformations to normality, as shown in Figure \ref{fig:decreased-normality}.
As indicated by @Raymaekers2024-zf, the general aim of power transformations should be to transform non-outlier data to normality, i.e. achieve *central normality*.
To achieve this, they devised an iterative procedure to find a robust estimate of the transformation parameter $\lambda$.
Briefly, this process requires identifying outliers in the data and weighting such instances during the optimisation process.
@Raymaekers2024-zf achieve this through weighted maximum likelihood estimation.
However, because this procedure iteratively estimates and updates $\lambda$, it can not be used here to simultaneously estimate $\lambda$, $x_0$ and $s$ for location- and scale-invariant power transformations.
Nonetheless, as a procedure, weighted MLE can be used for estimating the transformation, shift and scale parameters.

Here, weighted maximum likelihood estimation is based on equations \ref{eqn:box-cox-invariant-log-likelihood} and \ref{eqn:yeo-johnson-invariant-log-likelihood}.
Compared to @Raymaekers2024-zf, these log-likelihood functions includes additional terms to accommodate estimation of $x_0$ and $s$.
The weighted location- and scale-invariant Box-Cox log-likelihood function is:

```{=latex}
\begin{equation}
\label{eqn:box-cox-weighted-invariant-log-likelihood}
\begin{split}
\mathcal{l}_{\text{rBC}}^{\lambda, x_0, s} = & -\frac{1}{2} \left(\sum_{i=1}^n w_i \right) \log \left[ 2 \pi \sigma_w^2 \right] -\frac{1}{2 \sigma_w^2} \sum_{i=1}^n w_i \left( \phi_{\text{BC}}^{\lambda, x_0, s}(x_i) - \mu_w \right)^2 \\
& - \lambda \left( \sum_{i=1}^n w_i \right) \log s + \left( \lambda - 1 \right) \sum_{i=1}^n w_i \log \left[ x_i - x_0 \right]
\end{split}
\end{equation}
```
where $\mu_w$ and $\sigma^2_w$ are the weighted mean and weighted variance of the Box-Cox transformed feature $\phi_{\text{BC}}^{\lambda, x_0, s} (\mathbf{X})$:

```{=latex}
\begin{equation}
\sigma_w^2 = \frac{\sum_{i=1}^n w_i \left(\phi_{\text{BC}}^{\lambda, x_0, s} (x_i) - \mu_w \right)^2}{\sum_{i=1}^n w_i} \quad \text{with } \mu_w = \frac{\sum_{i=1}^n w_i \phi_{\text{BC}}^{\lambda, x_0, s} (x_i)} {\sum_{i=1}^n w_i}
\end{equation}
```

Analogously, the weighted location- and scale-invariant Yeo-Johnson log-likelihood function is:

```{=latex}
\begin{equation}
\label{eqn:yeo-johnson-weighted-invariant-log-likelihood}
\begin{split}
\mathcal{l}_{\text{rYJ}}^{\lambda, x_0, s} = & -\frac{1}{2} \left(\sum_{i=1}^n w_i \right) \log \left[ 2 \pi \sigma_w^2 \right] -\frac{1}{2 \sigma_w^2} \sum_{i=1}^n w_i \left( \phi_{\text{YJ}}^{\lambda, x_0, s}(x_i) - \mu_w \right)^2 \\
& - \left( \sum_{i=1}^n w_i \right) \log s + (\lambda - 1) \sum_{i=1}^n w_i \sgn(x_i - x_0) \log \left[1 + \frac{|x_i - x_0|}{s} \right]
\end{split}
\end{equation}
```
where $\mu_w$ and $\sigma^2_w$ are the weighted mean and weighted variance of the Yeo-Johnson transformed feature $\phi_{\text{YJ}}^{\lambda, x_0, s} (\mathbf{X})$:

```{=latex}
\begin{equation}
\sigma_w^2 = \frac{\sum_{i=1}^n w_i \left(\phi_{\text{YJ}}^{\lambda, x_0, s} (x_i) - \mu_w \right)^2}{\sum_{i=1}^n w_i} \quad \text{with } \mu_w = \frac{\sum_{i=1}^n w_i \phi_{\text{YJ}}^{\lambda, x_0, s} (x_i)} {\sum_{i=1}^n w_i}
\end{equation}
```

The weights $w_i$ in equations \ref{eqn:box-cox-weighted-invariant-log-likelihood} and \ref{eqn:yeo-johnson-weighted-invariant-log-likelihood} can be set using several weighting functions. 
Using $\dot{x}_i$ as an argument that will be defined later, we investigate three weighting functions:

- A step function, with $\delta_1 \geq 0$ as threshold parameter:
```{=latex}
\begin{equation}
w_i =
\begin{cases}
1 & \text{if } \left| \dot{x}_i \right| \leq \delta_1\\
0 & \text{if } \left| \dot{x}_i \right| > \delta_1
\end{cases}
\end{equation}
```

- A triangle function (or generalised Huber weight), with $\delta_1 \geq 0$ and $\delta_2 \geq \delta_1$ as threshold parameters:
```{=latex}
\begin{equation}
w_i =
\begin{cases}
1 & \text{if } \left| \dot{x}_i \right| < \delta_1\\
1 - \frac{\left| \dot{x}_i \right| - \delta_1}{\delta_2 - \delta_1} & \text{if } \delta_1 \leq \left| \dot{x}_i \right| \leq \delta_2 \\
0 & \text{if } \left| \dot{x}_i \right| > \delta_2
\end{cases}
\end{equation}
```

- A tapered cosine function [@Tukey1967-eb], with $\delta_1 \geq 0$ and $\delta_2 \geq \delta_1$ as threshold parameters:
```{=latex}
\begin{equation}
w_i =
\begin{cases}
1 & \text{if } \left| \dot{x}_i \right| < \delta_1\\
0.5 + 0.5 \cos\left(\pi \frac{\left| \dot{x}_i \right| - \delta_1}{\delta_2 - \delta_1} \right) & \text{if } \delta_1 \leq \left| \dot{x}_i \right| \leq \delta_2 \\
0 & \text{if } \left| \dot{x}_i \right| > \delta_2
\end{cases}
\end{equation}
```

All weighting functions share the characteristic that for $\left| \dot{x}_i \right|< \delta_1$, instances are fully weighted, i.e. when $\delta_1 > 0$ the weighting functions are symmetric window functions with a flat top.
The triangle and tapered cosine functions then gradually down-weight instances with $\delta_1 \leq \left| \dot{x}_i \right| \leq \delta_2$, and assign no weight to instances $\left| x_i \right| > \delta_2$.
Examples of these weighting function are shown in Figure \ref{fig:weighting-functions}.

```{r weighting-functions, warning=FALSE, message=FALSE, fig.cap=cap, fig.height=1.5}
cap <- paste0(
  "Weighting functions investigated in this study to make power transformations more robust against outliers. ",
  "In this example, the step function was parameterised with $\\delta_1 = 0.60$. ",
  "The triangle and tapered cosine functions were both parameterised with $\\delta_1 = 0.30$ and $\\delta_2 = 0.90$."
)

.plot_weighting_functions(plot_theme = plot_theme)
```

Each weighting function has an argument $\dot{x}$ that is related to the (transformed) feature in one of several ways:

- The weighting function uses empirical probabilities of the distribution of the original feature $\mathbf{X}$.
  After sorting $\mathbf{X}$ in ascending order, probabilities are determined as $p_i = \frac{i - 1/3}{n + 1/3}$, with $i = 1, 2, \ldots n$, with $n$ the number of instances of feature $\mathbf{X}$.
  Then $\dot{x}_i = p^{*}_i=2 \left( p_i - 0.5\right)$, so that argument is zero-centered.

- The weighting function uses the z-score of the transformed feature $\phi^{\lambda, x_0, s} (\mathbf{X})$.
  After @Raymaekers2024-zf, $z_i = \frac{\phi^{\lambda, x_0, s}(x_i) - \mu_M}{\sigma_M}$.
  Here, $\mu_M$ and $\sigma_M$ are robust Huber M-estimates of location and scale of the transformed feature $\phi^{\lambda, x_0, s} (\mathbf{X})$ [@Huber1981-su].
  Then $\dot{x}_i = z_i$.

- After sorting $\mathbf{X}$ in ascending order, the weighting function uses the residual error between the z-score of the transformed feature $\phi^{\lambda, x_0, s} (\mathbf{X})$ and the theoretical z-score from a standard normal distribution: $r_i =\left| \left( \phi^{\lambda, x_0, s}(x_i) - \mu_M)\right) / \sigma_M - F^{-1}_{\mathcal{N}}(p_i) \right|$, with $\mu_M$, $\sigma_M$ and $p_i$ as defined above.
  Then $\dot{x}_i = r_i$.


## Asymmetric generalised normal distributions

Modifications intended to make power transformations invariant to location and 
scale of a feature and methods to improve their robustness against outliers need 
to be assessed using data drawn from a range of different distributions. 
Since the power transformations are intended for use with unimodal distributions, 
the generalised normal distribution [@Subbotin1923-qk; @Nadarajah2005-xe] is a suitable option for simulating realistic feature distributions.
This distribution has the following probability density function $f_{\beta}$ for a value $x \in \mathbb{R}$:

\begin{definition}[Standard generalised normal distribution probability density function]
Let $x \in \mathcal{R}$.
Let $\beta > 0$ be a shape parameter.
Let $\Gamma$ be the gamma function.

Then the standard generalised normal distribution probability density function, without scale and location parameters, is:
\begin{equation}
f_{\beta}(x) = \frac{\beta}{2\Gamma\left(1 / \beta \right)} e^{-\left| x \right|^\beta}
\end{equation}
\end{definition}

For $\beta = 1$, the probability density function describes a Laplace distribution.
A normal distribution is found for $\beta=2$, and for large $\beta$,
the distribution approaches a uniform distribution. 

Realistic feature distributions may be skewed. 
Gijbels et al. describe a recipe for introducing skewness into the otherwise symmetric generalised normal distribution [@Gijbels2019-te], 
leading to the following probability density function:

\begin{definion}[Asymmetric generalised normal distribution probability density function]
Let $\alpha \in (0,1)$ be a skewness parameter.
Let $\mu \in \mathcal{R}$ be a location parameter.
Let $\sigma > 0$ be a shape parameter.

Then the probability density function of the asymmetric generalised normal distribution is:
\begin{equation}
f_{\alpha}(x; \mu, \sigma, \beta) = \frac{2 \alpha \left(1 - \alpha\right)}{\sigma}
\begin{cases}
f_{\beta}\left( \left(1 - \alpha \right) \frac{\left| x - \mu \right|}{\sigma} \right) & \text{, } x \leq \mu \\
f_{\beta}\left( \alpha \frac{\left| x - \mu \right|}{\sigma} \right) & \text{, } x > \mu
\end{cases}
\end{equation}
\end{definition}

$\alpha > 0.5$ creates a distribution with a negative skew, i.e. a left-skewed distribution.
A right-skewed distribution is created for $\alpha < 0.5$.
$f_{\alpha}$ thus describes the probability density function of an asymmetric generalised normal distribution,
which we will refer to here and parametrise as $\mathcal{AGN}\left(\mu, \sigma, \alpha, \beta \right)$.

We require a quantile function (or an approximation thereof) to draw random values
from an asymmetric generalised normal distribution using inverse transform sampling.
Gijbels et al. derived the quantile function $F_{\alpha}^{-1}(p)$, which incorporates
the quantile function of the symmetric generalised normal distribution derived by Griffin [@Griffin2018-bf]:

\begin{definition}[Asymmetric generalised normal distribution quantile function]
Let $p \in \left[0, 1\right]$ be a probability.
Let  $F_{\Gamma}^{-1}$ is the quantile function of the gamma distribution with shape $1 / \beta$, which can be numerically approximated. 
Let $F_{\beta}^{-1}$ be the quantile function of the generalised normal distribution:
\begin{equation}
F_{\beta}^{-1}(p) = \sgn\left(p - 0.5 \right) F_{\Gamma}^{-1}\left(2 \left|p - 0.5 \right|; 1 / \beta \right)
\end{equation}

Then the quantile function of the asymmetric generalised normal distribution is:
\begin{equation}
F_{\alpha}^{-1}(p; \mu, \sigma, \beta) =
\begin{cases}
\mu + \frac{\sigma}{1 - \alpha} F_{\beta}^{-1} \left( \frac{p}{2 \alpha}\right) & \text{, } p \leq \alpha \\
\mu + \frac{\sigma}{\alpha} F_{\beta}^{-1} \left( \frac{1 + p - 2 \alpha}{2 \left(1 - \alpha \right)} \right) & \text{, } p > \alpha
\end{cases}
\end{equation}
\end{definition}


## Central normality test

Power transformations aim to transform features to a normal distribution.
However, this may not always be successful or possible.
Deviations from normality can be detected by normality tests, such as the Shapiro-Wilk test [@Shapiro1965-zd].
In practice, normality tests may be too stringent with large sample sizes, outliers, or both.
Here we develop a test for central normality.

\begin{definition}[central portion of a sequence]
Let $\mathbf{X} = \left\{x_1, x_2, \ldots, x_n | x_i \in \mathbb{R}\right\}$ be a finite sequence of length $n > 0$,
ordered so that $x_1 \leq x_2 \leq \ldots \leq x_n$.
Let $p_i = \frac{i - 1/3}{n + 1/3}$, with $i = 1, 2, \ldots n$ be the percentile value
corresponding to each element in $\mathbf{X}$. For elements with tied values in $\mathbf{X}$,
percentile values are replaced by the average in their group, i.e.
if $x_j = x_{j+1} = \ldots = x_{j+m}$, then 
$p_j' = p_{j+1}' = \ldots = p_{j+m}' = 1/(m+1)\sum_j^{j+m}p_j$.
Furthermore, let central portion $\kappa \in (0,1)$.

Then the central portion of $\mathbf{X}$ is 
$\mathbf{X}_{\text{c}} = \left\{x_i \in \mathbf{X} \, | \,  \frac{1-\kappa}{2} \leq  p_i \leq \frac{1 + \kappa}{2}\right\}$.

\end{definition}

\begin{definition}[Residual errors of the central portion of a sequence]

Let the residual error for each element of $\mathbf{X}$ be $r_i =\left| \frac{x_i - \mu_M}{\sigma_M} - F^{-1}_{\mathcal{N}}(p_i) \right|$, 
with $\mu_M$ and $\sigma_M$ robust Huber M-estimates of location and scale of $\mathbf{X}$ [@Huber1981-su], 
and $F^{-1}_{\mathcal{N}}$ the quantile function of the normal distribution $\mathcal{N}(0, 1)$.

Then, the set of residual errors of the central portion of $\mathbf{X}$ (i.e. $\mathbf{X}_{\text{c}}$) is
$\mathbf{R}_{\text{c}} = \left\{ r_i \in \left\{ r_1, r_2, \ldots, r_n\right\} \, | \,  \frac{1-\kappa}{2} \leq  p_i \leq \frac{1 + \kappa}{2}\right\}$.

\end{definition}

\begin{definition}[Central normality of a sequence]

The central portion of a sequence is normally distributed if the sum of residual
errors is equal to zero:
$\sum_{r_j \in \mathbf{R}_{\text{c}}} r_j = 0$

\end{definition}

In practice, a finite sequence sampled from a normal distribution $\mathcal{N}(\mu, \sigma)$ will have a non-zero sum of residual errors.

\begin{definition}[Central normality test]
The null-hypothesis is defined as: $\mathcal{H}_0: \sum_{r_j \in \mathbf{R}_{\text{c}}}r_j = 0$

The alternative hypothesis is defined as: $\mathcal{H}_1: \sum_{r_j \in \mathbf{R}_{\text{c}}}r_j > 0$
\end{definition}

The null-hypothesis $\mathcal{H}_0$ is that the central portion of sequence $\mathbf{X}$
is normally distributed, with the alternative hypothesis $\mathcal{H}_1$ that it
is not normally distributed. To test this hypothesis, a test statistic is computed
and compared against a critical value.

\begin{definition}[Central normality test statistic]
Let $m$ be the number of elements of $\mathbf{R}_{\text{c}}$

The test statistic is then $\tau_{n, \kappa} = \frac{1}{m}\sum_{r_j \in \mathbf{R}_{\text{c}}}r_j$.

\end{definition}

The null-hypothesis should be rejected at significance level $\alpha$ if 
$\tau_{n, \kappa} \geq \tau_{\alpha, n, \kappa, \text{critical}}$.

The central portion of the data needs to be defined and the Type 1 error rates determined to provide critical test statistics.
We will do so in the [Simulation](#simulation) section.


# Simulation

We used simulated data to assess invariance to location and scale of the proposed power transformations,
to develop the empirical central normality test, and to determine weighting for robust transformations.
The $\lambda$ parameter for conventional power transformations (Eqn. \ref{eqn:box-cox-original} and \ref{eqn:yeo-johnson-original}),
as well as $\lambda$, $x_0$ and $s$ parameters for location- and scale-invariant power transformations 
(Eqn. \ref{eqn:box-cox-invariant} and \ref{eqn:yeo-johnson-invariant}) were estimated using the BOBYQA algorithm 
for derivative-free bound constraint optimisation [@Powell2009-zb] through maximum likelihood estimation.
The required algorithms were implemented in the `power.transform` R software package [@Zwanenburg2024-kq] (version 1.0.1).
Of note, the `power.transform` package shifts feature values into the positive domain if negative or zero values are present for Box-Cox power transformations.


## Invariance to location and scale

To assess whether the proposed power transformations lead to values of $\lambda$ 
that are invariant to location and scale of the distribution, we simulated three different sequences
We first randomly drew $10000$ values from a normal distribution: 
$\mathbf{X}_{\text{normal}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{N}\left(0, 1\right)$, 
or equivalently $\mathbf{X}_{\text{normal}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{AGN}\left(0, 1/\sqrt{2}, 0.5, 2\right)$.
The second distribution was a right-skewed generalised normal distribution 
$\mathbf{X}_{\text{right}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{AGN}\left(0, 1/\sqrt{2}, 0.2, 2\right)$.
The third distribution was a left-skewed generalised normal distribution 
$\mathbf{X}_{\text{left}} = \left\{x_1, x_2, \ldots, x_{10000} \right\} \sim \mathcal{AGN}\left(0, 1/\sqrt{2}, 0.8, 2\right)$.
We then computed transformation parameter $\lambda$ using the original definitions (Eqn. \ref{eqn:box-cox-original} and \ref{eqn:yeo-johnson-original}) and the location- and scale-invariant definitions (Eqn. \ref{eqn:box-cox-invariant} and \ref{eqn:yeo-johnson-invariant}) for each distribution.
To assess location invariance, a positive value $d_{\text{shift}}$ was added to each distribution with $d_{\text{shift}} \in [1, 10^6]$.
Similarly, to assess scale invariance, each distribution was multiplied by a positive value $d_{\text{scale}}$, where $d_{\text{scale}} \in [1, 10^6]$.

```{r shifted-distributions, warning=FALSE, message=FALSE, fig.cap=cap, fig.height=3.5}
cap <- paste0(
  "Invariant power transformation produces transformation parameters that are invariant to location and scale. ",
  "Samples were drawn from normal, right-skewed and left-skewed distributions, respectively, which then underwent a shift $d_{\\text{shift}}$ or multiplication by $d_{\\text{scale}}$. ",
  "Estimates of the transformation parameter $\\lambda$ for the conventional power transformations show strong dependency on the overall location and scale of the distribution, ",
  "whereas estimates obtained for the location- and scale-invariant power transformations are constant."
)

.plot_shifted_distributions(manuscript_dir = manuscript_dir, plot_theme = plot_theme)
```

The result is shown in Figure \ref{fig:shifted-distributions}.
For each distribution, transformation parameter $\lambda$ varied with $d_{\text{shift}}$ and $d_{\text{scale}}$ when estimated for conventional transformations.
In contrast, estimation of $\lambda$ for invariant power transformations was invariant to both $d_{\text{shift}}$ and $d_{\text{scale}}$.


## Central normality and empirical central normality test

To develop a test for central normality we need to consider two
parameters: the central portion $\kappa$ as a fixed parameter, and test
statistic $\tau_{\text{ecn}}$. We will first define the central portion
$\kappa$.

For each number of samples $n \in \left\{\lfloor 10^\nu \rfloor | \nu \in \left\{0.7500, 0.8125, \ldots, 4.0000 \right \} \right\}$
we randomly drew $m_d = 30000$ ordered sequences $\mathbf{X}$ from $\mathcal{N}(0,1)$.
This dataset was used to compute the critical test statistic values for the central normality test.
Additionally, we added $10 \%$ outliers by randomly replacing elements of each ordered sequence.
The dataset with outliers was used to compute the critical test statistic values for the empirical variant of the central normality test.
For each sequence we then computed $\tau_{n, \kappa}$ for $\kappa \in \left\{0.60, 0.70, 0.80, 0.90, 0.95, 1.00\right\}$.

```{r include = FALSE, eval = FALSE}
# This creates and sets the lookup table for critical test statistic.
load(file = "./R/sysdata.rda")

central_normality_lookup_table <- .get_test_statistic_lookup_table(
  manuscript_dir = manuscript_dir,
  k = 0.80,
  for_manuscript = FALSE
)

empirical_central_normality_lookup_table <- .get_test_statistic_lookup_table(
  manuscript_dir = manuscript_dir,
  k = 0.80,
  for_manuscript = FALSE,
  with_outliers = TRUE
)

usethis::use_data(
  central_normality_lookup_table,
  empirical_central_normality_lookup_table,
  two_sided_function_parameters, 
  internal = TRUE, 
  overwrite = TRUE
)
```

Figure \ref{empirical-central-normality-kappa} shows $\tau_{\alpha = 0.95, n, \kappa}$
as a function of $n$ for different values of $\kappa$. With decreasing
$\kappa$, the test statistic curve decreases. The curves
for $\kappa = 0.60$, $\kappa = 0.70$ and $\kappa = 0.80$ are similar, whereas curves for 
$\kappa \geq 0.90$ are affected by outliers, as can be observed by comparing curves
of the central normality test with those of the empirical variant.

Since the (empirical) central normality tests assesses whether the central part of a sequence is normally distributed,
we used $\kappa = 0.80$. Critical statistic values for central normality and empirical central
normality tests are shown are shown in Table \ref{tab:central-normality-critical-statistic} and Table 
\ref{tab:empirical-central-normality-critical-statistic}, respectively.

```{r empirical-central-normality-kappa, warning=FALSE, message=FALSE, fig.cap=cap, fig.height=2.5}
cap <- paste0(
  "Critical test statistic $\\tau_{\\alpha = 0.05, n, \\kappa}$ of the (empirical) ",
  "central normality test as function of $n$ for several values of central portion $\\kappa$. ",
  "The critical test statistics for central normality test are determined using fully normal data, ",
  "whereas the statistics for the empirical variant are determined using centrally normal data, ",
  "i.e. with fully normal data where 10% of elements are replaced by outliers. ",
  "emp: empirical"
)

.plot_test_statistic_centrality(
  plot_theme = plot_theme,
  manuscript_dir = manuscript_dir
)

```

To assess type I error rates for the (empirical) central normality test and compare
these to the Shapiro-Wilk test for normality, we randomly drew another 1000 sequences from a
standard normal distribution ($\mathcal{N}(0,1)$ for each 
$n \in \left\{\lfloor 10^\nu \rfloor | \nu \in \left\{0.7500, 0.8125, \ldots, 3.0000 \right \} \right\}$.
A second dataset was created by replacing elements with randomly drawn outliers, as described above.
For each sequence, the p-value for the respective test was used to reject the null
hypothesis that the sequence is (centrally) normal. The null hypothesis was rejected if $p \leq 0.05$.
The type I error rate was determined by computing the fraction of centrally normal sequences rejected this way.
Figure \ref{empirical-central-normality-error_rate} shows that the empirical central normality test had the expected type I error rate for
centrally normal data (with tails of the distribution not being normally distributed),
whereas the central normality test and Shapiro-Wilk test overestimated type I error rates.

```{r empirical-central-normality-error_rate, warning=FALSE, message=FALSE, fig.cap=cap, fig.height=2.5}
cap <- paste0(
  "Observed type I error rates for (central) normality tests in the presence of data derived from normal distributions with and without outliers. ",
  "The empirical central normality (ECN) test uses $\\kappa = 0.80$. CN: central normality"
)

.plot_test_dependency_sample_size(
  plot_theme = plot_theme,
  manuscript_dir = manuscript_dir
)

```

In Figure \ref{fig:empirical-central-normality-examples} we apply the empirical
central normality test to assess central normality of features that are composed
of a mixture of samples drawn from two normal distributions ($n = 100$ each).
With increased separation of the underlying normal distributions, the probability of the
feature being centrally normal decreases, as expected.

```{r empirical-central-normality-examples, warning=FALSE, message=FALSE, fig.cap=cap, fig.height=2.5}
cap <- paste0(
  "Bi-modal distributions and empirical central normality test results. ",
  "The feature (black) is a mixture of two identical sample sets (blue and orange, $n = 100$) ",
  "drawn from normal distributions that are offset by a distance $d$. ",
  "We use the empirical centrally normality test to compute the probability for the hypothesis that ",
  "the distribution is centrally normal. As may be observed, with increasing offset $d$ ",
  "the probability that the feature is centrally normal decreases. ",
  "Quantile-quantile plots are drawn below each distribution."
)

.plot_bimodal_distribution_test(
  plot_theme = plot_theme,
  manuscript_dir = manuscript_dir
)

```


## Robust transformations

Outliers may be present in data and affect estimation of transformation parameters.
The log-likelihood function can be weighted to assign less weight to outlier instances,
see equations \ref{eqn:box-cox-weighted-invariant-log-likelihood} and \ref{eqn:yeo-johnson-weighted-invariant-log-likelihood}.
We proposed three weighting function: step, triangle and tapered cosine, that have one, two and two parameters, respectively.
Each weighting function then takes one of three values as input:
probabilities of the empirical distribution of the original feature,
the z-score of the transformed feature values,
or the residual error between the z-score of the transformed feature values and their expected z-score based on the normal distribution.

To determine the weighting function parameters for each of the nine combinations,
$m_d=500$ sequences $\mathbf{X}_i$ ($i \in \{1, 2, \ldots, m_d\}$) were randomly 
drawn from randomly parametrised asymmetric generalised normal distributions.
Each distribution was parametrised with a random skewness parameter $\alpha \sim U\left(0.01, 0.99\right)$ 
and shape parameter $\beta \sim U\left(1.00, 5.00 \right)$.
Location and scale parameters were set as $\mu = 0$ and $\sigma = 1$, respectively.
To form each sequence $\mathbf{X}_i$, $n = \lceil 10^\gamma \rceil$ instances were then randomly drawn, 
with $\gamma \sim U\left(\log_{10}50, 3\right)$, resulting in a set of sequences with between $50$ and $1000$ elements each.

Outlier values were then drawn to randomly replace 10 percent of the elements of $\mathbf{X}_i$.
Outlier values were set according to @Tukey1977-xm, as follows.
Let $x^{*} \sim U\left(-2, 2\right)$.
Then the corresponding outlier value was:

\begin{equation}
x_{out} =
\begin{cases}
Q_1 - \left(1.5 - x^{*} \right) \text{IQR} & \text{if } x^{*} < 0 \\
Q_3 + \left(1.5 + x^{*} \right) \text{IQR} & \text{if } x^{*} \geq 0
\end{cases}
\end{equation}

$Q_1$, $Q_3$ and $\text{IQR}$ are the first quartile, third quartile and interquartile range of $\mathbf{X}_i$, respectively.
Outlier values randomly replaced elements in $\mathbf{X}_i$.

To find the optimal values for the weighting function parameters $\delta_1$ and $\delta_2$ (if applicable),
we minimised a composite loss $L = \sum_{i=1}^{m_d} L_{\text{cn},i} + 0.1 \sum_{i=1}^{m_d} L_{\lambda,i}$.
The composite loss consisted of two components: a loss term $L_{\text{cn},i} = \tau_{n_i, \kappa = 0.80,i}$, i.e. the mean of residual 
errors of the central 80% of elements of each sequence $\mathbf{X}_i$, which aimed at optimising central normality; 
and a loss term $L_{\lambda,i} = \max\left(0.0,|\lambda_{0,i} - \lambda_{i}| - \xi \right)$,
with $\lambda_{0,i}$ and $\lambda_{i}$ transformation parameters found
for sequence $\mathbf{X}_i$ prior to and after adding outliers, respectively, 
and tolerance parameter $\xi = 0.5$ for Box-Cox and $\xi = 0.3$ for Yeo-Johnson power transformations.
This second term aimed to prevent solutions that provide small improvements in central normality
at the cost of a poor fit of the tails of sequences.

Minimisation was conducted using the BOBYQA algorithm for derivative-free bound constraint optimisation [@Powell2009-zb].
The resulting weighting function parameters for weighted MLE are shown in Tables
\ref{tab:optimal-weighting-parameters-box-cox} and \ref{tab:optimal-weighting-parameters-yeo-johnson} 
for robust location- and scale-invariant Box-Cox and Yeo-Johnson transformations, respectively.

```{r include = FALSE, eval = FALSE}
# This writes the lookup table for two-sided weighting parameters.
# power.transform package.
load(file = "./R/sysdata.rda")

two_sided_function_parameters <- .get_optimised_weighting_function_parameters(
  manuscript_dir = manuscript_dir
)

usethis::use_data(
  central_normality_lookup_table,
  empirical_central_normality_lookup_table,
  two_sided_function_parameters, 
  internal = TRUE, 
  overwrite = TRUE
)
```

\begin{table}
\begin{center}
\caption{Optimal weighting parameters and corresponding loss for location- and scale-invariant Box-Cox power transformations.
$p^{*}$ indicates use of the empirical distribution of feature values, $z$ the z-score of the transformed feature values,
and $r$ the residual error between the z-score of transformed feature values and the expected z-score according to the normal distribution. 
The \textit{initial} column shows the starting parameter value for the optimisation process, with the corresponding boundary values in the \textit{limits} column. 
The \textit{optimal} column shows the optimal parameter values.
The \textit{loss} column shows the loss achieved by each method, under optimised parameters.}
\label{tab:optimal-weighting-parameters-box-cox}
\begin{tabular}{l r r r r r r r}

\toprule
method & \multicolumn{3}{c}{$\delta_1$} & \multicolumn{3}{c}{$\delta_2$} & loss \\
& initial & limits & optimal & initial & limits & optimal & \\

\midrule
non-robust               & ---  & ---       & ---  & ---  & ---       & ---  & 49.6 \\
$p^{*}$ (step)           & 0.80 & $(0, 1]$  & 0.80 & ---  & ---       & ---  & 38.5 \\
$p^{*}$ (triangle)       & 0.80 & $(0, 1]$  & 0.01 & 1.00 & $(0, 1]$  & 0.86 & 43.7 \\
$p^{*}$ (tapered cosine) & 0.80 & $(0, 1]$  & 0.00 & 1.00 & $(0, 1]$  & 0.90 & 42.7 \\
$z$ (step)               & 1.28 & $(0, 10]$ & 2.39 & ---  & ---       & ---  & 52.2 \\
$z$ (triangle)           & 1.28 & $(0, 10]$ & 2.39 & 2.40 & $(0, 10]$ & 4.92 & 52.7 \\
$z$ (tapered cosine)     & 1.28 & $(0, 10]$ & 1.07 & 3.63 & $(0, 10]$ & 3.43 & 57.4 \\
$r$ (step)               & 0.50 & $(0, 10]$ & 0.83 & ---  & ---       & ---  & 53.1 \\
$r$ (triangle)           & 0.50 & $(0, 10]$ & 0.80 & 0.81 & $(0, 10]$ & 0.84 & 54.4 \\
$r$ (tapered cosine)     & 0.50 & $(0, 10]$ & 0.76 & 0.77 & $(0, 10]$ & 0.76 & 53.2 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}



\begin{table}
\begin{center}
\caption{Optimal weighting parameters and corresponding loss for location- and scale-invariant Yeo-Johnson power transformations.
$p^{*}$ indicates use of the empirical distribution of feature values, $z$ the z-score of the transformed feature values,
and $r$ the residual error between the z-score of transformed feature values and the expected z-score according to the normal distribution. 
The \textit{initial} column shows the starting parameter value for the optimisation process, with the corresponding boundary values in the \textit{limits} column.
The \textit{optimal} column shows the optimal parameter values. 
The \textit{loss} column shows the loss achieved by each method, under optimised parameters.}
\label{tab:optimal-weighting-parameters-yeo-johnson}
\begin{tabular}{l r r r r r r r}

\toprule
method & \multicolumn{3}{c}{$\delta_1$} & \multicolumn{3}{c}{$\delta_2$} & loss \\
& initial & limits & optimal & initial & limits & optimal & \\

\midrule
non-robust               & ---  & ---       & ---  & ---  & ---       & ---  & 42.1 \\
$p^{*}$ (step)           & 0.80 & $(0, 1]$  & 0.78 & ---  & ---       & ---  & 35.1 \\
$p^{*}$ (triangle)       & 0.80 & $(0, 1]$  & 0.20 & 0.95 & $(0, 1]$  & 1.00 & 34.0 \\
$p^{*}$ (tapered cosine) & 0.80 & $(0, 1]$  & 0.54 & 0.95 & $(0, 1]$  & 1.00 & 32.6 \\
$z$ (step)               & 1.28 & $(0, 10]$ & 2.32 & ---  & ---       & ---  & 43.0 \\
$z$ (triangle)           & 1.28 & $(0, 10]$ & 1.28 & 1.96 & $(0, 10]$ & 3.43 & 49.7 \\
$z$ (tapered cosine)     & 1.28 & $(0, 10]$ & 0.41 & 1.96 & $(0, 10]$ & 3.75 & 51.3 \\
$r$ (step)               & 0.50 & $(0, 10]$ & 0.92 & ---  & ---       & ---  & 65.4 \\
$r$ (triangle)           & 0.50 & $(0, 10]$ & 0.87 & 1.00 & $(0, 10]$ & 0.87 & 66.0 \\
$r$ (tapered cosine)     & 0.50 & $(0, 10]$ & 1.06 & 1.00 & $(0, 10]$ & 1.07 & 67.1 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


## Assessing transformations using simulated data

Three datasets with 10000 sequences each were created to assess transformation
to normality. For each sequence $\mathbf{X}_i$, $n = \lceil 10^\gamma \rceil$
elements were randomly drawn, with $\gamma \sim U\left(1, 4\right)$, 
resulting in sequences with between $10$ and $10000$ elements. 

- A \textit{clean} dataset with each sequence drawn from $\mathcal{N}(0, 1)$ and
  transformed using an inverse power transformation with randomly drawn 
  transformation parameter $\lambda \sim U\left(0.00, 2.00 \right)$. 
  $\left(\phi_{\text{BC}}^\lambda\right)^{-1}$ and $\left(\phi_{\text{YJ}}^\lambda\right)^{-1}$
  were used as inverse transformations for assessing Box-Cox and Yeo-Johnson transformations,
  respectively. Prior to inverse transformation, sequences for Box-Cox transformations
  were shifted into the positive domain, with minimum value $1$.
  
- A \textit{dirty} dataset with each sequence drawn from $\mathcal{AGN}\left(\mu = 0, \sigma = 1/\sqrt{2}, \alpha, \beta \right)$,
  with randomly drawn skewness parameter $\alpha \sim U\left(0.01, 0.99\right)$ 
  and shape parameter $\beta \sim U\left(1.00, 5.00 \right)$.
  
- A \textit{shifted} dataset with each sequence drawn from $\mathcal{AGN}\left(\mu = 100, \sigma = 10^{-3} \cdot 1/\sqrt{2} , \alpha, \beta \right)$,
  with randomly drawn skewness parameter $\alpha \sim U\left(0.01, 0.99\right)$ 
  and shape parameter $\beta \sim U\left(1.00, 5.00 \right)$.

A dataset with outliers was created for each of the above datasets by replacing
10 percent of elements in each sequence, as described earlier.
  
In addition to no power transformation and location and scale-invariant power transformations,
conventional and Raymaekers and Rousseeuw's robust adaptation [@Raymaekers2024-zf] were assessed.
For the latter two methods, normalisation before standardisation using was additionally assessed using
the following two methods:

1. z-standardisation: $x^{\prime}_{i} = \left(x_i - \mu \right) / \sigma$, with $\mu$ and $\sigma$ the mean and standard deviation of sequence $\mathbf{X}$.

2. robust scaling: $x^{\prime}_{i} = \left(x_i - \text{median}\left(\mathbf{X}\right) \right) / \text{IQR}\left(\mathbf{X}\right)$,
   with $\text{IQR}$ representing the interquartile range of $\mathbf{X}$.

This results in nine types of power transformation. Transformation parameters
were optimised for each sequence. Subsequently the sum of residual errors 
and sum of residual errors of the central portion ($\kappa = 0.80$) were computed
for each sequence after transformation. Then, each method was ranked according 
to the sum of residual errors (data without outliers) or the sum of residual
errors of the central portion (data with outliers). The average rank of each
transformation method was computed over all 10000 sequences in each dataset.
Average ranks for Yeo-Johnson transformations are shown in Table
\ref{tab:comparison_methods_simulation_yeo_johnson}.
Location and shift-invariant Yeo-Johnson transformation ranked best for
all datasets without outliers. The robust variant ranked best in dirty and shifted
datasets with outliers, but not in the clean dataset.
Results for Box-Cox transformations are shown in Table \ref{tab:comparison_methods_simulations_box_cox}.

```{r include = FALSE, eval = FALSE}

# Yeo-Johnson on clean and dirty data.
clean_data <- .assess_simulated_data(manuscript_dir = manuscript_dir, data_type = "clean")
dirty_data <- .assess_simulated_data(manuscript_dir = manuscript_dir, data_type = "dirty")
shifted_data <- .assess_simulated_data(manuscript_dir = manuscript_dir, data_type = "dirty_shifted")

data <- rbind(clean_data, dirty_data, shifted_data, fill = TRUE)
data$dataset <- factor(data$dataset, levels = c("clean", "dirty", "dirty_shifted"))

# Add rank based on residual error for central normality.
data[outlier == FALSE, "method_rank" := rank(residuals), by = c("experiment_id", "outlier", "dataset")]
data[outlier == TRUE, "method_rank" := rank(residuals_central), by = c("experiment_id", "outlier", "dataset")]

table_data <- data[
  ,
  list(
    "avg." = round(mean(method_rank), digits = 2L),
    "10%" = quantile(method_rank, 0.10),
    "50%" = quantile(method_rank, 0.50),
    "90%" = quantile(method_rank, 0.90)
  ),
  by = c("transformation", "outlier", "dataset")
]

data.table::dcast(transformation ~ dataset + outlier, data = table_data, value.var = "avg.")

```


\begin{table}
\begin{center}
\caption{
Comparison of average rank between Yeo-Johnson transformation methods based on either residual error (without outliers) or residual error of the central portion
(with outliers; $\kappa = 0.80$) over 3 datasets with 10000 sequences each. The clean dataset consists of sequences derived through inverse Yeo-Johnson transformation
of data sampled from a standard normal distribution. The dirty dataset contains sequences sampled from asymmetric generalised normal distributions, centred at 0.
The shifted dataset also contains sequences sampled from asymmetric generalised normal distributions, but centred at 100, and scaled by 0.001.
Several transformation methods include normalisation before transformation, indicated by z-score normalisation (norm.) or robust scaling.
A rank of 1 is the best and a rank of 9 the worst. For each dataset, the best ranking transformation is marked in bold.
}
\label{tab:comparison_methods_simulations_yeo_johnson}
\begin{tabular}{l | l r r r r r r}

\toprule
& dataset: & \multicolumn{2}{c}{clean} & \multicolumn{2}{c}{dirty} & \multicolumn{2}{c}{shifted} \\
transformation & outliers: & no & yes & no & yes & no & yes \\

\midrule

none                                  & &         8.51  &         7.60  &         7.33  &         6.15  &         7.44  &         6.60 \\
conventional                          & &         3.70  &         5.70  &         3.87  &         7.06  &         6.49  &         5.93 \\
conventional (z-score norm.)          & &         4.72  &         5.68  &         6.38  &         5.01  &         5.43  &         4.87 \\
conventional (robust scaling)         & &         4.06  &         6.77  &         5.37  &         5.75  &         4.41  &         5.63 \\
Raymaekers-Rousseeuw                  & &         4.50  &         3.20  &         4.01  &         4.13  &         6.27  &         5.65 \\
Raymaekers-Rousseeuw (z-score norm.)  & &         5.27  & \textbf{2.66} &         6.23  &         3.59  &         5.28  &         3.48 \\
Raymaekers-Rousseeuw (robust scaling) & &         4.70  &         2.95  &         5.32  &         3.77  &         4.36  &         3.66 \\
invariant                             & & \textbf{3.13} &         6.77  & \textbf{3.22} &         6.12  & \textbf{2.64} &         6.03 \\
robust invariant                      & &         6.41  &         3.66  &         3.27  & \textbf{3.41} &         2.68  & \textbf{3.10} \\

\bottomrule
\end{tabular}
\end{center}
\end{table}


```{r include = FALSE, eval = FALSE}

data <- .assess_standardisation_before_transformation_simulation(lambda_limit = c(-4.0, 6.0))
data <- data[, mget(c("residuals", "lambda", "p_value", "transformation", "dataset"))]
data[, "residuals" := round(residuals, digits = 1L)]
data[, "lambda" := round(lambda, digits = 1L)]
data[, "p_value" := round(p_value, digits = 3L)]
data.table::dcast(data, transformation ~ dataset, value.var = "residuals")
data.table::dcast(data, transformation ~ dataset, value.var = "lambda")
data.table::dcast(data, transformation ~ dataset, value.var = "p_value")
```

# Experimental Results

## Invariance

Location- and scale-invariant power transformations are intended to yield improved
transformations to normality in the presence of large shifts in location, distributions that due to location and scale are not centered near zero, or both.
Earlier, we assessed these transformations using simulated data.
In the following, they are evaluated using examples from real datasets.
We focus on the Yeo-Johnson transformation because of its ability to handle features with negative values.
Results for Box-Cox transformations are shown in Appendix D.

```{r experimental-results-invariance, warning=FALSE, message=FALSE, fig.cap=cap, fig.height=5.0}
cap <- paste0(
  "Quantile-quantile plots for several datasets: age of patients with lung cancer (top row); ",
  "penguin body mass (middle row); ",
  "and latitude coordinates of houses sold in Ames, Iowa (bottom row). ",
  "Multiple quantile-quantile plots are shown: for the original feature (left column); ",
  "the feature transformed using the conventional Yeo-Johnson transformation ",
  "and Raymaekers and Rousseeuw's robust adaptation (middle column); ",
  "and the feature transformed using the non-robust and robust location- and-scale ",
  "invariant Yeo-Johnson transformations (right column)."
)

data <- .plot_experimental_invariance(plot_theme = plot_theme, method = "yeo_johnson")
data$plot
```

### Age of patients with lung cancer

A common feature in health-related datasets is age.
Here we use data on 228 patients with lung cancer that was collected and published by Loprinzi et al. [@Loprinzi1994-cd].
The age in the cohort was $62.4 \pm 9.1$ (mean Â± standard deviation) years.
Applying conventional and invariant Yeo-Johnson transformations to patient age yielded the following results, see Figure \ref{fig:experimental-results-invariance}:
no transformation (sum of residuals with normal distribution $\sum r_i = 16.5$, $p = 0.71$);
conventional transformation ($\lambda = 2.0$, $\sum r_i = 11.5$, $\mu_{YJ} = 1.8 \cdot 10^3$, $\sigma_{YJ} = 0.5 \cdot 10^3$, $p = 0.94$);
Raymaekers and Rousseeuw's robust adaptation ($\lambda = 2.0$, $\sum r_i = 11.5$, $\mu_{YJ} = 1.8 \cdot 10^3$, $\sigma_{YJ} = 0.5 \cdot 10^3$, $p = 0.94$);
location- and scale-invariant transformation ($\lambda = 0.9$, $\sum r_i = 8.8$, $\mu_{YJ} = 1.2$, $\sigma_{YJ} = 1.1$, $p = 0.95$);
and robust location- and scale-invariant transformation ($\lambda = 0.8$, $\sum r_i = 10.6$, $\mu_{YJ} = 1.2$, $\sigma_{YJ} = 1.0$, $p = 0.80$).

Location- and scale-invariant transformation led to a lower overall residual sum, indicating a better transformation.
Robust location- and scale-invariant transformation had a higher residual sum compared to the non-robust variant, which may be due to the lack of outliers in the data.
Conventional transformations inflated the mean $\mu_{YJ}$ and standard deviation $\sigma_{YJ}$ of the age feature after transformation.
The empirical central normality test did not detect any statistically significant deviations from central normality for any transformation (all $p \geq 0.78$).


### Penguin body mass

Gorman, Williams and Fraser recorded body mass (in grams) of 342 penguins of three different species [@Gorman2014-eo].
The body mass was $(4.2 \pm 0.8) \cdot 10^3$ (mean Â± standard deviation) grams, and not centrally normal ($p = 0.03$).
Applying conventional and invariant Yeo-Johnson transformations to body mass yielded the following results, see Figure \ref{fig:experimental-results-invariance}:
no transformation (residual sum $\sum r_i = 48.0$, $p < 0.001$);
conventional transformation ($\lambda = -0.5$, $\sum r_i = 32.2$, $\mu_{YJ} = 2.1$, $\sigma_{YJ} = 4 \cdot 10^{-3}$, $p = 0.06$);
Raymaekers and Rousseeuw's robust adaptation ($\lambda = -0.5$, $\sum r_i = 32.2$, $\mu_{YJ} = 2.1$, $\sigma_{YJ} = 4 \cdot 10^{-3}$, $p = 0.06$);
location- and scale-invariant transformation ($\lambda = 0.5$, $\sum r_i = 26.8$, $\mu_{YJ} = 0.9$, $\sigma_{YJ} = 0.9$, $p = 0.18$);
and robust location- and scale-invariant transformation ($\lambda = 0.4$, $\sum r_i = 23.1$, $\mu_{YJ} = 0.8$, $\sigma_{YJ} = 0.8$, $p = 0.40$).

Location- and scale-invariant transformation produced a lower overall residual sum, indicating a better transformation.
Moreover, conventional transformations led to low standard deviation $\sigma_{YJ}$ of the body mass feature after transformation.
The empirical central normality test did not detect any statistically significant deviations from central normality for any transformation (all $p \geq 0.38$).


### Latitude in the Ames housing dataset

Geospatial datasets usually contain coordinates.
The Ames housing dataset contains data on 2930 properties that were sold between 2006 and 2010 [@De-Cock2011-jf], including their geospatial coordinates.
The latitude was $42.03 \pm 0.02)$ (mean Â± standard deviation).
Applying conventional and invariant Yeo-Johnson transformations to latitude yielded the following results, see Figure \ref{fig:experimental-results-invariance}:
no transformation (residual sum $\sum r_i = 328$, $p < 0.001$);
conventional transformation ($\lambda = 62.1$, $\sum r_i = 319$, $\mu_{YJ} = 4.8 \cdot 10^{99}$, $\sigma_{YJ} = 0.1 \cdot 10^{99}$, $p < 0.001$);
Raymaekers and Rousseeuw's robust adaptation ($\lambda = 95.4$, $\sum r_i = 319$, $\mu_{YJ} = 6.4 \cdot 10^{153}$, $\sigma_{YJ} = 0.3 \cdot 10^{153}$, $p < 0.001$);
location- and scale-invariant transformation ($\lambda = 1.5$, $\sum r_i = 326$, $\mu_{YJ} = -1.2$, $\sigma_{YJ} = 0.8$, $p < 0.001$);
and robust location- and scale-invariant transformation ($\lambda = 1.4$, $\sum r_i = 311$, $\mu_{YJ} = -1.3$, $\sigma_{YJ} = 0.9$, $p < 0.001$).

Every transformation reduced the residual sum.
The non-robust location- and scale-invariant transformation did not improve over conventional alternatives and yielded a data distribution lacking central normality (empirical central normality test: $p=0.05$).
However, conventional transformations had high values for the $\lambda$ parameter, which could lead to numerical issues.


## Robustness against outliers

We previously simulated data to assess invariant power transformations and their robustness against outliers.
Here, we assess invariant power transformations in real data with outliers.

```{r experimental-results-outlier-robustness, warning=FALSE, message=FALSE, fig.cap=cap, fig.height=3.5}
cap <- paste0(
  "Quantile-quantile plots for two datasets with outliers: vehicle fuel consumption ",
  "(top row), where outliers are related to highly fuel-efficient vehicles; ",
  "and maximum arterial wall thickness in patients with ischemic stroke (bottom row). ",
  "Multiple quantile-quantile plots are shown: for the original feature (left column); ",
  "the feature transformed using the conventional Yeo-Johnson transformation ",
  "and Raymaekers and Rousseeuw's robust adaptation (middle column); ",
  "and the feature transformed using the non-robust and robust location- and-scale ",
  "invariant Yeo-Johnson transformations (right column). ",
  "Samples with observed quantiles below $-3.0$ or above $3.0$ are indicated by crosses."
)

data <- .plot_experimental_outlier_robustness(plot_theme = plot_theme, method = "yeo_johnson")
data$plot
```

### Fuel efficiency in the Top Gear dataset

The Top Gear dataset contains data on 297 vehicles that appeared on the BBC television show *Top Gear* [@Alfons2021-kc].
Within this dataset, the fuel consumption feature contains outliers due to highly fuel-efficient vehicles.
Applying conventional and invariant Yeo-Johnson transformations to the fuel consumption feature yielded the following results, see Figure \ref{fig:experimental-results-outlier-robustness}:
no transformation (residual sum $\sum r_i = 54$, $p=0.58$);
conventional transformation ($\lambda = -0.1$, $\sum r_i = 55$, $\mu_{YJ} = 3.0$, $\sigma_{YJ} = 0.3$, $p < 0.001$);
Raymaekers and Rousseeuw's robust adaptation ($\lambda = 0.8$, $\sum r_i = 48$, $\mu_{YJ} = 29$, $\sigma_{YJ} = 15$, $p=0.32$);
location- and scale-invariant transformation ($\lambda = -1.3$, $\sum r_i = 44$, $\mu_{YJ} = 0.5$, $\sigma_{YJ} = 0.1$, $p < 0.001$);
and robust location- and scale-invariant transformation ($\lambda = -0.9$, $\sum r_i = 50$, $\mu_{YJ} = 2.0$, $\sigma_{YJ} = 2.3$, $p=0.35$).

Outliers cause non-robust transformations to fail to transform the data to a centrally normal distribution (empirical central normality test $p < 0.001$ for conventional and invariant transformations).
Robust transformations produce distributions that are centrally normal (empirical central normality test $p > 0.05$).

### Maximum arterial wall thickness in an ischemic stroke dataset

The ischemic stroke dataset contains historic data from 126 patients with risk at ischemic stroke [@Kuhn2019-kt].
These patients underwent Computed Tomography Angiography to characterize the carotid artery blockages.
Angiography imaging was then assessed, and various characteristics related to the blood vessels and the disease are measured.
The maximum arterial wall thickness feature contains several instances with outlier values.
Applying conventional and invariant Yeo-Johnson transformations to this feature yielded the following results, see Figure \ref{fig:experimental-results-outlier-robustness}:
no transformation (residual sum $\sum r_i = 110$, $p=0.86$);
conventional transformation ($\lambda = -0.7$, $\sum r_i = 30$, $\mu_{YJ} = 1.0$, $\sigma_{YJ} = 0.1$, $p=0.02$);
Raymaekers and Rousseeuw's robust adaptation ($\lambda = 1.1$, $\sum r_i = 136$, $\mu_{YJ} = 7.2$, $\sigma_{YJ} = 14$, $p=0.91$);
location- and scale-invariant transformation ($\lambda = 0.2$, $\sum r_i = 12$, $\mu_{YJ} = -11.8$, $\sigma_{YJ} = 6.9$, $p=0.23$);
and robust location- and scale-invariant transformation ($\lambda = -0.6$, $\sum r_i = 27$, $\mu_{YJ} = 0.7$, $\sigma_{YJ} = 0.1$, $p=0.18$).

The conventional non-robust transformation failed to produce a centrally normal distribution (empirical central normality test $p=0.02$).
Robust transformations produce distributions that are centrally normal (empirical central normality test $p > 0.05$).

## Integration into end-to-end machine learning

We used 231 datasets containing at least one numeric feature from the Penn Machine Learning Benchmarks collection [@Romano2022-gq].
In this collection, 114 datasets correspond to regression tasks and 117 datasets
to classification tasks. Using the familiar auto-machine learning library [@Zwanenburg2021-so] (version 1.5.0),
each dataset was used to train a model for each of 32 process configurations.
Each process configuration specifies the learner (generalised linear model, L1-regularised linear models (Lasso), gradient boosted linear model, or
random forest), transformation method (none, conventional Yeo-Johnson, robust
invariant Yeo-Johnson, robust invariant Yeo-Johnson with empirical central
normality test (rejecting transformations with $p \leq 0.01$), and normalisation method (none,
$z$-standardisation), yielding 32 distinct configurations.
Before each experiment, each dataset was randomly split into a training (70%)
and holdout test (30%) set five times. Thus, a total of 27720 models were
created. Each model was then evaluated using the holdout test set using one of
two metrics, i.e. the root relative squared error (RRSE) for regression tasks and the
area under the receiver operating characteristic curve (AUC) for classification tasks.

For the purpose of assessing the effect of the difficulty of the task, we computed the median performance score over all models for each dataset and assigned one the following categories:

* very easy: $\text{AUC} \geq 0.90$ or $\text{RRSE} \leq 0.10$ (57 datasets)
* easy: $0.90 > \text{AUC} \geq 0.80$ or $0.30 \geq \text{RRSE} > 0.10$ (40 datasets)
* intermediate: $0.80 > \text{AUC} \geq 0.70$ or $0.60 \geq \text{RRSE} > 0.30$ (52 datasets)
* difficult: $0.70 > \text{AUC} \geq 0.60$ or $0.80 \geq \text{RRSE} > 0.60$ (33 datasets)
* very difficult: $0.60 > \text{AUC} \geq 0.50$ or $1.00 \geq \text{RRSE} > 0.80$ (48 datasets)
* unsolvable: $\text{AUC} < 0.50$ or $\text{RRSE} > 1.00$ (1 dataset)

To remove the effect of the dataset, and allow for comparing metrics, we ranked all performance scores for each dataset so that a higher rank corresponds to better performance.
Experiments yielding the same score received the same, average, rank.
Subsequently ranks were normalised to the $[0.0, 1.0]$ range.

```{r include = FALSE, eval = TRUE}

data <- .get_ml_experiment_data(manuscript_dir = manuscript_dir)

# Convert to data matrix. First aggregate within each experiment because the
# Friedman test is not for repeated measurements.
data <- data.table::dcast(
  data,
  dataset ~ learner + transformation_method + normalisation_method, value.var="value_rank",
  fun.aggregate = mean
)

h_overall <- friedman.test(as.matrix(data[, "dataset" := NULL]))
```

Significant differences exist between process configurations (Friedman test: $p < 10^{-8})$.

```{r include = FALSE, eval = TRUE}

data <- .get_ml_experiment_data(manuscript_dir = manuscript_dir)

# Convert to data matrix. First aggregate within each experiment because the
# Friedman test is not for repeated measurements.
data <- data.table::dcast(
  data,
  dataset ~ learner, value.var="value_rank",
  fun.aggregate = mean
)

h_learner <- friedman.test(as.matrix(data[, "dataset" := NULL]))

# h_learner <- stats::wilcox.test(
#     x = data[["random_forest_ranger"]],
#     y = data[["glm"]],
#     paired = TRUE,
#     alternative = "two.sided"
# )
```

```{r include = FALSE, eval = TRUE}

data <- .get_ml_experiment_data(manuscript_dir = manuscript_dir)

# Convert to data matrix. First aggregate within each experiment because the
# Wilcoxon signed rank test is not for repeated measurements.
data <- data.table::dcast(
  data,
  dataset ~ normalisation_method, value.var="value_rank",
  fun.aggregate = mean
)

h_normalisation <- stats::wilcox.test(
    x = data[["none"]],
    y = data[["robust_standardisation"]],
    paired = TRUE,
    alternative = "two.sided"
)
```

```{r include = FALSE, eval = TRUE}

data <- .get_ml_experiment_data(manuscript_dir = manuscript_dir)

# Convert to data matrix. First aggregate within each experiment because the
# Friedman test is not for repeated measurements.
data <- data.table::dcast(
  data,
  dataset ~ transformation_method, value.var="value_rank",
  fun.aggregate = mean
)

h_transformation <- friedman.test(as.matrix(data[, "dataset" := NULL]))
```

Considering single process parameters, 
the choice of learner (Friedman test: $p < 10^{-8})$),
normalisation method (Wilcoxon signed rank test: $p = 4 \cdot 10^{-8}$), 
and transformation method (Friedman test:$p = 0.007$,
all had a significant impact (at $p = 0.05$).

To estimate the marginal effects of process parameters, including transformation method, 
we first fit a regression random forest (ranger package [@Wright2017-rf] version 0.16.0): 2000 trees, 
node size 2, other hyperparameters default) with process parameters and task difficulty as predictors and normalised rank as response variable.
The estimated marginal effects are shown in Figure \ref{fig:marginal-effect-plot}.
On the scale of normalised ranks ($[0.0, 1.0]$), the overall estimated marginal effect of using a random forest instead of generalised linear model was $0.272$.
The overall marginal effect of using z-standardisation to normalise features was $0.008$.
Transformation methods had the following marginal effects:
$0.003$ for using conventional Yeo-Johnson transformation instead of no transformation;
$-0.007$ for using robust invariant Yeo-Johnson transformation instead of no transformation; 
$-0.009$ for using robust invariant Yeo-Johnson transformation with empirical central normality test instead of no transformation; 
$-0.010$ for using robust invariant Yeo-Johnson transformation instead of conventional Yeo-Johnson transformation;
and $-0.012$ for using using robust invariant Yeo-Johnson transformation with empirical central normality test instead of conventional Yeo-Johnson transformation.

```{r marginal-effect-plot, echo=FALSE, fig.cap=cap, fig.height=7.5, warning=FALSE}
cap <- paste0(
  "Estimated marginal effect of learners, normalisation and transformation methods on ranked model performance scores in 18560 machine learning experiments on 232 datasets. ",
  "The top-left panel shows the marginal effect of learners, i.e. random forests and generalised linear models (GLM). ",
  "Random forests outperform GLM models for all task difficulties. ",
  "The top-right panel shows the marginal effect of feature normalisation methods, i.e. no normalisation and z-standardisation. ",
  "z-standardisation is generally beneficial, but the estimated effect is neglible. ",
  "The bottom panel shows the marginal effects of different transformation methods, split by learner and normalisation method. ",
  "There is no consistent behaviour, and estimated effects are neglible. ",
  "Note that the ranges of the $x$-axes of the three main panels differ. ",
  "ECNT: empirical central normality test."
)

.plot_marginal_effects(plot_theme = plot_theme, manuscript_dir = manuscript_dir)
```

# Discussion

In their work on power transformation, Box and Cox already mention transformation with a shift parameter, but preferred the version in Eq. \ref{eqn:box-cox-original} for the theoretical analysis in their paper [@Box1964-mz], which subsequently became the convention.
Yeo and Johnson's power transformation lacks a shift parameter altogether [@Yeo2000-vw].
We showed that these power transformations are sensitive to location and scale of data distributions.
To mitigate this issue, we defined location- and scale-invariant variants of the Box-Cox and Yeo-Johnson transformations.
We furthermore assessed methods for making these transformations robust to outliers, and devised an empirical test for central normality.

Robust location- and scale-invariant transformations are a suitable replacement for their conventional counterparts.
They demonstrated robustness against outliers and prevent inaccurate transformations and potential numerical issues due to location and scale of the distribution of a feature.
This is particularly relevant for automated data processing, where such issues may go unnoticed.
Compared to non-robust location- and scale-invariant transformations, real-world examples with outliers showed higher residual errors of transformed features. 
Robust transformations seek to minimise residual errors for the central part of the distribution, instead of the entire distribution, including outliers.
The empirical central normality test indicated that robust transformations are better able to achieve central normality in the presence of outliers.
However, in a machine learning experiment of 232 real-world datasets that contained at least one numeric feature, we did not find a meaningful benefit -- nor detriment -- to model performance for location- and scale-invariant power transformations.
One reason may be that numeric features with large location shifts ($|\mu| > 1000.0$) were uncommon.
Of the 4886 numeric features in the 232 datasets, 266 (5%) features in 34 datasets had large location shifts, of which 200 appeared in just 2 datasets.
For the latter two datasets, the transformation method did not show significant difference between groups (Friedman test; $p > 0.05$).
```{r include = FALSE, eval = TRUE}

data <- .get_ml_experiment_data(manuscript_dir = manuscript_dir)
data <- data[dataset %in% c("Hill_Valley_with_noise", "Hill_Valley_without_noise")]

# Convert to data matrix. First aggregate within each experiment because the
# Friedman test is not for repeated measurements.
data <- data.table::dcast(
  data,
  dataset ~ transformation_method, value.var="value_rank",
  fun.aggregate = mean
)

h_transformation <- friedman.test(as.matrix(data[, "dataset" := NULL]))
```

Location- and scale-invariant transformations are realised by simultaneously optimising three parameters, i.e. transformation parameter $\lambda$, shift parameter $x_0$ and scale parameter $s$.
We derived the log-likelihood function to facilitate optimisation using MLE.
Alternatively, standardisation of a numeric variable (e.g., through subtracting its median value and division by its interquartile range) prior to conventional power transformations may achieve a similar effect in reducing sensitivity to the distribution's location and scale.
While this alternative helps prevent these issues -- provided that normalisation does not lead to negative values for Box-Cox transformation -- location- and scale-invariant transformations seem to provide an overall better transformation to normality (Appendix F).

We assessed several methods for robust power transformation.
Methods that relied on the z-score of the transformed feature or the residual error yielded worse results than the non-robust method.
This is partly due to the initial choice of threshold parameters.
Using different initial values, closer to the upper limits ($\delta_1 = 10.0; \delta_2 = 10.0$), led to a reduced loss.
However, these values effectively correspond to a non-robust transformation, where all instances receive the same weight.
Underperformance of these methods could be explained by their use of transformed features for setting weights.
Consequently, the weights change at each iteration in the MLE optimisation process.
This increases local variance in the log-likelihood function and creates local optima that the optimiser may not handle well.
As a consequence, optimal values for transformation parameter $\lambda$ might differ, which increases the presented loss for optimising threshold parameters.
Methods that relied on the empirical probability did not suffer from this issue, as weights remained fixed during MLE.

We introduced an empirical test for central normality to assess whether distributions deviate from normality in a way that might require closer inspection prior to further processing.
The empirical test for central normality differs from other tests for normality, such as the Shapiro-Wilk test [@Shapiro1965-zd], in that the test statistic is independent of the number of samples.
This makes this test more practical for assessing central normality for larger sample numbers, where other tests may detect inconsequential deviations from normality.

This work has the following limitations.
Firstly, we did observe several numerical stability issues for optimisation criteria other than MLE (Appendix B).
These appear in regions where transformation parameters would lead to very large or small numbers when using conventional power transformations.
For MLE stability issues were not observed.
Secondly, the empirical central normality test is based on simulations instead of statistical theory, and relies on a somewhat arbitrary definition of the central portion of a distribution.
Thus, while the test may asses whether data is sufficiently normally distributed for practical purposes, it should not be used as a strict test for normality.


# Conclusion

Compared to their conventional versions, robust location- and scale-invariant Box-Cox and Yeo-Johnson transformations reduce sensitivity to outliers and the location and scale of features.
An empirical central normality test can assess the quality of transformation of features to normal distributions.
The combination of both facilitate the use of power transformations in automated data analysis workflows.


# Data and code availability

Location- and scale-invariant power transformations were implemented in the `power.transform` package for R, which is available from [GitHub](https://github.com/oncoray/power.transform) and the [CRAN repository](https://cran.r-project.org/package=power.transform). The manuscript was created using R Markdown and is likewise available from the `power.transform` GitHub repository. Data and results for the machine learning experiment are separately available from [Zenodo](https://doi.org/10.5281/zenodo.14986689).


# References
